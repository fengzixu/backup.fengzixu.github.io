<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on LittleDriver</title>
    <link>http://littledriver.net/posts/</link>
    <description>Recent content in Posts on LittleDriver</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Nov 2018 15:41:12 +0800</lastBuildDate>
    
	<atom:link href="http://littledriver.net/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Detect Redis Config File</title>
      <link>http://littledriver.net/posts/detect-redis-config-file/</link>
      <pubDate>Fri, 16 Nov 2018 15:41:12 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/detect-redis-config-file/</guid>
      <description>Include Part Include include 可以允许用户在 Redis 的 Conf 文件中引用一份「已经准备好」的配置。一般来说，我们都会把一些通用的且很少变化的配置放在一个「配置模板」中，然后在真正的 Redis-Server 启动的配置文件中使用include 命令包含它。这样一来，Redis-Server 在启动的时候就会使用配置模板内的配置项。
你也可以在include 命令后按照正常的方法写入其他配置。若新添加的配置项不在「配置模板」内容之中，那么这就相当于做了一个「追加」操作，否则，那将是一个「覆盖」操作。更为重要的是，include 的内容不会被CONFIG REWRITE命令的执行结果覆盖。
# Example # include /path/to/local.conf # include /path/to/other.conf  Load Part loadmodule loadmodule 可以允许用户在 Redis 的 Conf 文件中指定在 Redis-Server 启动时要加载的「外部模块」。因为 Redis 本身也是通过 C 语言进行实现的，所以它在redismodule.h文件中提供了一些 C API， 可供用户在使用 C 语言实现「外部模块」的时候使用。
# Example # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so  Network Part bind bind配置项，允许用户指定一个或多个特定网卡的地址，并且将 Redis-Server 绑定在这个地址上。设置好了之后，若想通过 Redis-Cli 等工具访问 Redis-Server 的时候，host 参数必须指定bind 所绑定的地址。否则无法访问 Redis-Server。
默认情况下，Redis 将此配置项的缺省值指定为：127.</description>
    </item>
    
    <item>
      <title>K8s GC Design Principle</title>
      <link>http://littledriver.net/posts/k8s-gc-design-principle/</link>
      <pubDate>Thu, 15 Nov 2018 22:40:55 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/k8s-gc-design-principle/</guid>
      <description>Ref: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/garbage-collection.md#orphaning-the-descendants-with-orphan-finalizer Warning：设计文档的对应的 k8s 版本为1.7
 Q: What is GC of Kuernetes ?
A:
GC 是 Garbage Collector 的简称。从功能层面上来说，它和编程语言当中的「GC」 基本上是一样的。它清理 Kubernetes 中「符合特定条件」的 Resource Object。（在 k8s 中，你可以认为万物皆资源，很多逻辑的操作对象都是 Resource Object。）
Q: What are dependent mechanisms to clear needless resource objects?
A:
Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。当然，你也可以利用ObjectMeta.OwnerReferences自由的去给两个 Resource Object 建立关系，前提是被建立关系的两个对象必须在一个 Namespace 下。
// OwnerReference contains enough information to let you identify an owning // object. Currently, an owning object must be in the same namespace, so there // is no namespace field.</description>
    </item>
    
    <item>
      <title>Head First AUFS and Docker Image</title>
      <link>http://littledriver.net/posts/head-first-aufs-and-docker-image/</link>
      <pubDate>Fri, 26 Oct 2018 10:13:21 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/head-first-aufs-and-docker-image/</guid>
      <description>WARNING：文中的一些Demo， 均是模仿了陈皓老师在 https://coolshell.cn/articles/17061.html 文章中给出的实例。在这里只做学习和记录使用，欢迎大家去原文观看，若有版权问题，可联系我删除。
 Docker Image 和 AUFS 是什么关系？ Image 是 Docker 部署的基本单位，一个 Image 运行在一个 Docker Container 上面。这个 Image 包含了我们的程序文件，以及这个程序依赖的资源的环境。Docker Image 对外是以一个文件的形式展示的（更准确的说是一个 mount 点）。既然说到文件，那么它肯定是受到文件系统来管理的。
在 Linux 内核 4.0以及之前的版本上（主要是 Ubuntu 和 Debian），Docker 使用 AUFS 来管理 Docker Image 的存储。虽然，在一些新的 Docker 版本中，已经使用了其他不同的方案来管理镜像，如 DeviceMapper，overlay2。但是 AUFS 是一个比较标准且简单的实现方式，通过 AUFS 来了解 Docker Image 的原理是一个不错的选择。
什么是 AUFS？ AUFS 是 Union File System 众多实现方式的一种。Union File System 从字面意思上来理解就是「联合文件系统」。它将多个物理位置不同的文件目录「联合」起来，挂载到某一个目录下，形成一个抽象的文件系统。
概念理解起来比较枯燥，最好是有一个真实的例子来帮助我们理解：
首先，我们建立 company 和 home 两个目录，并且分别为他们创造两个文件
root@rds-k8s-18-svr0:~/xuran/aufs# tree . .</description>
    </item>
    
    <item>
      <title>Head First Linux Namespace</title>
      <link>http://littledriver.net/posts/head-first-linux-namespace/</link>
      <pubDate>Wed, 24 Oct 2018 16:16:21 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/head-first-linux-namespace/</guid>
      <description>什么是 Linux Namespace？它解决了什么问题？ 简单来说，Linux Namespace 是操作系统内核在不同进程间实现的一种「环境隔离机制」。
举例来说：现在有两个进程A，B。他们处于两个不同的 PID Namespace 下：ns1, ns2。在ns1下，A 进程的 PID 可以被设置为1，在 ns2 下，B 进程的 PID 也可以设置为1。但是它们两个并不会冲突，因为 Linux PID Namespace 对 PID 这个资源在进程 A，B 之间做了隔离。A 进程在 ns1下是不知道 B 进程在 ns2 下面的 PID 的。
这种环境隔离机制是实现容器技术的基础。因为在整个操作系统的视角下，一个容器表现出来的就是一个进程。
Linux 一共构建了 6 种不同的 Namespace，用于不同场景下的隔离：
 Mount - isolate filesystem mount points UTS - isolate hostname and domainname IPC - isolate interprocess communication (IPC) resources PID - isolate the PID number space Network - isolate network interfaces User - isolate UID/GID number spaces  Docker 的网络隔离机制——Linux Network Namespace Docker 使用的网络模型是 CNM（Container Network Model），根据官方的设计文档，它的结构大致如下：</description>
    </item>
    
    <item>
      <title>Deep into the process and thread 2</title>
      <link>http://littledriver.net/posts/deep-into-process-and-thread-2/</link>
      <pubDate>Sun, 21 Oct 2018 18:23:07 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/deep-into-process-and-thread-2/</guid>
      <description>先来先服务 最短作业优先 &amp;amp;&amp;amp; 最短剩余时间优先 轮转调度 优先级调度 最短进程优先        Q： 为什么都说「进程切换」是个比较昂贵的操作，它昂贵在哪呢？
A:
首先，就是由用户态向内核态的切换。因为我们需要保存旧的进程的状态。其次，我们可能需要执行一个比较复杂的「调度算法」，挑选出一个合适的候选进程。除此之外，每一次进程的切换都会伴随着 CPU 高速缓存的失效。在新的进程被切换到 CPU 上开始运行之后，高速缓存需要从内存中动态装入一些和新的进程运行有关的信息。
Q: 在什么情况下需要进行进程的调度（切换）？
A:
进程切换发生的时候是必然会进行进程调度的，因为此时 CPU 空闲，需要让新的进程在上面运行。那么，什么场景下会发生进程间的切换呢？
 CPU 时间片消耗完：这种场景是较为普通和正常的，操作系统为了让所有的进程都能够得到 CPU 的资源，只分配每个进程一定的 CPU 时间片，当时间片消耗完后，进程正常退出，就需要调度新的进程上来。 I/O 中断触发：当一个进程因触发 I / O 活动而阻塞之后，若相应的 I / O 设备完成了所需的任务，会向 CPU 发送 I / O 中断。此时，需要决定到底调度那类进程运行：阻塞之后满足条件的，随机的就绪进程，刚刚在运行的进程（被中断打断） 触发阻塞条件：一个进程可能会执行一些会阻塞自身的操作：如阻塞的系统调用，等待某一个资源的释放。此时，操作系统会将另外一个就绪的进程切换上来。  一般来说，一个进程的在运行期间过多的是在 CPU 上进行计算，那么它被认为「计算密集型」的。反之，如果大多数时间都消耗在了 I_O 等待上，那么它被认为是「I / O密集型」的。对于当前的 CPU 和 I_O 设备来说，我们可能需要的问题可能更多的和 I / O密集型的进程有关。因为 CPU 的发展速度是远大于 I / O 设备的。</description>
    </item>
    
    <item>
      <title>Deep into the process and thread</title>
      <link>http://littledriver.net/posts/deep-into-process-and-thread/</link>
      <pubDate>Fri, 19 Oct 2018 21:08:03 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/deep-into-process-and-thread/</guid>
      <description>Q: 什么是进程？
A:
进程其实是一个比较抽象的概念，它是用来描述多道程序设计系统中的一个工作单元。单纯的给进程下一个定义是没有任何意义的。比如现在所谓的标准答案：进程是操作系统中运行的程序。对于进程，我们更多的要理解它是一个「复合体」，它是一系列活动的组合。它是一个逻辑上的概念，并不是一个现实世界中具体的事物。这一点和 k8s 中的 pod很像。所以，我更倾向于将进程理解为操作系统中的一个复杂且基本的工作单元。
Q: 子进程被创建之后和父进程是如何隔离的？
A:
通常情况下，在 Linux 系统当中，一旦子进程被创建，那么子进程和父进程就会分别占有两块独立的地址空间。相互之间是隔离的，并且可以通过一些方式来进行通信或者共享某些资源。但是，在之后操作系统发展的过程当中，对于父子进程的创建过程可能会有一些优化，而不仅仅是粗暴的将父进程地址空间中所有的东西都 copy 一份给子进程。这里也是有一个比较重要的机制：COW（写时复制机制）。
Q: Linux 中的进程和 Windows 中有哪些不同？
A:
Linux 系统中的进程是有严格的「父子关系」的，并且所有的进程会以树形的层次结构组织起来。其中祖先进程可认为是 Init，它是进程树中的根。而 Windows 中的进程，无论父子，都是靠一个叫做「句柄」的概念对一个进程进行标识的，并且这个句柄是可以传递的。所以在 Windows 中，进程间没有严格的父子关系。
Q: 什么是线程？
A:
线程是轻量级的进程。进程由操作系统来管理而线程由进程来管理。不同进程之间的地址空间是隔离的，但是不同线程之间的地址空间是共享的。一般来说，一个进程通常会有一个主线程，进程负责向内核申请线程运行所需要的资源和环境，而线程才是真正执行程序的单位。
Q: 有了进程为什么还需要线程？
A:
从程序性能的角度来说，很多程序在一个进程中都会做很多任务。这些任务可以大致的被划分为两类，一类是 I/O, 一类是计算。I/O 通常消耗的时间会比较长，对于只有主线程的进程来说，它会一直处于等待状态，内核分配给他的 CPU 时间片也会被白白的消耗。计算类的任务则会直接消耗 CPU 资源，最大限度的利用了已分配的时间片。所以，如果一个程序中同时包含这两类任务的话，计算类的任务很可能被 I/O 类的任务阻塞，最终导致整个程序的效率下降。因为线程是存在于进程的地址空间中的，如果可以在进程地址空间中创建多个线程，并且让这些线程重叠执行，分别去运行不同类型的任务，就可以在一定的 CPU 时间片内，将程序的效率尽可能的提高。通过上面的一些思考，我们甚至可以延伸出另外一个问题：多线程技术一定会对我们的程序产生积极的影响么？其实也不尽然。如果我们的程序中既包含大量的 I/O 操作，也包含大量的计算操作，那么多线程技术是可以提升我们程序的效率的。因为此时由于多个线程重叠的进行，最大限度的利用了 CPU 的时间片。如果我们的程序基本都是计算类的任务，很少有 I/O 操作，那么多线程的引入可能不会对提升程序的效率有太大的帮助。因为即使线程间的切换消耗再小，还是有 CPU 时间片上面的损耗的。同样，这个问题的思考方式还可以延伸到：多进程技术一定会对我们的程序有积极的影响么？
从资源共享的角度来说，不同进程间的地址是不同的，所以它们在共享一些资源的时候就会比较麻烦，可能需要借助第三方的东西，比如文件。然而对于同一个进程中的不同的线程来说，这种内存上的隔离是不存在的，它们可以很方便的去共享一些资源。看到这里你可能会说，在地址空间不隔离的条件下，多个线程对同一个资源可能会出现竞争的想象。对于这个问题，我们要明确两点：首先，线程间共享资源的初衷是让多个线程合作，而不是让它们竞争。其次，如果不可避免的发生了竞争，也可以通过一些互斥的机制来解决。
最后还要提及一点的就是，大多数操作系统对于多线程的实现都是在「用户态」下，且线程中维护的必要信息会较进程少很多。这就造成了线程是比进程更轻量级的。如果不可避免的发生频繁和切换操作，那么很明显线程在这种场景下会更具优势。
Q: 进程和线程之间的关系是什么？
A:
进程更倾向于从操作系统申请资源，并对这些资源进行统一的管理，提供一个良好的运行环境。线程则更注重利用已经分配好的资源运行程序。也就是说，实际上在 CPU 上调度执行的并不是进程而是线程。
Q: 如何实现线程？
A:
实现线程有两种思路：在用户态实现 or 在内核态实现。
当我们想在用户态实现「线程」的时候，就意味着「线程」或者说是「多线程」对于内核来讲应该是透明的。内核与具有单个控制线程的主进程还是按照原来的模式运行（进程模型）。所以，我们很自然的就能够想到，在用户态下需要一系列「过程」的集合来实现和线程有关的操作以及「多线程」技术。这个「过程」的集合可以被称作为是一种 Runtime 系统。</description>
    </item>
    
    <item>
      <title>Head First SDS in Redis</title>
      <link>http://littledriver.net/posts/head-first-sds-in-redis/</link>
      <pubDate>Sun, 14 Oct 2018 18:37:32 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/head-first-sds-in-redis/</guid>
      <description>Redis 设计与实现之动态字符串 Q: 什么是 SDS
A:
SDS 是 Redis 在实现过程中使用的一种「动态字符串」。由于 Redis 的代码基本都是通过 C 语言来实现的，所以 SDS 在最底层还是依赖于char buf[]来存储数据。SDS 对象的数据结构大致如下图所示
可以看出，SDS 结构体成员中有三个属性：len，free，buf。其中 len 标识一个 SDS 对象管理的字符串有效字符是多少个，而 free 则代表这个 SDS 在不扩充空间的前提下还可以存储多少个有效字符，buf 则是一个char[]类型的指针，它指向一段连续的内存空间，这里才是真正存储字符串的地方（有效字符串是指除\0以外的字符串集合）。
Q: 有了 C 字符串，为什么还需要 SDS？
A:
通过阅读相关数据以及对 Redis 文档的查阅，可以总结出以下几点使用 SDS 而不适用原生 C 字符串的好处
 * 更高效的获取一个 SDS 对象内保存的字符串的长度 * 杜绝缓冲区溢出 * 减少因字符串的修改导致的频繁分配和回收内存空间操作 * 二进制安全 * 和 C 语言有关字符串的库函数有一个更高的兼容性  其实看到这里，如果你之前使用其他语言中的「普通数组」实现过一个「动态数组」的话，那么除了「二进制安全」这一条好处可能不太理解之外，其余的应该都比较熟悉。下面我们就来分别说一下这几个好处。
Q: 如何更高效的获取字符串的长度？
A:
这个问题在传统的 C 字符串中算是一个痛点。在一个线性的数据结构中，我们都只能通过遍历这个数据结构中所有的有效元素才能够获取它准确的长度，这个操作的时间复杂度是 O(N) 级别。但是当我们只是把 C 字符串作为 SDS 这个数据结构中的一个成员时，我们就可以通过增加另外一个成员len来实时的计算字符串的准确长度。计算的方式也很简单，就是在字符串做「新增元素」的操作时对len+1，做「减少元素」的操作时对len-1。这样一来，就可以通过访问len来获取 SDS 内存储的字符串的长度。类似于这样的实现：</description>
    </item>
    
    <item>
      <title>How to Deploy Jaeger Cluster</title>
      <link>http://littledriver.net/posts/how-to-deploy-jaeger-cluster/</link>
      <pubDate>Wed, 26 Sep 2018 17:58:57 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/how-to-deploy-jaeger-cluster/</guid>
      <description>Deploy Jaeger in Kubernetes Preparation 通过一张 Jaeger 的架构图，我们可以知道，要在我们的开发环境中部署一套Jaeger，需要部署以下几个组件
 jaeger-agent jaeger-collector data-storage  Elasticsearch Cassandra   由于我们想将 jaeger 部署到 k8s 集群中，针对于这个特定的部署环境，我们可以对部署方案做如下的梳理：
 部署方式： helm+jaeger 的 chart 包（参考：https://github.com/jaegertracing/jaeger-kubernetes, https://github.com/helm/charts/tree/master/incubator/jaeger） 存储中间件：helm + ElasticSearch 的 chart 包（参考：https://github.com/helm/charts/tree/master/incubator/elasticsearch） 底层存储方案：宿主机外挂500G 数据盘+ Ceph RBD 访问：ingress+nginx—ingress-controller+service  通过对GitHub - jaegertracing/jaeger-kubernetes: Support for deploying Jaeger into Kubernetes的了解，我们可以知道，其实 jaeger 本身的组件部署时比较简单的，直接 kubectl applpy 一个编排文件即可搞定。唯一比较麻烦的是对底层存储的配置。针对这样的情况，我们决定将 jaeger 部署的顺序做如下安排：
 准备一个 k8s 集群（笔者有一个一主一从的 k8s 集群，基于虚拟机建立的），主从节点各挂在一块500G 的数据盘 Ceph RBD 集群和 k8s 混布（╮(╯▽╰)╭，没办法，穷啊），创建需要分配存储的测试 pod，查看 pvc 和 pv 的创建情况 部署 Elasticsearch 部署 Jaeger，测试集群内部是否能够成功访问 jaeger-query 部署 ingress+nginx-ingress-controller，测试集群外部访问情况   本文默认用户已经部署好了 k8s 集群并且挂载了数据盘，因为 k8s 的部署步骤也比较复杂，足以写另外一篇文章了。而且对于挂载磁盘的问题来说，用户所处平台的不同（云主机，物理机，本地的虚拟机）可能处理的方式也不太一样。这两部分在本文中就不做过多的描述了。</description>
    </item>
    
    <item>
      <title>Head First of Tracing System</title>
      <link>http://littledriver.net/posts/head-first-of-tracing-system/</link>
      <pubDate>Wed, 26 Sep 2018 17:58:42 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/head-first-of-tracing-system/</guid>
      <description>什么是 Link Tracing？ 为什么我们需要 Tracing？ Link Tracing 字面意思就是链路追踪，它是一个抽象的概念。针对于一个分布式的系统来说，「链路」主要是某个请求从进入到这个系统一直到被处理完成的整个路径。而「追踪」就更好理解了，它可以给我们提供一定的信息，方便我们了解在这个链路上都发生了什么。
传统的「服务」像是一锅大杂烩，将所有的功能都集成到一个 binary 中，如监控，日志收集，UI，存储等等。多个模块硬耦合在一起，带来的后果就是整个系统变得臃肿和不可控，修改起来也相当的麻烦。更新频率较低的功能，往往会受到更新频率较高的功能的影响。由于种种原因，越来越多的开发团队企图将他们的「大杂烩」剥离成一个个相互合作的微服务。多个具有合作关系的微服务统称为一个「分布式系统」（笔者自己对分布式系统的简单理解）。
分布式系统以及微服务给开发人员和运维人员带来好处的同时也引入了一些难题：
 由于服务和服务之间依靠网络通信，请求链路变长使得延迟有一定的升高，所以我们可能需要做一些优化 现代服务多使用「并发」来实现一些 feature。并发逻辑若出现 bug，在一个分布式系统中就需要一个有效的措施去定位和解决  而「链路追踪」技术就旨在为分布式系统解决这些问题。它提供一些方便且有效的手段，使我们可以清晰的了解到整条请求链路中各个阶段的耗时。若在请求处理的过程中出错，尤其是在一些不是我们自己实现的组件中出错，「链路追踪」也可以准确的捕获这类信息。目前已经有了一些成熟的「分布式链路追踪」系统，如 Zipkin or Jaeger。
TracingGraph 如果让我们通过一个图来描述一个请求的「链路」，基本上可以画成上面的样子，从1-8。这个「链路追踪」图示有优势也有劣势
 优势：  可以看清楚各个组件之间的调用关系。从用户的角度出发观察整个请求的处理链路较为直观  劣势：  整个请求执行的过程中，无法区分哪些逻辑是串行的，哪些逻辑是并行的 无论是其中的一个小步骤还是整个请求，都无法观察到它们的运行时间   上面的「链路追踪」图，在保留了「可以看清调用关系」的基础上，针对我们之前谈到过的几个问题作出了改进。在整个的 tracing 过程中，每一个带有不同颜色的矩形区域都被称作是一个 Span，它代表了一个调用的过程（逻辑上的一个工作单元）。一个 Span 的长度结合 X 轴可以判断它的 processing duration。并且，在按照层级将调用分类之后，可以明显的区分出「串行」和「并发」的逻辑（如图中的container start-up 调用和 stoage allocation两者就是并发执行的，而 container start-up 和 start-up scripts 就是串行执行的）。
Jaeger Jaeger 是一个由 Uber 公司开发的分布式的链路追踪系统。它遵循了 OpenTracing 提出的和「链路追踪」有关的一系列的数据模型和标准。jaeger 还实现了 OpenTracingAPI（golang），使得应用程序接入 jaeger 更加的方便。一个 jaeger 通常包含以下几个组件：
 jaeger-client：业务接入 jaeger 所需要的 SDK，由特定的语言实现 jaeger-agent：作为 daemon 进程部署在每一个 host/container 上，用以收集追踪数据发送至 collector jaeger-collector：收集从 jaeger-agent 上反馈而来的数据，以特定的存储组件进行持久化（es） jaeger-ui&amp;amp;jaeger-query：提供 UI 界面和查询追踪数据的服务，使得用户能够方面的查看每个请求的「链路追踪」信息  如果是部署在生产环境的 k8s 集群中，除了上述说到的几个组件之外，还需要一个持久化存储的中间件，为 jaeger 管理海量的「追踪数据」。对于 jaeger 来说，存储中间件有几个可以供选择：</description>
    </item>
    
    <item>
      <title>The brief of PV and PVC</title>
      <link>http://littledriver.net/posts/notion-of-pv-and-pvc/</link>
      <pubDate>Wed, 26 Sep 2018 17:58:12 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/notion-of-pv-and-pvc/</guid>
      <description>Overview 在 kubernetes 上部署服务，无论是「有状态」的，还是「无状态」的，可能大部分都有存储数据的需求。随之而来的就是对存储资源的需求。对于 k8s 来说，最底层的存储资源，我们可以直接利用 local storage，即机器的本地磁盘，也可以使用 ceph rbd 这种存储插件，单独的搭建一个存储的集群，使得运行在k8s 上的服务可以使用 network storage。
但是，在搭建好了一个 ceph 存储的集群之后，我们要如何使用它呢？
PV &amp;amp;&amp;amp; PVC 在 k8s 中，定义了两个资源来管理和使用集群中的存储资源：
 Persistent Volumes： 可以认为是 k8s 集群提供的存储资源的一种抽象，由 k8s 集群自动创建 PersistentVolumeClaim： 可以认为是 User(广义的 User，泛指一切想使用存储资源的事物) 对存储资源的一个请求声明  对于两者的「合作」模式，可以简单的做如下理解：
用户申请一个定量的存储资源，创建一个 PVC，集群内部的某些组件在收到 PVC 的创建消息之后，根据要求创建一个相应的 PV，并且会为这个 PV 分配实际的存储空间（在本地磁盘，或者是在 ceph rbd 这种网络存储上）。但是，如果不能满足 PVC 所申请的 volume 资源，那么 PV 不会被创建，而这个 PVC 也会一直保留在那里，直到它所申请的 Volume 容量被满足。
对于 PV 来说，集群对它的提供方式一般有两种：动态和静态。静态的 PV 一般都是由 k8s 集群内特定的组件预先分配好的，在用户需要使用的时候，可以直接将 PVC 和 PV 做一个「绑定」的操作即可。而动态的 PV 则需要一个叫做 StorageClass 的东西。</description>
    </item>
    
    <item>
      <title>Contrain Pod Scheduling</title>
      <link>http://littledriver.net/posts/contrain-pod-scheduling/</link>
      <pubDate>Tue, 11 Sep 2018 16:52:58 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/contrain-pod-scheduling/</guid>
      <description>Overview 当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可以认为这个调度的过程对我们来说是「随机」的。因为在没有了解清楚 Kubernetes 的调度规则之前，我们也不知道创建的一个 Pod 将会被调度到哪台节点上。 但是在日常开发的过程中，尤其是基于 k8s 开发一些数据库相关应用的时候，我们通常对 Pod 被调度的节点是有要求的，比如：我们需要将主节点强制调度到某台机器上，或者我们需要主从节点所在的 Pod 不能调度到同一个 Node 上。
这些要求在 kubernetes 上是可以实现的，它提供以下几种方式来方便使用者干预 Pod 的调度策略：
 NodeSelector Taints and Tolerations Anti-Affinity/Affinity  NodeSelector NodeSelector 是依靠 LabelSelector 实现的一种调度策略。若想使用 NodeSelector， 需要分为两部分来考虑
Node 我们需要在集群中某一个我们想要调度到的 Node 上打上一个label，比如，A Node上的硬盘是 SSD，那我们就可以使用 kubectl 命令将 A Node 打上一个 disktype=ssd 的 Label。 kubectl label nodes &amp;lt;node-name&amp;gt; &amp;lt;label-key&amp;gt;=&amp;lt;label-value&amp;gt;
Pod 对于 Pod 来说，我们需要在它的 spec 段内，加入 nodeSelector 段。并且在 nodeSelector 段中填入「目的 Node」 所携带的 Label。比如我们想将一个 Pod 强制调度到 Node A 上，那么在构造 Pod 的基本信息的时候，就要相应的做如下修改（以 yaml 文件为例）：</description>
    </item>
    
    <item>
      <title>Heade First Redis Sentinel</title>
      <link>http://littledriver.net/posts/heade-first-of-redis-sentinel/</link>
      <pubDate>Mon, 27 Aug 2018 16:00:20 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/heade-first-of-redis-sentinel/</guid>
      <description>WARNING: 本篇文章是在阅读了 Redis Sentinel 的设计文档之后产出的。但是由于该设计文档已经被官方标识为 draft 且时间也比较久远，笔者在阅读这份文档的时候还是发现了几处与当前新版本实现不同的地方，甚至是有一些错误的。所以本文的目的也就在于：先借助该设计文档对 Sentinel 这套高可用的方案有一个宏观上的了解，具体的实现细节，之后会另写几篇博文对 Sentinel 的源码进行分析。若是有能力直接阅读源码的读者可直接去阅读源码。如果你在阅读这篇文章的时候，发现了一些错误并且愿意帮忙改正的话，请私信联系我。
 先说明几个这篇 blog 使用的名词
- 「Redis Sentinel」指通过 Sentinel 实现的 Redis 高可用方案 - 「Sentinel 节点」指 Redis 集群中运行的某一个 Sentinel 节点（redis-server）  Q: 什么是「Redis Sentinel」?
Redis Sentinel 是一套「方案」。它能够提升 Redis 集群的可用性，也就是我们常说的「高可用」
Q: Sentinel 通过哪些功能可以实现 Redis 集群的「高可用」？
Redis 集群的「高可用」，在我理解，可以分为「用户」和「服务」两个维度进行讨论
- 用户维度 - Sentinel 通过某些命令可以让用户实时获取当前集群的 Master 节点的地址（Sentinel 会进行故障转移操作） - 预设了一些「通知」机制，可以在 Redis 集群内部发生异常的时候通知给集群的维护者或者使用者 - 服务维度 - 监控集群中主从节点以及 Sentinel 节点的健康状态 - 集群 Master 发生故障时可自动进行「故障转移」操作来恢复集群  总结下来，这套「高可用」方案既可以保证 Redis 集群自身的健康，同时也在发生故障的时候尽量降低对集群使用者的影响。Redis 的作者对这套高可用方案有着更加清晰的概括</description>
    </item>
    
    <item>
      <title>Head First Scheduler of Golang</title>
      <link>http://littledriver.net/posts/head-first-scheduler-of-golang/</link>
      <pubDate>Tue, 14 Aug 2018 15:45:25 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/head-first-scheduler-of-golang/</guid>
      <description>我们的程序是如何被运行的？ 学习过操作系统的人，应该对进程和线程的模型都是有所了解的。按照我的理解：「进程」是操作系统资源分配的基本单位，它给程序提供了一个良好的运行环境。「线程」则是一个轻量级的进程，一个「进程」中可以有很多线程，但是最终在一个 CPU 的核上只能有一个「进程」的其中一个「线程」被执行。所以，我们的一个程序的执行过程可以粗略的理解为：
 程序的可执行文件被 Load 到内存中 创建进程&amp;amp;创建主线程 主线程被 OS 调度到合适的 CPU 执行  goroutine 是什么？ 看了很多文章对于 goroutine 的描述，其中出现最多的一句话就是「The goroutine is a lightweight thread.」。在结合了对操作系统的线程模型的理解之后，我觉得 goroutine 就是一个在用户空间（usernamespace）下实现的「线程」，它由 golang 的 runtime 进行管理。goroutine 和 go runtime 的关系可以直接的类比于线程和操作系统内核的关系。至于它是不是轻量级，这需要和操作系统的线程进行对比之后才能够知道。在此我们先避免「人云亦云」。
goroutine 和 thread 有什么不同？ 目前看起来 goroutine 和 thread 在实现的思路上是比较相似的。但是为什么说 goroutine 比 thread 要轻量呢？从字面的意思上来理解，「轻量」肯定意味着消耗的系统资源变少了。
内存消耗 OS 从 OS 的层面来说，内存大致可以分为三个部分：一部分为栈（Stack）另外一部分为堆（Heap），最后一部分为程序代码的存储空间（Programe Text）。既然在逻辑上 OS 已经对内存的布局做了划分，如果栈和堆之前如果没有遵守「分界线」而发生了 overwrite，那么结果将是灾难性的。为了防止发生这种情况，OS 在 Stack 和 Heap 之间设置了一段不可被 overwrite 的区域：Guard Page
thread 通过对 OS 中线程模型的了解，我们可以知道：同一个进程的多个线程共享进程的地址空间。所以，每一个 thread 都会有自己的 Stack 空间以及一份 Guard Page用于线程间的隔离。在程序运行的过程中，线程越多，消耗的内存也就越多。当一个线程被创建的时候，通常会消耗大概1MB的空间（预分配的 Stack 空间+ Guard Page）。</description>
    </item>
    
    <item>
      <title>Redis 数据持久化机制</title>
      <link>http://littledriver.net/posts/redis-persistence-1/</link>
      <pubDate>Sun, 05 Aug 2018 20:11:46 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/redis-persistence-1/</guid>
      <description>Redis 有两种持久化数据的机制  AOF： 将对数据库所有的「写操作」以追加的方式，写入一个文件当中。待 Redis 重启之后，可以通过这些指令恢复数据 RDB： 以生成数据集快照的方式，全量备份数据。生成一个 dump 文件，落盘保存  两种持久化机制可以同时启用，redis-server 默认在启动的时候，会使用它们持久化的数据对自身的数据集进行恢复。但是会优先使用 AOF，因为RDB 在备份的过程中，如果集群出现重启等极端现象，会丢失一部分数据。而 AOF 基本上是间隔一秒执行一次fsync，最大限度的确保不会丢失数据。
RDB 持久化的大致过程  redis-server 每隔一段时间就执行一次 BGSAVE 命令 redis-server 的主进程会 Fork 一个子进程进行持久化操作 子进程将此时内存中的数据写入进一个临时文件中 写入成功之后，原子的将旧的 rdb 文件替换为新的并删除旧的备份文件  RDB持久化过程享受了 OS 中 COW （ Copy-on-write ） 机制的优势。早期 Linux 内核的 Fork 过程会无脑的直接将父进程的各种资源 Copy 一份给子进程。这种旧的机制在效率上有很大的问题。但是在有了 COW 机制之后，若父进程的内存段中的内容都没有修改，那么就只会分配给子进程很少的一部分资源，如进程描述符等等。实际上此时父子进程是共享内存地址空间的，达到了资源共享的效果。若父子进程共享的内存中的内容有修改，才会真正的去复制一份内存当中的内容给子进程.
所以，你会发现，在你的redis-server 没有接受任何写操作的时候，你执行 bgsave 和 save 是非常快的。但是，若你一遍在疯狂的写入数据，一边在执行bgsave或者 save，他们的执行时间就会随着写入速度的增加而增加，且有可能发生 OOM 的现象。
另外还有一个问题就是：Redis 在执行 bgsave 操作的时候，如果此时还有写操作在执行，那么最新的修改会被同步到新的 RDB 的文件中么？根据 COW 的机制思考一下就可以知道，当发生写入操作之后，父子进程的实际物理内存已经分开了两份，而最终本次 RDB 操作产生的备份文件也是根据子进程内存中的内容来的，所以，可以说 RDB 执行完成后文件中的内容，就是 fork 函数执行那一刻，内存中的内容（此时，父子进程共享内存）</description>
    </item>
    
    <item>
      <title>关于「争论」的一点思考</title>
      <link>http://littledriver.net/posts/a-little-thinking-of-arguments/</link>
      <pubDate>Sun, 05 Aug 2018 20:11:46 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/a-little-thinking-of-arguments/</guid>
      <description>关于「争论」的一点思考 当我和其他人发生争论之后，我都有回家之后自己反思一下这个过程。这个行为在我18年初进入容器团队之后愈发频繁。促使我这么做的原因就是，在我和思路比我更清晰，想问题更透彻的人交流的过程中，我发现我的思考是站不住脚的。这导致我在一些问题的讨论上不占优势，不能准确的把我的想法表达出来。
 简单的概括一下，核心的观点只有一个：「尽量想好和这个问题相关的全部细节再开始说话」
 改掉一个「说话不经过谨慎思考」的毛病会比较难，但是好在我的工作环境中有在这方面做得比较好的人，以至于每次讨论问题如果不下三个回合我就被对方的问题问住的时候，我就知道，这是因为我的一次思考上的缺陷，导致的一次失败的讨论。如果此时我还浑然不知的话，那么这可能就会进化为一次「争论」，因为人总是希望在讨论中说服对方，而「争论」也总是会给参与的双方造成不小的伤害。
所以，不仅限于在工作当中，生活中即使和我父母，女朋友相处的时候，我也尽量做到在「全面」和「细致」的思考后再来对一个问题进行交流。因为我觉得这是一种解决问题最好的方式，也是一种最有效的沟通方式。虽然我现在还不能100%做到这一点，但是我已经意识到了，并且会提醒自己改正，甚至会影响我身边的人做一些转变，我觉得就是一个好的开始。
通常在讨论一个问题的过程当中，我会大致按照以下的思路进行思考问题：
在我是被提问者的情况下  我会反复和提问者确认「你的问题是什么」？在这期间其实会碰到很多提问者是没有想清楚自己的问题的。犯这个毛病的包括我自己。但是当我自己做被提问者的时候我才会发觉这个问题是如此的严重 我会问，你想怎么做？这个阶段主要的目的就是确认，提问者是来讨论「方案」的，还是来寻求「帮助」的。如果是寻求帮助，且时间允许，我可能会先把这个问题了解清楚再和他交流。如果是讨论「方案」，我会针对这个方案提出我的一些疑问，直到我明白了以下几点：a) 这个方案核心的逻辑是什么 b)这个方案到底能不能解决问题 若这个问题我熟悉，我会想想我会怎么解决。若我觉得时间较长，可能会中断这次交流，等准备好了再进行下一次。若时间比较短，我会想好之后再和对方说明我的方案。若我对问题不熟悉，我也会直接告诉提问者，我无法帮助你 在双方第一次阐述过自己的方案之后，可能会有一些冲突。此时，我会希望和被提问者找出问题的矛盾点，看是因为对方案理解上的偏差导致的矛盾，还是方案本身就有硬伤。 找出矛盾点，摆证据，为什么你的方案不行，我的行。如果我的不行，那么对方也需要摆证据，证明不行  基本上在上述5个步骤执行完之后，问题是可以顺利解决的。即使在这次沟通的过程当中不能解决，那么至少能够明白是哪些问题不能解决，这对于下一次沟通是非常有帮助的。总的来说，按照上面的思路，应该会是一个比较有效的沟通方式。 但是「争论」往往都是在4，5步的过程中产生的。产生「争论」的原因挺多的，我目前也没有找到根本原因。但就经验来说，基本上有以下因素：
 没理解清楚对方的意图 急迫的想说服对方，已经忽略了沟通的目的 对方的思路本身就是混乱的，自己都没有搞清楚矛盾究竟在什么地方 自己没表达清楚自己的意图  以上四个因素，都是我自己在沟通的过程中遇到了以及我自己常犯的几个错误。个人觉得一个比较有效的方法，就是听人家说完了，先等个几分钟，不说话。想清楚了，或者借助一些工具，把自己的思路整理好，再开始说话。
在我是提问者的情况下  若我是寻求「帮助」： 我会描述清楚问题的现象以及我做的操作，我执行这些操作所处的环境 我期望的回答有三种：
 我遇到的问题是「预期行为」，这个预期行为包括异常的也包括正常的。总之，需要给我一个这个问题产生的原因。异常的话我等待处理，正常的话我了解之后就算结束了 我遇到的问题是「非预期行为」，给我一个时间点，我回去等待处理。或者现在不能处理，要告诉我原因. 我自己的操作姿势不对，没仔细看文档，或者文档本身就不全，那么直接给出我正确的姿势  若我是寻求「方案」：
 我会先描述清楚，问题是什么 我会描述我的方案是什么 等待对方的提问并回答提问 问对方的方案。对方若没有方案，可以给出一些思路。若还没有，可以针对我的方案给出一些建议，如果能找出缺陷，那是极好的   我在作为提问者的过程当中，是被怼的最多的。当然，这也是我决定要改正这个问题的一个动机。其实上面说的一大堆，最终的目的都是要解决问题，既然要解决问题，无外乎就需要经过以下几个阶段
 问题是什么？为什么会出现这个问题？这个问题是「问题」么？ 有没有解决方案？解决方案是什么？如果有多个方案我为啥要选择这个？这个方案带来的成本是什么？收益又是什么？性价比是不是最高的？对方的方案有啥致命缺陷？ 实施确定的解决方案需要什么资源？  其实，人在交流的过程当中，很难像上面说的那么理智和冷静，不然那就不是人了，而是机器。人会受到很多情绪和性格方面的影响，甚至是身体不舒服或者遇到了心烦的事，都会让这次交流失败。我其实并不是一个脾气很好的人，我也非常的易于受到情绪的影响。更重要的是，我这个人是典型的吃软不吃硬，要是真较上劲，其实我就不会在乎这个事情本身了，会把它转换为一次人和人之间的对抗。这一点，我相信我女朋友和我妈，都是有切身的体会的，我为此也经常会问她们和我交流问题的感受。 写这么多的原因，是因为最近低效且消极的交流有两次。虽然我之前在尽力避免，但是「交流失败」的结局出现的频率有点高，我就决定今天再梳理一下我交流问题的「方法论」。一个是提醒自己要尽力的进行理性交流，另外一个就是告诉自己允许「交流失败」的发生。一旦意识到，目前已经不是在寻求解决问题的办法而是在相互扯皮或者发泄情绪，应该立即停止交流。如果多次交流还解决不了，那只能交给能够对这个问题负责的人去决断了。
做人，做事都是这样，问心无愧就行了。有错就改错，没错就忘了那些「不愉快」的事情即可。原则只有一个：「解决问题」</description>
    </item>
    
    <item>
      <title>Golang Log Level Best Pratice-1</title>
      <link>http://littledriver.net/posts/golang-log-level-best-pratice/</link>
      <pubDate>Thu, 03 May 2018 13:39:17 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/golang-log-level-best-pratice/</guid>
      <description>写在前面 在使用 Golang 语言开发的过程中，被广大开发者广泛使用的 Debug 方式应该就是观测服务输出的关键性日志信息了。这也就是我们俗称的「日志 Debug」 方式。虽然 Golang 也可以通过一些断点调试的方法去 Debug，这种方式在 Demo 阶段或者仅仅做一个 Experiment 的时候可能是比较好的，但是在真正的生产环境中或者说高并发的场景下，断点调试就会显得力不从心了。所以，主流的方式可能仍然是「日志 Debug」。
对于 Log 的使用，可能大多数人都倾向于使用一些第三方的日志工具。毕竟 Golang 官方的 Log 库提供的功能以及对 Log 输出的可定制性都是有限的。主流的日志工具库有以下两个：
 https://github.com/sirupsen/logrus https://github.com/golang/glog  这两个日志工具库都提供了丰富的功能，其中一个共性的功能就是「按照级别输出日志」。因为在输出日志的时候，可能会有以下输出级别给你选择，不同的级别可能代表着这条日志输出的优先级，重要性，作用都是不同的：
 Error Fatal Info Warn Debug  那么在接下来的两篇关于「Golang Log」的文章当中，将会根据笔者的实践经验，在日志级别的使用以及日志库工具的使用上给出一些我自己的「Best Practice」。
Warn Warn 这种日志级别一直让我感觉比较困惑，按照单词的字面意思来理解，它是用来输出警告信息的。那警告这种级别到底是用于什么场景呢？如果是错误的话那应该直接用 Error，如果是简单的输出一些信息的话，那么用 Debug 和 Info 都是可以的。这样看来，Warn 这种级别就很尴尬，和其他的级别没有明显的区别，导致开发者在选择日志输出级别的时候就多了一种令人困惑的选择。如果你一直在使用 Warn 这种日志级别在输出「错误」信息的话，建议还是改成 Error 较好。最主要的原因就是，滥用 Warn 级别可能会给我们的日志造成很大的噪音，因为大家潜意识里就认为 Warn 不重要，Error 才是需要注意的。这样一来，把错误信息放在 Warn 里面输出就是非常不合适的，很有可能让我们忽略一些关键的信息。
日志输出级别上的 Warn 和监控报警级别中的 Warn 显然不是一回事，前者是将输出的日志信息划分等级，以此来帮助我们追查问题和观察程序运行的情况。后者则是用来像我们报告服务的某些指标已经很接近 Error 报警的阈值，提醒我们要注意。并且监控报警中的 Warn 级别也是提倡要善用的，滥用同样会导致噪声。</description>
    </item>
    
    <item>
      <title>gRPC Deadline Explanation</title>
      <link>http://littledriver.net/posts/grpc-deadline-explanation/</link>
      <pubDate>Sat, 21 Apr 2018 21:23:55 +0800</pubDate>
      
      <guid>http://littledriver.net/posts/grpc-deadline-explanation/</guid>
      <description>什么是 gRPC Deadline gRPC 框架中的 Deadline 的概念主要是针对于客户端而言的。它表明了一个 RPC 请求在完成之前或者被错误终止之前，gRPC client 需要等待多长时间。如果我们在使用 gRPC 框架进行 RPC 请求的时候没有指定这个值，它的默认值是依赖于不同编程语言的实现的。理论上来说， 若不指定，应该是一个非常大的值。
为什么要设置 Deadline 一个 RPC 请求的处理端大部分是我们所实现的一个服务，如果此时客户端请求不设置 Deadline，那么服务端的资源就会一直被占用（如内存，CPU，网络端口等），而且，任意一个客户端请求都可能会达到默认的 Deadline 最大值。
什么是一个合适的 Deadline 值 对于 Deadline 值的设定，gRPC 官方的文档中并没有给出一个具体的最佳实践。仔细一想，这也是比较正确的。因为使用 gRPC 框架的服务性质各不相同，所以一个「最佳」的值，即使给出来也是没有多的意义的。所以，我们就得出了一个结论：「Deadline 的最佳值是和业务紧密相关的」。
上面在提到「为什么要设置 Deadline 值」的时候，我们举了一个客户端和服务端的例子。但其实在真正的工业环境当中，gRPC 请求的通信双方基本上同时扮演着客户端和服务端的角色。在请求过程中角色的不同，就导致他们是相互独立的两个个体。对于一次请求来说，它是否成功可能在服务端和客户端上的认知上是有差异的。如，一个请求从 A 发送至 B，B 处理完成之后发送 Response。此时 B 会认为本次的 RPC 请求已经成功结束。但是，由于各种各样的问题，该 Response 可能没有按时到达 A 端。那么 A 在等待这个回应的时候很有可能过了它设置的 Deadline 值，或者是默认值。此时，A 会认为本次请求失败。在理解这里的时候，如果联想一些「TCP 三次握手」以及「全双工通信」的原理，迁移一下就会很容易明白了。对于这个问题，gRPC官方的文档中是建议我们能够在 Application Layer 去检查和解决他们。
 PS: 笔者在使用 gRPC 框架到公司的项目中时，也被这个问题搞得非常的头疼。一开始是觉得官方肯定会给出一个 Deadline 的最佳实践的，然而并没有。这种客户端和服务端对一次 RPC 请求成功与否的认知差别，会在服务刚刚设置这个 Deadline 的时候稳定性会受到一定的影响。由于是和网络请求相关联的值，那么它受到网络环境好坏的影响也是非常大的。所以，笔者觉得这个 Deadline 的值是要定期去审视和修改的。因为随着业务的变动，同一个请求所需要的时间会有所变化，而且这个时间的设置一定程度上还要对网络环境进行容错。目前觉得最好的时间就是对服务的 gRPC 请求增加可视化监控，监测 DEADLINE_EXCEEDED出现的比例。如果发生了陡增的现象，那么就提醒你可能要重新调整 Deadline 的阈值了。</description>
    </item>
    
    <item>
      <title>记一次追查 gRPC Server 报错的过程</title>
      <link>http://littledriver.net/posts/the-process-of-resolving-a-bug-for-grpc-server/</link>
      <pubDate>Thu, 19 Apr 2018 23:56:41 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/the-process-of-resolving-a-bug-for-grpc-server/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Kubernetes pod schedular strategy</title>
      <link>http://littledriver.net/posts/kubernetes-pod-schedular-strategy/</link>
      <pubDate>Tue, 17 Apr 2018 14:34:48 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/kubernetes-pod-schedular-strategy/</guid>
      <description>概述 在 k8s 中，调度 Pod 到 Node 上通常是不需要我们关心的。K8s 会自动的帮我们寻找具有合适资源的 Node，并且 Pod调度在上面。但是，有的时候，我们需要将 Pod 调度到一些特定的 Node 上面，比如一些挂在了 SSD 硬盘的 Node。因为有这样的需求，k8s 可以让我们自己控制 Pod 调度至 Node 的策略。这些策略是通过 labelSelector 来实现的。
NodeSelector NodeSelector 是PodSpec 中的一个 Field。它是一个 key-value 的 pair。key 对应了 Node 中的 label，value 对应了Node 中的 labelValue。当这个 Pod 被创建之后，k8s 会按照这个 nodeSelector 的规则在集群中进行匹配，找到合适的 Node 进行调度。否则，这个 Pod 将不会被成功调度并且会报错： No nodes are available that match all of the following predicates&amp;hellip;
Affinity and anti-affinity Affinity（anti-affinity） 是对 NodeSelector 的一种功能上的扩展，NodeSelector 可以做到的东西，它一样可以做到。功能上的加强有以下几个方面：
 不仅支持对单个的 key-value pair进行匹配，还支持逻辑运算的语义。如 AND 等 设置的调度策略将分为：强制和非强制两种类型。强制类型则和 NodeSelector 的功能一样，如果匹配失败，那么也就意味着调度失败。非强制类型则优先会匹配设置好的策略，如果没有匹配成功，k8s 会自动按照它的默认策略调度 Pod 至 Node 上。 调度策略可供设置的粒度更细，不但支持 NodeLabel 粒度的，还支持 PodLabel 粒度的。这也就是说，我们不但可以根据 Node 本身的 label 设置调度策略，还可以根据目标 Node 上所运行的 PodLabel 设置。如 RedisAPP 中，主从节点的 Pod 肯定是不能被调度到一个 Node 上的。这个功能的产生，主要是考虑到了同一个 Node 上面运行的 Pod 之间会有业务上的影响  Node affinity NodeAffinity 分为两种类型：</description>
    </item>
    
    <item>
      <title>Redis Sentinel Explanation 1</title>
      <link>http://littledriver.net/posts/redis-sentinel-explanation-1/</link>
      <pubDate>Sun, 01 Apr 2018 19:49:35 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/redis-sentinel-explanation-1/</guid>
      <description>什么是 Sentinel? Sentinel 这个词在 Redis 中有很多不同的含义，它可以代表 Redis 的一种高可用的方案，也可以代表一个以 Sentinel 模式启动的 Redis 实例，甚至是可以代表你的 Redis 集群中，多个以 Sentinel 模式启动的 Redis 实例集合。我们可以这样来理解和定义 Sentinel：
 Sentinel 是一套方案。它在集群模式下为我们的 Redis 集群提供了「高可用」的保证。 Sentinel 也是一个小型的分布式系统，在多个 Sentinel Process 的协同作战下，保证了 Redis 集群的「高可用」。减少故障误报率，在部分 Sentinel Process 异常的情况下，仍能够为集群提供可靠的服务。
 Sentinel 都有哪些功能？ 上面说到，Sentinel 为我们的 Redis 提供「高可用」的保证。那么，他提供了哪些措施去实现「高可用」呢？让我们首先来看一下，在没有 Sentinel 的时候，使用一主一从的模式部署我们的 Redis 集群，可能会在使用上遇到哪些问题：
 健康检测（monitor）：我们需要一个可靠的检测机制去观察 Redis 实例的健康状态 通知（notification）：当 Redis 实例发生故障的时候，我们需要一个可靠的通知机制来告知集群的管理者 故障自动处理（failover）：一些简单的，处理方式可以被固化的故障能够自动被修复。一方面，能够最大限度的保证集群对用户的可用性，另外一方面，能够加快故障处理速度，减轻维护者的负担 负载均衡（LB）：当集群发生故障的时候，如果进行了主从切换，那么要把最新可用的 Master 节点地址通知给用户 服务发现（Service discovery）：自动的查找并监控集群内所有的实例，不需要人工去配置所有的监控关系  Sentinel 基本上是从以上五个维度对 Redis 做了「高可用」的保证。相对于 Mysql 来说，Redis 的「高可用」方案采取了和集群本身的实例分离的方式来做。也就是说，「高可用」的逻辑并没有和数据节点的逻辑混杂在一起。这一点在部署使用 Sentinel 方案的 Redis 集群就可以看出来：数据节点和 Sentinel 节点是需要分开部署的，使用的配置文件也是不一样的。</description>
    </item>
    
    <item>
      <title>k8s 之 StatefulSets</title>
      <link>http://littledriver.net/posts/k8s-%E4%B9%8B-statefulset/</link>
      <pubDate>Sat, 24 Feb 2018 17:13:44 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/k8s-%E4%B9%8B-statefulset/</guid>
      <description>Q： 什么是 StatefulSets？
A: StatefulSets 是一种 workload。k8s 中的一个 workload 通常由 CRD 和 controller 两部分构成，CRD 交由用户使用，创建资源实例，描述对资源期望的状态。而 controller 主要负责保证资源的状态与用户的期望是一致的。StatefulSets 和 deployment 有着相似的作用，提供了 pod 的部署操作和相应的扩缩容操作。但是与 deployment 不同的是：statefulsets可以保证 pod 的操作顺序，这些操作包括创建，终止，更新。在 StatefulSets 中每一个 Pod 都有一个唯一的标识符，即使内部的容器运行的 app 相同，两个 pod 也是不能够互换的。这也是 StatefulSets 可以保证 pod 启停顺序的一个原因。
Q: StatefulSets有哪些特性？他们是通过什么来保证这些特性正常的？
A:
 网络方面：通过 headless service来提供 StatefulSets 中 pod 的访问
 存储方面：通过 PersistentVolume Provisioner 来提供静态存储，最大限度保证 pod 数据的安全，即使 pod 或者 statefulsets 被删除或者更新，其中的数据也并不会丢失
 业务方面：通过 Ordinal Index + Stable Network ID + Stable Storage 来唯一的标识一个 Pod。 标识一个 Pod 的组成元素，也侧面反映了 StatefulSets 的特性。</description>
    </item>
    
    <item>
      <title>Kubernetes 之 Operator(一)</title>
      <link>http://littledriver.net/posts/kubernetes-%E4%B9%8B-operator-%E4%B8%80/</link>
      <pubDate>Wed, 24 Jan 2018 22:19:11 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/kubernetes-%E4%B9%8B-operator-%E4%B8%80/</guid>
      <description>Q: 什么是 Operator? A: Operator 在 k8s 系统中可以认为他是一个集 resource 和 controller 的结合体。他是对 resource 和 controller 的一个高度的抽象。通过扩展 Kubernetes API来达到这一效果。
Q: Operator 是如何工作的？ A: 在 k8s 组件的架构中，可以将 Operator 理解为用户和 resource 之间的一个桥梁。而用户想对 resource 做什么操作的话，需要先通过调用 API Server，将请求转发到 Operator 的身上（这里可能说的不准确， operator 是通过监听 API Server 上对于其创建的资源所做的操作来进行响应的）。通过这样的理解，我们就可以看出，operator 一方面需要管理部署在集群 node 中的应用，另外一方面需要与 API Server 进行交互，以便响应用户的需求。在 CoreOS 的官网上，同样给出了这样一个文档，里面以 etcd 这个 operator为例，描述了 operator 具体的工作模式。 Kubernetes Operators，总结下来无非就是三个步骤：
 观察资源目前的状态 对比资源期望的状态 将资源目前的状态 Fix 到期望的状态  Q: Operator 存在的意义是什么？ A: 笔者认为，从 Operator 的使用角度来讲，它最大的意义就是代替操作手册，代替人工去维护部署在集群上面的多个应用。应用的个数越多，运维这些应用的成本越高(如特定的领域知识)，越能够体现出一个 Operator 的价值。Operator 是基于 controller 的，也就是说，Operator 提供的功能会比 controller 本身更加强大，甚至是融合了一些特定业务场景的知识。</description>
    </item>
    
    <item>
      <title>TCP/IP 协议--UDP用户数据报协议</title>
      <link>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8D%B7%E4%B8%80-udp-%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Mon, 27 Nov 2017 08:41:36 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8D%B7%E4%B8%80-udp-%E5%8D%8F%E8%AE%AE/</guid>
      <description>什么是 UDP 协议 UDP 是一个简单的面向数据报的传输协议，它处于传输层中。无论是 TCP 还是 UDP 都是有端口的概念的，端口一般又和 socket 联系在一起。所以说，基本上一个进程的输出，都会对应一个 UDP 或者 TCP 的数据报。
UDP 数据报的组成 UDP 数据报一共可分为5个部分
 目的端口号 源端口号 UDP 数据报长度(首部+数据部分，最低为8B) 校验和 数据部分  目的端口号和源端口号都可以视作为对应了发送端和接收端的两个进程。UDP 数据报的长度包含了首部和数据部分，并且最小不能低于8，因为前4部分构成了 UDP 数据报的首部。这四个字段的字节数是8B。换句话说，网络中是可以传输数据部分为0字节的 UDP 数据报的。
关于校验和字段，UDP 和 TCP 数据报都会有。唯一的区别是，UDP 是可选的，TCP 是必须的。UDP 计算校验和的方式和 IP 数据报计算的校验方式一样。除此之外，为了计算校验和，UDP 或者 TCP 数据报还会包含一个伪首部部分。它包含 IP 数据报的某些内容，通过源 IP 和目的 IP，我们可以知道是否这个数据报不应该由我们这台主机来处理，协议字段可以让我们了解到，这个数据报是应该交由 UDP 端口的进程来处理还是 TCP 端口的进程。
IP 分片 当 IP 数据报的长度，也就是总长度减去首部长度超过了 MTU 大小的时候，可能就会涉及到分片的操作。分片的标准应该是按照发送端所在网络的 MTU 进行的，但是当数据报流动到了其他的网络，并且两个网络之间的 MTU 是不一样的，很可能再次发生分片操作。因为网络层的 IP 协议并不是可靠的，面向连接的。那么，当接收端的网络层接收到一堆一些被分片了但是又属于同一个数据报的报文的时候，就需要按照一定的规则将他们组装起来，提供给传输层。
 标识字段： 在 IP 数据报的首部，通常有一个16bit 的标识字段。它是内存当中维持的一个计数器。每当网络层发送一个 IP 数据报，那么这个标识字段就会被加1。一个比较大的 IP 数据报在分片的时候，原始数据报中的标识字段会被复制到各个分片的数据报中。 标志字段：在标识字段的后面紧接着3bit 的位置，有一个标志字段。当 IP 数据报发生分片的时候，除了最后一份分片的数据报之外，其余的每一片数据报都需要将某一位置为1，标识还有“更多”的分片数据报，相当于告诉接收端的网络层，这不是最后一份分片数据报。 片偏移字段：此字段是紧接着标志字段的，一共有13bit 左右。它标识了分片数据报的起始字节距离原始数据报开始处的位置是多少 总长度值：数据报被分片之后，相应分片的数据报总长度不再为原始数据报的长度，应为该分片数据报的实际长度  IP 数据报因大小问题可能会导致分片，并且在分片之后，对于接收端来说也是可以通过 IP 首部字段将这些分片的数据组装在一起的。但是这里有一个非常严重的问题，IP 分片一旦发生，甚至分片的次数越多，数据报在网络传输的过程中丢失的概率也就越大。由于 IP 协议并不为数据传输提供可靠性，当某一分片的数据报丢失，传输层的 TCP 协议很可能会重传整个数据报。如果这种出错的概率较高但是出错的分片数站总分片数的比例比较低，就会对网络造成很大的负担。并且，很多时候，如果是在通信过程中的某个路由发生分片，我们的发送端甚至都是不知情的，因为它没有任何的超时重传和确认的机制。</description>
    </item>
    
    <item>
      <title>TCP/IP 协议动态选路</title>
      <link>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8A%A8%E6%80%81%E9%80%89%E8%B7%AF/</link>
      <pubDate>Sat, 18 Nov 2017 21:13:49 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8A%A8%E6%80%81%E9%80%89%E8%B7%AF/</guid>
      <description>What is dynamic routing? 在之前的文章中，我们已经讲过静态选路的概念以及相应的行为。简单来说，静态选路，主要是路由表内容生成的方式是静态的，也就是选路策略，因为之前我们提到过，选路分为选路机制和选路策略。比如，通过 route 命令添加，通过配置文件添加，抑或是通过 ICMP 重定向报文来学习。这种静态选路适用于不同的网络之间只有单点链接并且网络本身很小的情况。
当网络的规模变大，不同的网络之间通过多个路由互联，且通信的路径也不唯一的时候，我们很自然的就需要网络之间的路由器也能够进行通信。让我们来看一张网络层的工作示意图。
这个图中很多的通信路径以及节点，相信我们都比较熟悉了。现在我们要注意图中左上角的的一个节点：routing daemon。它代表了路由的守护进程。什么是守护进程，可以戳这里了解一下守护进程-维基百科，这里面有一个比较有意思的概念叫「脱壳」。无论是具有路由功能的主机，还是路由器，在他们内部都有一个这样的 routing daemon 程序，来通过 RIP（路由信息协议）来进行通信，RIP 是内部网关协议的一种。通过告知对方路由器或者具有路由器功能的主机，自己所连接的网络情况，从而可以让接收此信息的路由器或者主机更新路由表。既然是通过别人告知信息的方式来更新路由表，那么路由表中的信息就可能会发生变动。这样一来，我们其实也就可以理解，动态选路中的「动态」也是和选路策略相关的。
RIP 选路信息协议 对于 RIP 信息协议，我们首先要了解的是，RIP 数据报的内容是包含在 UDP 数据报中的。它和 ICMP 差错报文被包含在 IP 数据报中是相似的形式。只不过 ICMP差错报文中的 UDP 首部是属于 ICMP 数据报的一部分，但是 RIP 报文显然是 UDP 数据报的一部分。 通过 RIP 的报文格式我们可以看出，首部大概有4个字节，其中command 字段是比较有用的，表明了该报文是一个请求报文还是一个应答报文。比较重要的就是图中所标识的20个字节的位置，它代表了要插入到路由表中的某一条记录的目的地址。至于一个 RIP 数据报最多可以携带多少个路由信息，其实TCP/IP 协议这本书上说的25个已经有点过时了。它采用20*25+4=504B 的计算方式，并且假定一个UDP 数据报的大小应该是512B。其实我们在今天我们携带的路由信息条数可以远不止25，但是，由于网络中的环境比较复杂，之所以规定一个标准的 UDP 数据报是512B，还是有道理的：
 以太网(Ethernet)数据帧的长度必须在46-1500字节之间,这是由以太网的物理特性决定的.这个1500字节被称为链路层的MTU(最大传输单元).但这并不是指链路层的长度被限制在1500字节,其实这个MTU指的是链路层的数据区.并不包括链路层的首部和尾部的18个字节.所以,事实上,这个1500字节就是网络层IP数据报的长度限制.因为IP数据报的首部为20字节,所以IP数据报的数据区长度最大为1480字节.而这个1480字节就是用来放TCP传来的TCP报文段或UDP传来的UDP数据报的.又因为UDP数据报的首部8字节,所以UDP数据报的数据区最大长度为1472字节.这个1472字节就是我们可以使用的字节数。:) 当我们发送的UDP数据大于1472的时候会怎样呢？这也就是说IP数据报大于1500字节,大于 MTU.这个时候发送方IP层就需要分片(fragmentation).把数据报分成若干片,使每一片都小于MTU.而接收方IP层则需要进行数据报的重组.这样就会多做许多事情,而更严重的是,由于UDP的特性,当某一片数据传送中丢失时,接收方便无法重组数据报.将导致丢弃整个UDP数据报。 因此,在普通的局域网环境下，我建议将UDP的数据控制在1472字节以下为好. 进行Internet编程时则不同,因为Internet上的路由器可能会将MTU设为不同的值.如果我们假定MTU为1500来发送数据的,而途经的某个网络的MTU值小于1500字节,那么系统将会使用一系列的机制来调整MTU值,使数据报能够顺利到达目的地,这样就会做许多不必要的操作.鉴于 Internet上的标准MTU值为576字节,所以我建议在进行Internet的UDP编程时.最好将UDP的数据长度控件在548字节 (576-8-20)以内.
 上面的这段对于数据报大小的解释，相信大家可以领会到一点就是，首先，在当今的网络环境下，我们完全可以传输数据大于等于1472B，但是这样首先会造成在网络层进行分片，其次就是即使发送的源主机所在的链路质量较高没有分片，但是通信过程中质量较低的线路往往会遇到这样的瓶颈。并且，还有一点非常重要，就是 UDP 数据报的通信是不可靠的，因为不是面向连接的，只要其中的某一个分片丢失了，那么整个 UDP 数据报都会被接收方丢弃，消耗的网络资源相当于浪费了，如果发送端应用层也会做重试处理，对网络造成的负担应该可想而知。
另外一个不可忽略的因素就是，RIP 协议在提出来的时候，时间还比较早，使用的网络也都是以低速链路为主，MTU 可能只有500B-600B，所以，去掉各种协议的头部之外，规定一个默认的 RIP 数据报大小，也是合情理的。
RIP 协议的工作方式 路由器之间使用 RIP 协议在交换路由表信息的时候，通常分为主动和被动两种形式。 其中，主动是指我们向其他路由器发送 RIP 请求报文，其他路由器同样使用 RIP 协议发送给我们一个应答报文。被动则是指，我们在没有发送请求的情况下，接收到的其他在同一网络上的路由器发来的 RIP 报文。RIP 报文内部包含的每一条路由记录信息中，都有一个叫做度量的字段，这个字段表明发送该 RIP 报文的路由器距离这个路由地址有多远，它和 TTL 一样，采用「跳数」来计数。RIP 协议中规定，度量最大为15，也就是说，如果发现RIP 协议的应答报文中有度量超过15的路由记录，则可视为这条路由是无效的。这也是 RIP 的一个重大的缺陷。由于有多个相邻路由发来其路由表的信息，所以在 RIP 响应报文到达的时候，可以根同一目的地址但是度量值小的为优先条件，从而筛选出最优的选路信息。</description>
    </item>
    
    <item>
      <title>TCP/IP 协议 IP 选路</title>
      <link>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE-traceroute-%E7%A8%8B%E5%BA%8F-2/</link>
      <pubDate>Thu, 09 Nov 2017 22:41:48 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE-traceroute-%E7%A8%8B%E5%BA%8F-2/</guid>
      <description>IP 选路通常包含两个部分：选路策略和选路机制。我们平时常说的 IP 选路，大都都指的是一个 IP 数据报如何从路由表中找到一个合适的下一跳机器的 Ip 地址，这属于选路机制的内容，即决定一个 IP 数据报向哪个接口来发送分组。另外一部分是选路策略，策略么，顾名思义就是一些规则，这些规则也就是指的路由表中的条目，即把哪些映射条目放入路由表中。
在 IP 选路的过程当中，肯定少不了要搜索路由表，它按照以下顺序进行搜索：
 搜索主机地址严格匹配的路由记录 搜索网络地址匹配的路由记录（网络号和子网号） 搜索路由表中的默认路由记录  从一个简单的路由表说起 通过 netstat -r 我们可以查看本机的路由表
路由表的第一列表明目的地址，乍一看目的地址为47.94.38.154/32的这一条可能觉得比较困惑，这是因为我们的路由表中采取 CIDR 的形式来表示 IP 地址。47.94.38.154/32 斜线的前半部分表明一个 IP 地址，后面的32表明了这个地址的前缀长度，它的前缀和它的 IP 地址长度是相同的，主机号和网络号之前的间距为0。也就是说，这一个目的地址记录的是一个主机地址。第二列表明数据报下一跳的地址。
Flags 这一列的值，大致有以下几种（仅列出与 UNIX 系统重合的部分）：
 U: 表示可用的路由 G：表示gateway 的路由是一个网关，如果不标识 G，那么说明这个路由是和本机相连在一个网络中的 H：标识该路由是一个主机 S: 表明这条路由记录是通过 route 命令加到路由表中的  在上面的4个标志中，G 是最重要的，它区分了直接路由和间接路由。当 G 出现的时候，说明发送数据报的主机并没有和目的主机直接相连，gateway 列对应的值表明其是一个间接路由。否则，认为发送数据报的主机是和目的主机直接相连的，gateway 列对应的值表明这是一个直接路由。 那么对于目的地址为47.94.38.154/32这条记录来说，它的下一跳地址是一个可用的间接s路由，并且这条路由记录是通过 route 命令条件进来的。所以在向目的主机发送数据报的时候，IP 数据报目的地址为47.94.38.154/32, 而以太网帧中的物理目的地址为192.168.2.1这个路由器的硬件地址。
通常情况下 H 标志如果被设置，那么对应记录中的目的地址是一个完整的主机地址。否则目的地址是一个网络地址，主机号部分应该为为0. 但是我们可以看到，目的地址为47.94.38.154/32这条记录，其 Flags 列中并没有 H标志，所以在 IP 选路进行路由表匹配的时候，就会匹配网络号和子网号，如果带有 H 标志，那么肯定会优先匹配完整的主机地址。值得注意的是，这条记录的即使是通过匹配网络号和子网号，事实上也会对整个目的地址进行匹配，因为目的地址 CIDR 表示形式的前缀为32，就说明目的地址所对应的子网掩码是255.</description>
    </item>
    
    <item>
      <title>TCP/IP 协议 traceroute 程序(1)</title>
      <link>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE-traceroute-%E7%A8%8B%E5%BA%8F-1/</link>
      <pubDate>Tue, 07 Nov 2017 23:04:36 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE-traceroute-%E7%A8%8B%E5%BA%8F-1/</guid>
      <description>什么是 traceroute 程序 traceroute从字面意思上来看，他是一个有着追踪功能，并且可以查看具体追踪路径的这么一个程序。实际上它的功能的确也很类似，它为我们提供了一个仔细观察 IP 数据报从一台主机到达另外一台主机所经过的所有路由。对于网络层而言，这就是一次通信的整体的路径。
其实早在了解 IP 协议的时候，IP 数据报首部字段中，有一个选项字段，这个选项字段大概有40个字节长（IP 固定首部长度为20个字节，最大60个字节），选项字段里面其实是是可以存储 IP 数据报所经过的路由信息的。比如使用ping -r 1.1.1.1 就可以开启这个功能。但是由于 IP 数据报首部选项字段长度有限，因此如果一个 IP 数据报经过的路径太长，是没办法全部存储起来的。并且，RR 选项是单向的一个功能，发送端至接收端通信路径上面的路由信息，最后都需要接收端发给发送端一个数据报携带上这些信息。这样一来一回，IP 首部的选项字段所能够存储的有效路由信息就更少了。
traceroute 程序核心武器 traceroute 程序功能的实现依赖于以下几个元素： 1. ICMP 超时报文 2. ICMP端口不可达差错报文 3. IP 数据报首部的 TTL 字段
traceroute 工作原理 当 traceroute 为了探测其运行的主机与目的主机之间的路由情况时，通常会发送一个数据报，初始的情况下，这个数据报在网络层被包装之后 TTL 值是被设为1的，那此时当到达第一个路由的时候（如果源主机没有和目的主机在同一个以太网内），TTL 值变为0，该数据报被丢弃，路由器会发送给源主机一份 ICMP 超时报文，报文中 IP 首部字段里源主机的 IP 地址就是该路由的地址。因此，traceroute 也就知道了通往目的主机路径上面的第一个路由的信息。
以此类推，IP 数据报首部的 TTL 时间逐渐增大，当到达目的主机的时候，并不会再向源主机发送 ICMP 超时报文，而是发送一个 ICMP 端口不可达报文来通知源主机，现在已经到达目的主机了。
看完 traceroute 程序大致的工作原理，相信大家是有一些疑惑的，比如：
 traceroute 在发送数据报的时候，为什么使用了 UDP 协议而不是 TCP 协议 端口不可达的 ICMP 报文究竟是怎么产生的  首先我们来说第一个, 其实在使用 traceroute 程序的时候，我们是可以指定传输层的协议的，通过-P的参数就可以指定 TCP 协议，traceroute 默认使用 UDP 协议。使用 TCP 协议通常主要想去诊断，源主机和目的主机上的某一个具体的服务连接是否有问题。因为如果你指定了—P 参数去运行 traceroute 的时候，还是会通过 TCP 三次握手建立连接的。Tranceroute 程序最主要的作用是观察源主机与目的主机之间的路由路径，应该尽可能的把数据发送出去，因为数据报本身也是探测性质的，TCP 的性能也相对来说比较差，如果没有特殊的需求，UDP 确实是一个比较好的选择</description>
    </item>
    
    <item>
      <title>TCP/IP 协议卷一之 IP 网际协议初探</title>
      <link>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8D%B7%E4%B8%80%E4%B9%8B-ip-%E7%BD%91%E9%99%85%E5%8D%8F%E8%AE%AE%E5%88%9D%E6%8E%A2/</link>
      <pubDate>Mon, 30 Oct 2017 22:09:54 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/tcp-ip-%E5%8D%8F%E8%AE%AE%E5%8D%B7%E4%B8%80%E4%B9%8B-ip-%E7%BD%91%E9%99%85%E5%8D%8F%E8%AE%AE%E5%88%9D%E6%8E%A2/</guid>
      <description> IP 数据报字段  IP 数据报长度（首部长度+数据长度)用一个16位的字段进行标识，最大数据报长度为65535，但是链路层都会对数据报进行分片处理。数据报总长度字段是需要的，链路层会读取这个字段的值，来判断是否数据报的长度达到了链路层封装数据包的最小长度，如果没有达到的话，还需要填充一些字节来保证链路层的传输效 TTL 值标识了数据报可以经过的最大路由个数，也就是这个数据报的生存时间。当 TTL 到达0的时候，该数据报被丢弃，并且向源主机发送 ICMP 报文。 IP 数据报内的首部校验和字段是对首部字段进行计算得到的一个数值。发送方对首部字段每16位进行计算，反码求和，存在校验和字段中。接收方以同样的形式进行计算，最终应该得到的值为1。如果最后值不唯1，那么由上层进行重新发送，不会使用 ICMP 报文进行报错处理。  IP 路由选择 IP 层在内存当中有一个路由表，路由表项基本上是源 IP 到下一跳 IP 映射的一条记录。一台pc 可以作为主机来使用，也可以作为路由器来使用。路由器和主机在功能上最大的区别就是，主机在收到一个 ip 数据报的时候，如果发现目的 ip 不是自己或者广播地址，那么就会直接丢弃。但是路由器会对这个数据报继续进行转发操作。所以说，一般路由器的网络模型都是只有网络层和链路层。
进行路由选择的时候，大致会遵循以下的顺序 1. 路由表中是否有与目的主机 IP 严格匹配的表项，如果有，直接使用其表项中的目的 IP 进行继续转发 2. 路由表中是否有与目的主机网络号相匹配的表项。这一般出现在有局域的时候，某一个局域网内的所有主机都可以使用某一个网络号来进行标识。这一类特性也极大的缩减了路由表的规模。 3. 路由表中是否有默认跳转的 IP 地址
IP 数据报在传输的时候，如果接收数据报的机器不是目的主机，那么数据报就不会再向上层传输，也就是说不会经过传输层。并且还有一个需要特别注意的是，在转发的过程中，链路层的硬件地址是一直在变的，唯独IP 数据报内的目的 IP 地址不会变。这一个特性其实也可以反证我们刚说的，在 IP 数据报进行路由的时候，是不会经过上层的。
带有子网划分的路由选择过程 为了减少内存当中路由表的规模，我们一般都会通过划分子网的方式来解决这个问题。当我们拿到本机(所经过路由) IP，目的主机 IP，以及我们所处的子网掩码的时候，我们的比较过程会按照如下的大致顺序。
 目的主机的网络号是否与本机的网络号相同（知道 IP 地址就知道了哪一类的 IP，从而也就知道了网络号的位数） 网络号相同则根据子网掩码分别对本机 IP 和目的 IP 进行与运算，得出的子网号看是否相同，如果相同，那么就证明目的主机就处在本机所在的子网 如果子网号不相同，说明还需要在继续查找路由表 上述过程发生在 IP 路由选择的第二步，也就说在没有找到与目的 IP 严格符合的路由表项的时候。  </description>
    </item>
    
    <item>
      <title>Python 的函数与作用域</title>
      <link>http://littledriver.net/posts/python-%E7%9A%84%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BD%9C%E7%94%A8%E5%9F%9F/</link>
      <pubDate>Sun, 08 Oct 2017 22:24:17 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/python-%E7%9A%84%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BD%9C%E7%94%A8%E5%9F%9F/</guid>
      <description>def 是什么 python 中实现一个自定义的函数，以 def 开头。类比 c，golang 这种静态类型的语言，有的是以 func 开头，有的直接省略类似的「关键字」，直接写函数签名。学习Python 的一个很大的误区，就是我们认为 def 也是Python 中定义函数的一个关键字。在Python 中，def 不是一个关键字，而是一个可执行的语句。如果不考虑类，一般来说，函数都实现在某一个模块中，那么当这个模块被导入的时候，def 语句就会自动执行，创建一个函数对象，并且把这个对象赋值给对应的函数名。也就是说，函数的定义和普通变量的定义是没有区别的，函数名仅仅是 def 语句创建出来的函数对象的一个引用而已。
python 中使用 def 语句创建的函数对象，并不要求每一个都要有返回值或者显式的 return 调用。仅当你需要这个函数的返回值的时候，才使用 return 进行返回，否则，函数默认返回一个 None 对象。既然Python 是一个动态类型的语言且没有所谓的「编译」，「链接」阶段，那么也就是说在一个模块被执行之前，某一个函数变量名具体引用了哪一个函数对象我们是不清楚的，这个关系是在模块运行的时候才能够确定的。并且，如果函数内部有一些明显的运行时的 Bug，在 def 语句执行的时候也不会去检测，只有在调用者调用这个函数发生错误的时候才能够清楚。 å 所以，在Python 中，函数变量和其他任何变量没有区别。def 是一个可执行语句，并不是一个关键字。
函数中的多态思想 如果是有编程基础的人，对多态这个词应该不难理解。我是这样理解的
 多态，就是某一个操作的意义取决于被操作的对象类型
 一个最直接的例子就是，C++中，子类和父类有一个函数签名相同的方法。定义一个父类类型的指针，当这个指针指向父类对象或者子类对象的时候，执行的同名方法是两个不同类中所分别定义的；或者在 Java 中，「+」这个操作既可以用于字符串的链接，也可以用于数字的四则运算。python 则比 java 还要方便，因为它是一种动态类型的语言，python 的世界中只有对象和引用的区别，没有数据类型之分，一切事物都是对象，被某个变量引用。所以在编写Python 的函数中，尽量不要去用类似「type」等方法检测参数的类型，这其实就是一种典型的用静态语言的思维在使用动态语言。仔细想想Python 的一个便捷之处就是在编程的时候不需要考虑数据类型，一旦某个函数内的某个操作是传递进来的参数所不具有的，让其抛出一个异常也是正确的选择。
所以，只要是在函数内部对参数做的操作，被传递进来的对象都支持，这个函数就能够正常工作，这和参数的数据类型是没关系的。
作用域 作用域这个词，从字面意思上来理解，就是某个事物起作用的区域。在编程语言中，通常指某个变量的生存周期。某个变量的作用域和它第一次被赋值的位置是相关的。
 非嵌套函数内部：本地作用域 嵌套函数内部：非本地作用域 模块内部（文件内部，函数外部，包括Python 内置模块）：全局作用域  本地作用域与全局作用域 python 中的全局作用域都是要和模块一起提出才是有意义的，这里的「全局」是指模块内全局，模块和模块之间，也就是文件和文件之间是完全隔离开的。一个我们在开发过程中所常见的现象就是某个模块的全局变量对外是作为这个模块的某个属性被使用的。本地作用域一般指调用函数所构造的一个作用域，本地作用域一般和函数调用相关。从操作系统的层面上来讲，每一个函数的调用都会开辟一块栈的空间用来存放函数调用的上下文或者一些本地变量，再结合我们一般说作用域都是和变量的生命周期联系在一起，就不难发现，每一次对函数调用，都会创建一个新的本地作用域。
各个作用域之前可能会存在一个作用域屏蔽的问题，比如在某一个函数内定义的某一个变量和函数外部的某一个变量同名，那么在函数内部对这个变量的更改将不会影响到函数外部的变量。这个现象看起来很像是一个「屏蔽」的效果，函数内的变量屏蔽的外部的。但其实这和解析变量的规则是有关的，后面的段落将会介绍相关的细节。
另外，要谨记的一点就是，只有在发生变量赋值的时候，才会涉及到作用域变化的问题。原地对一个对象的修改是不涉及到任何作用域变更的问题的。原因有以下几点：
1.Python 中，变量和对象是不同的两个概念。变量通常指的是引用变量，指向内存中实际存在的一个对象 2. 作用域这一概念是针对变量的，而不是针对对象的</description>
    </item>
    
    <item>
      <title>Data Structure Review - Circular Queue</title>
      <link>http://littledriver.net/posts/data-structure-review-circular-queue/</link>
      <pubDate>Thu, 28 Sep 2017 16:39:57 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/data-structure-review-circular-queue/</guid>
      <description>0x01 为什么需要循环队列 普通的队列结构无论在逻辑上还是在物理存储上，都是一个连续的线性结构。队列的插入和弹出操作符合「FIFO」原则。我们一般在操作队列的时候，都会有两个指针，一个指向队列头部，另外一个指向队列尾部。当我们想弹出一个元素的时候，可以清空队列头部指针所指向的元素，然后将指针向队列尾部移动一个元素的长度。当我们想插入一个元素的时候，可以在队尾指针所指向的位置放入我们想要插入的元素，然后队尾指针向后移动一个元素的长度。
一个队列初始的时候，队头和队尾指针都是指向同一个位置的，一般来说就是底层线性结构的第一个元素的位置(如数组)。如果现在只有十个元素的空间可以用于实现队列，但是我们却要求插入20个元素，中间会不定的弹出元素，这样是否可以实现呢？
乍一看10个位置想要插入20个元素是不现实的，不过中间会不定的弹出一些元素，只要是在插入一个元素之前，有一个或一个以上的元素被弹出了，就是有可能实现的。那么问题来了，插入元素的位置和弹出元素的位置是由两个不同的指针来控制的，队尾指针如果移动到了底层线性结构的边界应该怎么办？队头指针同样也会遇到这个问题。答案是要使用循环队列。
0x02 循环队列长什么样 循环队列是一个在逻辑上成环，但是物理上还是线性的一种数据结构。底层存储队列元素的结构没有变，只不过在使用这些空间上面使用了一些小的技巧。
 Circular Queue is a linear data structure in which the operations are performed based on FIFO (First In First Out) principle and the last position is connected back to the first position to make a circle.
 0x03 循环队列的实现 0x031 enqueue In [10]: def enqueue(item): ...: # queue 为空/一直插入元素到没有可用空间/循环插入后没有可用空间 ...: if queue and ((rear == len(queue) - 1 and front == 0) or (rear + 1 == front)): .</description>
    </item>
    
    <item>
      <title>Python中的引用与拷贝</title>
      <link>http://littledriver.net/posts/python%E4%B8%AD%E7%9A%84%E5%BC%95%E7%94%A8%E4%B8%8E%E6%8B%B7%E8%B4%9D/</link>
      <pubDate>Sun, 20 Aug 2017 16:48:31 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/python%E4%B8%AD%E7%9A%84%E5%BC%95%E7%94%A8%E4%B8%8E%E6%8B%B7%E8%B4%9D/</guid>
      <description>从赋值说起 之前在 python 变量相关的文章中，提到过赋值行为在 python 中和其他语言有何异同。说白了，其实就是默认传递引用，而不会拷贝整个对象。这种做法一个比较好的地方就是避免的在使用大型对象的时候，由于一些使用上的不规范而造成巨大的开销。但是凡事都有两面性，方便的同时，带来的坏处就是，同一个对象的引用可以有多个，那么只要这个对象的数据发生了变化，受影响的将是指向他的所有的引用。这种行为通常是我们不想看到的。
其实，这种行为，和 c++中的浅拷贝的行为是一直的。c++中也有引用的概念。浅拷贝通常也只是拷贝引用or 指针，而不会真正的拷贝整个对象。依稀记得，如果要在 C++中的自定义类型实现深拷贝，还需要自己实现拷贝构造函数。在 python 中，对于 list，dict 等数据结构，有一些现成的方法可以调用：
a = [1, 2, 3] b = a //shallow copy c = a[:] //deep copy, get a new object g = list(a) //deep copy. cool! d = {1: 2, 3: 4} e = d //shallow copy f = d.copy()  需要注意的是：列表和字典上面所提到的深拷贝方法，都无法拷贝嵌套结构：
In [12]: a Out[12]: [1, [2, 3, 4], 5] In [13]: b Out[13]: [1, [2, 3, 4], 5] In [14]: a[1][0] = 10000000 In [15]: a Out[15]: [1, [10000000, 3, 4], 5] In [16]: b Out[16]: [1, [10000000, 3, 4], 5]  其中， a 中的列表，及时通过[:]形式进行拷贝，仍然无法对其中嵌套的列表进行深拷贝操作。这个时候，可以使用 copy 标准库内的 deepcopy 方法，来达到嵌套深拷贝的目的：</description>
    </item>
    
    <item>
      <title>Python迭代器与解析（1）</title>
      <link>http://littledriver.net/posts/python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E8%A7%A3%E6%9E%901/</link>
      <pubDate>Sun, 30 Jul 2017 15:33:45 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E8%A7%A3%E6%9E%901/</guid>
      <description>迭代 迭代这个概念，在很多编程语言当中都是存在的。说白了，就是对一个『可迭代对象』进行遍历的过程。如 for 循环，while 循环等等，都是对一个对象进行迭代操作。那么这个『可迭代对象』到底是什么呢？
可迭代对象 简单来说，可迭代对象就是一个具有 __next__方法的对象。当这个对象被用在 for 循环等一系列迭代的场景的时候，这个方法就会起到相应的作用。如，python 当中的文件对象想按照逐行的顺序来进行迭代的话，有以下几种方式：
# 1 for line in open(&#39;test.py&#39;): print(line.upper(), end=&#39;&#39;) # 2 for line in open(&#39;test.py&#39;).readlines(): print(line.upper(), end=&#39;&#39;) # 3 while True: line = file.readLine() if not line: beak; print(line.upper(), end=&#39;&#39;)  首先第一种方式，应该是迭代一个文件对象的最优选择：
 该对象在循环中自动调用__next__方法，逐行读取文件，不会浪费内存 调用迭代器，在 python 中几乎是以 C 语言的速度在执行的  第二种方式，一个明显的缺点，就是 readlines 方法将文件中所有的内存一次性都加载到了内存中，形成了一个以每一行内容为一个元素的字符串列表。如果文本内容过大超过了机身内存的大小，很可能会出现意想不到的问题。
第三种，虽然没有内存方面的问题，但是比起在 for 循环使用迭代器进行处理，while 循环是在python 虚拟机当中运行 python 的字节码，所以在速度上，相较第一种中方式，还是差了一些。
迭代器 关于迭代的第三个概念，我们之前说过，迭代是一种行为，可迭代对象是可以在其上进行迭代行为的一个对象，也就是一个具有__next__方法的对象。之所以对于可迭代对象有这样的一个定义，是因为之前讨论的文件对象比较特殊，文件对象本身是自己的一个迭代器。也就是说，真正具有__next__方法的对象应该是迭代器，而不是我们之前所谓的可迭代对象。可迭代对象的范围应该比迭代器更大，以列表举例，它本身是没有迭代器的，但是列表依然是 python 当中可迭代的对象之一，那么列表这个对象的__next__方法从何而来呢？是通过一个叫做 iter 的方法得到的，该方法接受一个对象，返回一个具有next方法的迭代器，next方法在内部会调用迭代器的__next__方法。
所以，for 循环在处理列表这类可以迭代但是本身不是迭代器的对象时，都会将列表这个对象传递给内置函数 iter，得到一个具有 next 方法的迭代器，然后每次循环调用next方法来迭代对象。</description>
    </item>
    
    <item>
      <title>Python 中的变量、对象、引用</title>
      <link>http://littledriver.net/posts/python-%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E5%AF%B9%E8%B1%A1%E5%BC%95%E7%94%A8/</link>
      <pubDate>Sun, 30 Jul 2017 15:31:38 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/python-%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E5%AF%B9%E8%B1%A1%E5%BC%95%E7%94%A8/</guid>
      <description>&lt;p&gt;很多编程语言都有所谓的引用，对象，变量等概念。这些概念在强类型的语言中貌似并不是那么的重要，但是在动态类型的语言中，还是值得去仔细思考一下的&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning for APUE(4)</title>
      <link>http://littledriver.net/posts/learning-for-apue-4/</link>
      <pubDate>Tue, 27 Jun 2017 11:05:53 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-for-apue-4/</guid>
      <description>刷新缓存 操作系统内核在写入和读取数据的时候，从cpu到硬盘，还有很长的一段路。这段路上，为了减少数据传输的延迟，操作系统采取的很多的措施。如：就近原则的读取，内存，寄存器的使用，甚至是高速缓存，多级cache。这些办法都能够有效的提高数据传输的速率。当内核想向一个文件写入数据的时候，它并不会直接将数据写入文件所在的硬盘块，而是先将数据写入高速缓存当中，当高速缓存满了，或者说需要重用高速缓存的时候，就将高速缓存的数据写入到磁盘中。但是这个动作也不是立即发生的，写入磁盘的任务会进入一个队列中进行排队，等待处理。
为了保持高速缓存和硬盘上面的数据的一致性，就需要提供一定的接口，使得用户可以主动的发起写入数据到磁盘的这种行为。为此，linux上面提供了三个api可以实现这种效果：
 sync fsync fdatasync  三个系统调用均可以实现上面所说的功能，但是fsync是对特定的文件起作用，而fdatasync只会将文件的“数据”部分写入至磁盘，文件类型等信息是不会写入的。相反，其他两个方法是会把文件所有的信息以及数据都会写入。
操纵文件描述符&amp;ndash;fcntl fcntl这个函数的作用，就是通过参数中传递进来的文件描述符来修改文件相关的描述符标志和状态标志信息。不过一般修改标志位的方式，都是先获取目前的标志位，然后根据位或运算增加新的标志位。
首先是对于文件描述符标志位的设定，其中有一个标志位在介绍dup函数的时候也出现过，就是FD_CLOSEXEC。这个标志位设定的效果在于，此文件描述符所在进程，如果通过execl函数执行了另外一个子进程的话，那么在execl执行的这个进程中，此文件描述符即使被传递过去也无法使用了，但是如果是使用fork的话，那么在子进程中还是可以使用的。相关的一个例子，可以看看这篇文章http://blog.csdn.net/ustc_dylan/article/details/6930189。
按照之前说明的设置文件描述符标志位的规则，首先可以调用fcntl函数，使用F_GETFD命令来获取相应的标志位，使用位或运算计算之后，再使用F_SETFD命令来设置新的标志位即可。
文件描述符的信息应该是属于某一个进程的，所以它应该存在于进程表项中。除了文件描述符之外，还和进程打开文件相关的一个指标就是文件的状态标志。内核为操作系统中打开的文件维护了一张表，这张表内就存储着被打开文件的状态标志信息。之前在讨论文件共享的时候就说过，文件状态标志信息应该是和文件描述符绑定的。也就是说，进程，文件描述符，文件状态标识三者是1对多，1对1的关系。一个进程可以同时打开很多文件，所以是一对多。但是每打开一个文件，获得一个文件描述符，都和唯一的一个文件状态标识所绑定，所以是1对1的关系。
文件状态标志之前在open函数中有过说明，其中一个很重要的标志就是访问方式标志。如只读，只写，读写等等。访问方式标志的取值为0-5，每一个值代表了一种访问方式。但是他们并不是按位来排列的。所以在确定一个文件描述符对应的文件状态标识中到底是使用了何种访问标志，需要使用一个屏蔽字通过按位与的方式来取出访问标志的值到底是多少。
除了文件访问标志之外，另外一个重要且常见的标志就是O_SYNC，同步标志。这个标志可以在调用open函数的时候对文件进行设置，也可以通过fcntl函数来进行设置。但是后者在某些操作系统上面设置之后也并不会生效，所以一般都是在open函数的时候就设置好。该标志被设置之后，在想文件写入数据的时候，就不会采用系统原有的，先写入缓存，然后进队列，适当的时候再写入磁盘这套机制。而是直接采用同步I/O的形式，直到写入的数据已经落在磁盘上了，那么这次写入的操作才会返回。</description>
    </item>
    
    <item>
      <title>Thinking in Java-1</title>
      <link>http://littledriver.net/posts/thinking-in-java-1/</link>
      <pubDate>Sun, 25 Jun 2017 17:13:19 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/thinking-in-java-1/</guid>
      <description>一切皆是对象 Java编程中，最重要的不是语法，感觉是一种oop的思想。最近刚刚开始学java，但是还是感觉java这门语言在被设计的时候，还是站在解决问题的 这个角度去考虑的，并且对oop思想贯彻的非常好。相比C艹来说，java更关注解决问题，而不是效率等一些高级的问题。
Java中其实有对象和引用这两个概念，说白了，对象就是内存中的内容，而引用则可以认为是和指针一样的东西，用来标识这块内存。这样一来，在想操作这个对象的时候，我们就可以通过操作这个引用来实现。因为对象和引用两个概念是分开的，所以在Java中，引用是可以脱离对象独立存在的。如：
String s; String a = &amp;quot;xuran&amp;quot;  s就是一个还没有和特定String类型对象关联起来的一个引用，a也是一个引用，但是已经和“xuran&amp;rdquo;这个字符串对象关联起来了。 除了上述a这个引用和对象关联起来的方式之外，还可以用一种更加通用的方式：
String a = new String(&amp;quot;xuran&amp;quot;);  new这个关键字的作用就在于创建一个新的对象。将这个对象赋值给a，就相当于将a这个引用和新建的对象关联起来。因Java对象的生存周期是其内部的GC机制来控制的，所以Java通过new关键字创建的对象，都存储于堆上。引用和对象不同，在Java语言中，一个引用变量的生命周期通常都是在程序运行前就已经确定的，这也就是为什么有了作用域这一概念的原因（我理解是这样),所以Java当中的对象引用一般都是创建在堆栈上面的。堆栈上面的创建和回收操作要比堆上面的效率更高，因为堆栈中的每一项大小都是固定的，在分配和回收的时候，只需要移动栈顶指针即可。但是堆不同，它的创建和回收都比堆栈要复杂的多。
Java中对于一些基本的数据类型的存储方式上面也比较特殊，比如int类型，是一种基本的整数类型。但是在Java中同时还提供了一种Integer的类型。前者在创建的时候，采用直接声明的方式int a = 3;这种形式的创建并非创建了一个引用变量a，而是创建了一种”自动“变量，这个变量存储于堆栈中，里面存放的是赋值给他的值。而Integer a = new Integer(3);这种形式，就和上面所说的一样，是创建了一个引用变量，并且把引用和具体的对象建立了联系。并且在Java中，基本数据类型的种类要比C艹少了很多，比如没有无符号数，也没有int8，int16这种，java中所有基本的数据类型的大小都是固定的。
Java中的引用和对象由于存储位置的不同，自然会引起一个问题，那就是当引用变量脱离它的作用域的时候，这个引用变量就是不可见的，在堆栈中就已经被回收了。但是对象是存储于堆中的，这样就会造成了内存泄漏，因为创建的对象没人用，但是还没有被回收。所以Java中有一套垃圾回收的机制来处理这些问题。
Java在自定义类型中，对于一个类中的字段，可以是基本数据类型，也可以是一个对象的引用。对于一个特定类的对象在定义之后，它所具有的字段的默认值是不同的：
 如果是引用的话，那么Java将不会对它赋值一个准确的值，需要自己在构造函数中准确的初始化 如果是基本数据类型，Java会将他们赋值一个默认的0值  但是这种默认复制的行为只发生在类的成员中，在Java中，局部变量在定义的时候，如果没有赋值直接使用，也是会报错的。这一点和golang就有很大的不同，golang在这中默认赋值的行为上面是统一的。
初始化 Java中用于初始化类对象成员的方式和C艹中是一样的，默认提供了一个构造函数，但是自己可以指定带参数的，或者是重写不带参数的构造函数。一旦自定义了一种形式的构造函数之后，那么在构造这个类型的对象的时候，就必须按照这种构造函数的形式进行调用，否则会编译错误。如，你定义了带参数的构造函数，此时再想调用默认的就不行了。
对于使用构造函数来初始化类内成员的方式，需要注意一点的就是，Java对于类成员默认的初始化行为是一直存在的，即使你自己定义了构造器。举例来讲，基本数据类型的成员会变成类型对应的0值，而引用类型会被初始化为NULL。当默认的初始化行为结束之后，才会自动调用类的构造器对类成员进行第二次初始化。
除了使用构造函数可以初始化类内成员，还可以直接在类中进行初始化。如：
int a = 3; }  静态成员的初始化比较特殊，有以下几点需要注意：
 静态成员只有在需要的时候才会被初始化，”需要“的时候包括创建了类内有静态成员的对象实例，以及使用某个静态成员的时候 静态成员初始化优先于非静态成员 一个类中的静态成员，只在这个类第一个对象被创建的时候，才会被初始化。静态成员在整个程序的生命周期只初始化一次  那么这里就涉及到一个初始化顺序的问题。类内成员的初始化顺序，遵循在类中定义的顺序，但是却会在任何方法被调用之前初始化，这里是包括构造函数的。也就是说，如果类中直接初始化某几个变量，那么这些变量在构造函数调用之前就会执行它们预定的初始化操作。
在初始化一个数组的时候，如果事先知道一个数组内部的元素，我们可以直接在创建数组的时候，利用大括号初始化一个数组。这种存储的分配方式和直接使用new关键字是一样的。直接定义一个数组的引用，但是没有跟它初始化一个真正的数组对象的时候，它的值应为NULL。Java中允许数组之间的赋值行为，直接使用赋值号和C++中的浅拷贝的行为是一样的，只不过是多了一个引用指向了同一个数组对象上，所以他们的更改会相互影响。
在使用new创建一个基本数据类型的数组的时候，会自动创建一系列的基本数据类型变量，直接在堆上为其开辟了空间。但是如果是创建非基本数据类型的数组的时候，我们实际上是创建了一个引用数组，在未给这些引用链接到真是存在的对象之前，如果擅自使用数组元素，是会抛出异常的。所以，往往在创建一个非基本类型的数组之后，在使用之前还要为每一个引用都链接一个真实的对象。
方法重载 重载这个概念，在C艹里面也是存在的。简单来说，就是对名字相同的函数，赋予不同的形参列表，即形成了函数重载。在执行的时候，会根据传递参数的不同而调用不同的方法。甚至在java中，还参数顺序的不同，也可以作为一种重载的形式，这也侧面说明了，重载概念中形参列表的不同，不仅仅是在于形参的类型和个数，顺序也是衡量的目标之一。
在对方法传递基本类型参数的时候，如果目标参数的类型窄于要传递的参数，此时必须进行强制类型转换才可以进行传递。如果反过来，相当于数据类型的一个提升，是没问题的。
this &amp;amp;&amp;amp; static Java中对于成员方法的调用一般来讲，都是默认的将调用对象的引用作为第一参数传递给了该成员方法，方法内部一般如果想使用它的话，直接用this这个关键字即可。并且还可以通过this在一个构造器中调用另外一个构造器，但是这种用法需要注意的是，调用语句必须放在开头且只能嵌套一个。如果出现了形参名字和成员字段名相同的时候，为了消除歧义，就使用this来表明成员字段名。
static和this在语义上是相反的，this标明的调用当前方法的一个对象，也就是说，有this出现的方法一般是和特定对象绑定的。但是static是和具体对象无关的，它属于整个类，比较像一个类内的全局方法，任何一个对象都可以调用它，也可以通过类名直接调用。但是在static的函数内，是不能够调用非static方法的，反过来是可以的。
除了static的方法之外，还有就是static的成员字段值。一般非static的成员字段值都是跟对象绑定的，不同的对象，对于同一个类成员的字段可以有不同的值，但是static是属于整个类的，所以，任何一个对象对static成员字段值的修改，都会影响其他的对象。</description>
    </item>
    
    <item>
      <title>Learning-Process-In-Modern-Operating-System(4)</title>
      <link>http://littledriver.net/posts/learning-process-in-modern-operating-system-4/</link>
      <pubDate>Tue, 30 May 2017 11:10:32 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-process-in-modern-operating-system-4/</guid>
      <description>进程间通信(IPC) 竞争条件 在多个进程同时运行的情况下，如果他们都需要使用某一块共享内存中的数据，那么最后的结果取决于这些进程精确的执行顺序。这种情况就叫做竞争条件。日常开发中，这类的情况也是非常常见的。如两个进程都会使用某一个存储于共享内存中的变量，A进程先读到这个值，但是由于时间片用完等原因被切换到另一个进程B运行，B在运行期间改变了共享内存中那个变量的值。此时A再被调度执行的时候，显然它读到的变量的值就是不对的。尤其是算数操作，对这个变量做+1操作，那么这种情况将会导致错误的结果。
临界区 产生上述所说的竞争条件的原因是因为多个进程能够同时读取共享的资源，并且没有一定的先后顺序。为了阻止这种情况，就需要在一个进程访问共享资源的同时，另外一个进程在一旁等候。也就是说，同一时间只能有一个进程来访问某一块共享的资源。 进程中，访问这种共享资源的代码被称为临界区。在这里需要特别注意的是，临界区指的是代码而不是某个共享资源。要保证同一时间某个共享资源只能被一个进程访问，就需要调整多个进程不能在同一之间执行临界区内的代码。
忙等待的互斥 要实现进程之间临界区的互斥，有以下几种方式：
屏蔽中断 这种做法是很暴力的。即在进程进入临界区之后，立刻屏蔽来自外界的所有中断。这样一来，cpu就无法依靠中断来切换进程的执行了。除非进入临界区进程自己打开中断或者主动退出。但是，这种情况只能是在单cpu下才能够成立的。因为多cpu的情况下，即使是屏蔽中断也只是屏蔽了执行diable指令的那个cpu，其他的cpu并不知道该进程屏蔽中断的事实，因此他们所调度的进程仍然可以访问共享内存。其次，就算是在单核下面这种办法是有效的，但是它的危险性也会很高。如果一个进程屏蔽中断但是恶意的不打开中断，那么这个进程就会一直执行，操作系统有可能因为这种原因被卡死。
锁变量 这里所说的锁变量和我们之前在写程序的时候说的锁变量是不一致的。日常开发中所说的互斥锁，用于同一个进程中的不同的线程上面。锁住的也是进程内的共享资源。但是这里说的锁变量是进程级别的，因为进程之间本身就是独立的，所以要想所有的进程都能够访问一个变量，那么这个变量就是共享的。既然是共享的，那么该锁变量本身的互斥性又有谁来保证呢？显然是没办法的，这么想的话，就进入了无限递归了。
严格轮换法 严格轮换法也是依靠某一个共享变量，这个共享变量有一个初始值。多个不同的进程在可以进入临界区之前都会读取这个共享变量，如果这个共享变量的值指示当前进程可以进入临界区，那么该进程就可以正常访问，并且在离开临界区之前将此共享变量置为另外一个值。想用这种办法的一个前提就是，共享变量的取值集合数对应了共享它的进程数。每一个进程在想进入临界区之前都要忙等待轮训这个变量的值是否属于自己的那个。当离开临界区之前也要把响应的共享变量置为其他进程所需要的值才行。
//process 0 enter_region{ while(1){ while(turn != 0) continue; ... turn = 1; break; } } //process 1 enter_region{ while(1){ while(turn != 1) continue; ... turn = 0; break; } }  如上面的伪代码所示，如果现在有两个进程需要进入同一块临界区。共享变量初始值为0，那么0号进程将首先获得该临界区的访问权限，1号进程由于检查turn的值并不是属于自己可以获取权限的值，就进入循环，使用忙等待的方式，不断的访问turn变量。当0号进程执行完毕，离开临界区的时候，会将turn置为1，此时将锁的使用权限给了process1，process1得到锁之后便可以访问临界区。
目前还只有两个进程，使用起来还不是特别的麻烦。但是如果有多个进程的时候，其中一个进程执行完毕，接下来这个锁的权限给谁，看起来是依靠当前的这个进程来决定的。那么这就有一定的几率是会成环的。
另外一个缺点就是，如果0号进程把锁的权限给了1号，但是1号还没有进入临界区。那么0号就得一直等到1号进入临界区并执行完毕，然后再把锁的权限给0号。此时0号将会经历相当长一段忙等待的时间。忙等待是非常消耗cpu资源的，它也被成为是自旋锁。如果两个多个进程之间不是严格的轮换，那么这种效率是非常低的。解决临界区互斥方案的一个重要的原则，就是保证处于临界区外部的进程不应该阻止想要进入临界区执行的进程。轮换法的方案，显然在多个进程执行步调不一致的时候，违反了这一原则。并且，多个线程的情况下，锁权限传递成环也违反了不能使某些进程无限期等待进入临界区的原则。
TSL指令 TSL指令已经属于硬件层面上的互斥措施了。该指令首先将内存中的一个字lock读入到寄存器内，并且在该内存地址上插入一个非零值。读和写的操作保证是原子性的，指令结束之前，都会锁住内存总线，以此来避免其他cpu来访问这块内存。锁住内存总线和屏蔽cpu中断不同，屏蔽cpu中断只是针对执行disable指令的那个cpu而言，但是对于其他的cpu并没有影响。但是锁住内存总线就不一样了。
tsl指令在用于解决竞争条件的问题上，有以下处理方式：
 将内存中的lock的值复制到寄存器中并且将内存中的lock值置为非零值 将寄存器中的lock值与0进行对比，如果相等则证明临界区可以进入。否则继续第一步，直到寄存器中的lock值为0 退出临界区的时候，将内存中lock的值用move指令变为0即可释放临界区的访问权限。 </description>
    </item>
    
    <item>
      <title>Learning-Process-In-Modern-Operating-System(3)</title>
      <link>http://littledriver.net/posts/learning-process-in-modern-operating-system-3/</link>
      <pubDate>Tue, 30 May 2017 09:08:08 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-process-in-modern-operating-system-3/</guid>
      <description>经典线程模型 对于进程来讲，它将进程本身运行所需要的众多资源都包含在了进程的地址空间中，对他们加以管理，来保证进程的正常运行。而在进程中，真正被调度并执行一定逻辑的却是线程。一个进程被创建之后，至少有一个线程，这个线程叫做主线程。其实，借用上一篇文章讨论进程和线程的观点，线程确实可以被当做一个迷你版的进程。试想一下，多个进程运行在操作系统内，共享的资源是硬盘，cpu时间，内存等。那么多个线程在进程的地址空间内，共享的资源是操作系统分配给他们所属进程的资源。唯一一点不同的就是，进程和进程之间是相互独立的，但是多个线程之间确是在同一个进程的地址空间内的。
既然多个线程在同一个地址空间内，那么对共享的资源进行读写的时候，肯定会涉及到互斥的问题。但是操作系统给出的答案是不可能+不需要。操作系统认为，同一个进程中的多个线程为的是合作，而不是竞争。如果明确的存在竞争关系，那么还是应该借用多进程的处理模式，而不是多线程。但是在实际的开发过程中，我们知道，线程安全是你代码鲁棒性的一个重要因素，我想，操作系统的设计也是想让这部分我们自己来管理，而不是在底层就做好，假想用户的需求。
如何实现线程 用户态 优势 在用户态中实现线程以及相关的操作，一个最大的好处就是，这种方式可以在很多系统层面上不支持线程的操作系统中运行起来。在用户态中实现线程，通常有一个概念叫做runtime system。runtime管理着进程中线程的各种行为，如创建，销毁，调度等。runtime system还负责维护一张线程表，它和操作系统级别的进程表是一样的，保存了线程的各种信息。并且他还包含了一系列管理线程操作的集合，如：pthread_create, pthread_exit等。
另外一个优点就是，线程的很多操作，都可以在本地完成，不需要内核额外的支持，频繁的使用系统调用效率肯定会降低。所以在线程的操作都可以在runtime系统中进行管理的前提下，彼此之间的工作效率会比较高。并且，在用户态中实现线程，如果有一些想要扩展的功能，在用户空间扩张一些信息表以及堆栈是很方便的。相对内核来讲，由于线程的数量众多，一旦堆栈数量一多事非常麻烦的。
劣势 在用户空间实现线程的劣势，其中一个就是使用阻塞类的系统调用。线程概念的出现，就是让多个不同的线程可以配合起来一起工作。其中一个线程因为一些原因不能运行的话，runtime system就会调度另外一个线程执行。但是如果一个线程使用了一个阻塞的系统调用，那么结果将是灾难性的。这次的调用不仅仅阻塞了使用这个系统调用的线程，它是阻塞了整个进程的，也就是说阻塞了这个进程中的所有线程。这样一来就违背了线程带给我们的好处。其中一个解决办法就是将所有的阻塞系统调用改成非阻塞的，这是比较暴力的一种方法，需要改进操作系统。另外一个就是将阻塞的系统调用换成select系统调用+原始阻塞调用的集合。先通过select来判断预期的阻塞调用是否真的因为条件不满足而会被阻塞住，如果会，那么就通知runtime system来调用另外一个线程执行。等到下一次轮到那个线程的时候，再去使用select来进行检验即可。但是这样一来，一个简单的系统调用的问题就会涉及到很多复杂的更改，而且效率也很低。
另外一个劣势，是由缺页中断引起的。其实缺页中断并不能算是一个劣势，只不过在用户态实现线程的时候，如果某一个线程引起的缺页中断，但是系统却只认识其所在的进程，那么很自然，系统会阻塞整个进程，直到i/o结束，缺失内存页中的内容从硬盘中加载进来。
第三个要说明的劣势，是由runtime system包含的对线程处理的过程集合引起的。因为操作系统不认识多线程，只认识多进程。那么在线程的调度上面就没办法借助操作系统进行时间片轮转的处理方式。用户态中，线程之间的调度，如果不是从线程内部发出一个信号，如thread_yield或者thread_join等，从外部是没办法来直接从一个线程切换到另外一个线程上的。 }}
内核态 劣势 在内核态实现线程最大的劣势，就是开销大。内核在处理进程的调度以及创建和销毁的时候，开销其实就是不小的。加之线程的数量远远超过进程的数量，一方面是切换，创建，销毁这类的操作比较耗费资源，还有一方面就是，之前在用户空间中维护的线程相关的信息，如线程表等将都会由内核来维护。
优势 优势也很明显，无论一个线程是因为阻塞的系统调用，还是因为缺页中断等原因。内核都会检查其所在进程以及其他进程中的线程有无可以调度运行的，如果有的话，那么就可以进行调度，而不会影响整个进程的运行。
混合实现 其实不光是操作系统，很多现实生活中的事情也是一样。两种方法各有优劣，那么就将二者的优点结合起来，但是将缺点减半。混合实现的方式便是内核线程和用户线程两者结合起来使用。一个内核线程对应一个用户线程的集合。内核线程通过操作系统来调度处理，但是用户线程通过用户空间本身来进行调度处理。这样一来，既降低了系统资源的开销，也解决了用户态实现线程出现的一些问题。</description>
    </item>
    
    <item>
      <title>nginx Learning notes(2)</title>
      <link>http://littledriver.net/posts/nginx-learning-notes-2/</link>
      <pubDate>Wed, 03 May 2017 17:50:55 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/nginx-learning-notes-2/</guid>
      <description>本博文是春哥nginx教程的读书笔记，博文内容如有侵权可以私信我删除。 hnustphoenix@gmail.com。春哥的nginx教程地址 https://openresty.org/cn/ebooks.html
 Nginx请求过程 rewrite，access, content应该是nginx中对一个请求处理的最核心的三个阶段。但是，当我们想对一个请求做更多的控制的时候，nginx也为我们提供了更多的阶段。
post_read post_read是nginx处理用户请求进行的第一个阶段，这个阶段的指令在nginx解析完请求的头部之后就立即开始运行了。其中ngx_realip模块就是这个阶段中比较常用的，它的作用是可以通过特定的指令修改请求头的来源地址。如下面的例子：
server{ set_real_ip_from 127.0.0.1 real_ip_header X-LOCAL-IP }  这两条来自ngx_realip模块的指令，在处理请求的时候，会把来自本地的请求中的来源地址设置成X-LOCAL-IP这个头部字段所制定的ip。如果该字段的值是空或者不是一个合法的ip格式，那么是不会更改的。nginx中对于请求来源地址有一个相关的内建变量可以直接访问：$remote_addr。如果nginx配置文件中配置了上面在post_read阶段执行的指令并且在rewrite阶段中我们要使用remote_addr这个变量，此时它就是被上面的指令改过的版本。
之所以需要改动请求来源地址，是因为，如果在nginx前面还有一层代理的话，那么按照原始的方式获取来源ip地址肯定是不准确的，代理服务器一般会在代理的过程中将用户请求的来源地址写入到一个特定的http头部字段中，这样nginx在收到这个请求的时候，就可以做一个翻译，以便后续的阶段使用。post_read阶段是在nginx读取到请求的头部就开始运行的，改写请求来源地址的这类事情还是越早做越好。
server-rewrite和rewrite 以我目前所学的Nginx的知识来讲，rewrite阶段一般分为两类，一类是server之内，一类是location之内。location虽说是被包含在server内的，但是location外server内还是可以算作一种特殊的rewrite阶段&amp;mdash;-server-rewrite阶段。server-write阶段的执行顺序仅次于post-read，在rewrite之前运行。
find_config find_config阶段内，不支持其他nginx模块注册处理程序，这阶段内主要的作用就是将请求和location进行配对。配对成功之后，就会进入和请求对应的location配置块中，location中第一个经历的阶段就是rewrite阶段。
post_rewrite post_rewrite阶段和find_config阶段一样，不支持nginx模块在此阶段注册处理程序。这一阶段的主要工作是根据rewrite阶段的指令看是否要执行内部跳转操作。上一篇文章中说过rewrite指令是可以实现内部跳转的。rewrite实现内部跳转的原理实际上就是通过改写请求的url，然后使请求的处理阶段倒回find_config阶段。重新将请求的url和location块进行匹配。这个&amp;rdquo;回退&amp;rdquo;的操作之所以没有在rewrite阶段中完成的原因是，rewrite阶段可以执行多个rewrite指令，但是后面的rewrite指令的结果是会覆盖前面的。对于内部跳转这个动作来讲，rewrite阶段的rewrite指令相当于做了一次记录，记录了内部跳转的目的地，最后真正实施内部跳转操作的，实际上是在post_rewrite阶段。
之前说过，rewrite指令不仅仅可以用来Location内部，还可以用在server内Location外。这个时候进行的请求重写并没有发生在post_rewrite阶段，而是发生在server_rewrite阶段，这个阶段还没有执行find_config阶段内的配对操作，所以不能算是内部跳转，可以算作是内部重定向把，毕竟真正第一次配对的find_config阶段还没有执行。
preaccess 通过这个阶段的名字我们就可以看出来，此阶段是在access阶段前运行的。ngx_req_limit和ngx_req_zone两个模块分别控制请求的频度和并发度，他们都是运行在preaccess这个阶段中的。前面提到的可以修改请求源地址的ngx_realip模块也在这个阶段注册了处理程序，按道理来说，ngx_realip在post-read阶段来做是最合适的，post-read阶段和任何的location块都是无关的。但是如果我们想在post-head以后的处理阶段中进行需改请求源地址操作的话，就要依赖于ngx_realip在preaccess阶段注册的处理程序。修改请求源站地址的指令可以写在location中，这样在preaccess阶段中就可以执行相关的指令了。
access 这个阶段在上一篇文章中已经有所提及，是负责对请求做一些访问性的控制操作。除了ngx_access标准模块，ngx_lua中的access_by_lua指令也运行在这个阶段。
post_access post_access和post_rewrite一样，后者是运行在rewrite之后，前者则是运行在access阶段之后。post_rewrite会将rewrite阶段中的内部跳转指令综合起来，最终在该阶段只跳转一次。post_access会将access阶段中注册的多种访问控制相关处理程序综合起来，根据satisfy指令给出一个逻辑或关系的表达式，任意一个请求在access阶段有satisfy指令的情况下，最终都会根据post_access阶段给出的逻辑表达式来计算其是否能够继续被处理。虽然post_access阶段是在access阶段之后被处理，但是实际上这两个阶段是相互配合的，rewrite和post_rewrite是一样的。
try_files try_files阶段不支持其他nginx模块注册处理程序。try_files指令接受N个参数，在try_files阶段，该指令会依次检查前N-1个参数对应的文件系统的位置是否存在一个明确的文件对象。如果存在，那么就改写请求uri的值为对应的参数值，否则直接执行一个内部跳转，跳转到第N个参数所制定的url中。
try_files指令在从左向右匹配的时候，只要是有一个参数被验证在文件系统中有明确的文件对象，那么他就不继续向下匹配了，改写完请求uri之后，就直接进行后面的请求处理阶段。这么说来，在try_files匹配的过程中，对于前N-1个参数来讲都是一个或的关系。
并且，在第N个参数上，也就是前n-1个参数都没匹配到，无条件内部跳转过去的位置，除了可以指定一个uri来跳转到具体的一个location块，还可以用等号加状态码的形式(=404)，让内部跳转到这个位置的时候，直接根据状态码返回一个响应。</description>
    </item>
    
    <item>
      <title>Learning-Process-In-Modern-Operating-System(2)</title>
      <link>http://littledriver.net/posts/learning-process-in-modern-operating-system-2/</link>
      <pubDate>Thu, 27 Apr 2017 22:49:25 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-process-in-modern-operating-system-2/</guid>
      <description>有了进程为啥还要线程 上一篇和进程相关的文章讲过为啥操作系统需要进程这个概念，其实说白了，就是操作系统要干太多的事了，一个人搞不定，得让多个人一起来搞，多个人的职责不同，需要耗费的时间和资源也就不同，为了把他们的工作时间重叠起来，让他们好好的配合，提升操作系统整体的工作效率，才有了进程这个概念。这么一说，有了进程就万事大吉了啊，为何还需要线程这个概念呢？
线程，是一个轻量级的进程。其实看线程的概念就知道了，明显和进程是一回事，只不过线程本身更加轻量级。操作系统本身看做是一个整体的话，那么他有很多事需要处理，自然要分配给不同的进程。那么对于一个进程来说，它要做的事情也绝对不仅仅是很单一的，同样，在进程这个范围内，也需要有很多的工作一起进行。那么套用上面讲过的概念，一个进程自然也需要很多人帮它来完成一件很大的任务，通过内部的调度让多个人的工作可以准并行的执行，那么整个进程的效率就会非常高。这里的“很多人”指的就是线程，一种轻量级的进程。
线程的优势有以下三点：
 一个进程当中可能需要同时做很多事情才能够满足用户对系统工作效率的要求。所以需要一种轻量级进程的概念 线程比进程更加轻量级，创建，销毁以及频繁的调度在效率上都比进程进行相关的操作高出不少 在非cpu密集型的进程中，如果有了多线程的帮助，进程整体的效率会有非常大的提升。对i/o密集型的效果尤其明显 不同进程之间的地址空间是独立的，互不干扰。但是同一个进程里面的不同线程，他们是共享同一块内存地址空间的，在操作同一个文件或者资源的时候，数据一致性的问题很好处理。  一般的进程如果不刻意创建子线程的话，只有一个主线程，也成为控制线程，整个程序也被称作是单线程的程序。举一个例子来说明，多线程工作方式的好处：假设你现在想做饭，这顿饭里面包含一个汤，一碗米饭以及一个炒菜。如果你是一个人来做这些事情的话，假设每一件事情在做的过程当中都要求你全程看护，阻塞了你这个人不能做其他的事情。那么你就必须将这三件事情按照顺序一件一件的完成。其中炒菜是最慢的，不仅要摘菜，洗菜，还要切菜，最后还要炒熟。这浪费了大量的时间。菜做好了发现还有两件事丝毫还没有进展。这个时候你就会想，要是有另外两个人，在我做菜的同时帮我煲汤和做米饭就好了。于是乎，在多个人参与到这件事中之后，每个人有不同的职责。如果厨房可以支持三个人同时使用的话，那么做出这一顿饭就会非常的快。即使同一时间只允许一个人使用，那么也能够保证，在等米饭煮熟的过程当中可以把做菜和煲汤的人换进来做他们应该做的事情，这样一来，同一段时间可以做多件事情，效率也肯定比最原始的方式要高了。
通过上面的例子我们可以知道，做饭就是进程，炒菜，米饭，煲汤就是线程。允许多个人同时使用厨房就是多cpu并行，不允许就是单cpu通过调度线程。
隐藏的boss&amp;mdash;有限状态机 前面说的单线程和多线程的设计模型，其实都基于一点：我们在做一件事情的过程中。无论大小都有可能会被一些系统调用给阻塞住。如果是单线程的设计模型，那么整个cpu就一直处于空转状态，直到这个阻塞的系统调用结束。而多线程设计模型在保证了仍然使用阻塞系统调用的前提下还能够提高整件事情的效率。但是有没有另外一种方案，可以使用非阻塞的系统调用呢？
既然是非阻塞的，那说白了就是异步的，异步的你就肯定的保存上下文，保存状态。一个线程的执行情况会在多种状态下切换，那么我们就必须要把他们保存起来才行。每执行一个线程调用了非阻塞系统调用之后就可以保存下来当前状态，然后去接收新的请求。新进来的请求可能是一个全新的线程，也可能是上一个线程非阻塞系统调用的返回值。无形中，我们就为系统中多线程的执行情况维护了一套堆栈信息。同一个线程在有限的几个状态下切换，直到完成。这一套模型就被称为是有限状态机。
其实说到底，多线程的设计模型在多cpu的系统内才能够发挥出最大的优势,但是也有前提，就是这个线程所在的进程，需要做的i/o密集型的任务占比比较大。单cpu多线程虽然调度线程的开销较小，但是总之还是不能实现真正的并行。</description>
    </item>
    
    <item>
      <title>Lua Learning notes(1)</title>
      <link>http://littledriver.net/posts/lua-learning-notes-1/</link>
      <pubDate>Mon, 24 Apr 2017 18:30:28 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/lua-learning-notes-1/</guid>
      <description>Chunks 在Lua中，chunk是lua一系列语句的集合。Lua的语句结尾加分号分割，虽然语言上没有强制要求，但是每一个语句后面都加上分号会让程序的逻辑更加清晰。Lua的命令行终端为我们提供了组合运行多个chunk的方式。
 lua -la -lb 这个命令会搜索a文件和b文件，并且分别运行这两个文件中的chunk 交互模式下调用dofile函数，可以把一个lua脚本文件中包含的chunk加载进来，然后无缝的调用。  全局变量 Lua中全局变量的行为和python中的普通变量是一样的，我们可以在任何需要某个全局变量的地方直接使用它。如果该全局变量之前没有被初始化，那么这个变量的值就是nil，证明这个变量是不存在的。要删除这个变量的时候也是直接给这个变量赋值为nil即可。只要我们给某个全局变量赋值了，那么就算是创建了这个全局变量。在Lua中，创建全局变量是不要事先声明的，直接赋值即可。
命令行参数 Lua脚本在解析命令行参数的时候，以脚本名这个参数为起点，下标为0，右侧以此递增，左侧以此递减。
lua -e &amp;quot;sin=math.sin&amp;quot; script a b  构造一个命令行参数列表的时候，argv[1]为a，argv[-1]为sin=math.sin。其余的以此类推。
数据类型 Lua的数据类型和Python一样，都是动态的，不需要事先定义好某一个变量的数据类型，这点和go是不同的。Lua中也有一种预定义的数据类型叫function,那也就是说，在lua中也可以像在go中那样，把一个函数赋值给某一个变量，之后就可以像使用这个函数一样使用这个变量。
Nil 一个没有经过赋值的全局变量，他的值就是nil，类型为Nil。如果想回收或者删除某个全局变量，那么直接将这个全局变量设置为nil即可。
Booleans Lua中的布尔类型和golang中的含义有所不同。golang中的布尔类型只有true和false两种，其余的整数或者是非零值等都不会自动被转换成相对应的布尔值，但是Lua中是这样的。Lua中除了false和nil为假之外，其他值都为真，这其中包括0或者空字符串。
Strings Lua的字符串和golang中的一样，都是一经定义不允许修改。并且在Lua中，字符串可以使用单双引号来分割，特殊情况下可以使用[[&amp;hellip;]]来表示字符串，它类似python中的三引号，支持多行扩展，且不会翻译字符串中的转移序列，如果第一个字符是换行符还会被忽略掉。
Lua中当一个字符串参与算数操作的时候，会将其转换为数字，执行相应的计算。如果字符串不能成功转换成数字，那么会报错。Lua中的字符串连接符不是+，而是*..*两个点号。所以，如果你想通过链接的方式将一个数字和字符串链接成一个字符串的时候，就可以使用它。虽然Lua中字符串可以在进行算数操作的时候转换为数字，但是将一个字符串和数字进行比较是错误的，应该使用tonumber函数将一个字符串转换为数字。如果转换失败，tonumber会返回一个nil值。
表达式 关系运算 lua中的==和~=分别用于比较两个操作数相等还是不相等。==在比较同类型数据的时候会比较他们的值，比较不同类型的数据的时候会直接返回false。Lua在比较引用类型对象的时候会检查他们所引用的对象是否是同一个。nil值只会和自己相等。
逻辑运算 Lua中的逻辑运算和其他一些语言中的逻辑运算的结果是不一样的,逻辑运算符认为 false 和 nil 是假(false)，其他为真，0 也是 true。python中的and和or运算符最后计算的结果都是True或者False。但是在Lua中有一套特殊的规则：
a and b -- 如果 a 为 false，则返回 a，否则返回 b a or b -- 如果a为true，则返回a，否则返回b  一个很实用的技巧:如果 x 为 false 或者 nil 则给 x 赋初始值 v
x = x or v  但是在Lua中，not 的结果只返回 false 或者 true</description>
    </item>
    
    <item>
      <title>nginx Learning notes(1)</title>
      <link>http://littledriver.net/posts/nginx-learning-notes-1/</link>
      <pubDate>Mon, 24 Apr 2017 18:30:13 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/nginx-learning-notes-1/</guid>
      <description>本博文是春哥nginx教程的读书笔记，博文内容如有侵权可以私信我删除。 hnustphoenix@gmail.com。春哥的nginx教程地址 https://openresty.org/cn/ebooks.html
 变量插值 变量插值可以在不使用明确的字符串连接符的情况下，将字符串中的变量替换成相应的字符串，并将它和剩余的字符链接在一起。nginx中没有转义功能，也就是说必须通过不支持变量插值功能的模块将一个变量赋值为$符号，这样在其他输出语句中使用的时候，才能够正常的输出$这个值。如果在变量插值中，变量后面紧接着普通的字符，这个时候为了能够不产生歧义，将使用{}将变量括起来。
set和geo指令，除了给变量赋值的功能之外，还有创建变量的功效。如果将某个值赋值给一个之前不存在的变量，那么将会自动创建该变量。但是如果，实现没有创建就直接读取这个变量的值，那么就会报错。nginx中，创建变量和给量赋值发生在不同时期内。变量在nginx配置加载的时候就已经创建的。但是并没有做赋值。只有在使用这个变量真正需要使用的时候，才会发生相应的赋值操作。这也就随之带来了两个问题：
 运行过程中不能够创建新的变量 变量如果没有定义就开始使用，那么nginx服务器可能启动失败  nginx配置中的变量一经定义其作用范围就是整个配置，所以在不同的server模块下也是可以共享同一个变量的定义的。但是这里要注意的是， 虽然不同server模块之间可以共享变量的定义，但是变量的赋值行为却是无法共享的。如：
server{ location / { echo &amp;quot;$foo&amp;quot; } location /test { set $foo hahah echo &amp;quot;$foo&amp;quot; } }  这种nginx配置文件，是可以成功启动的。因为nginx中允许共享变量的定义。 但是当我们使用curl来访问/和/test的时候，得到的结果可以证明，$foo这个变量的在每一个location都有一个独立的副本。其实想想，前面说的共享变量定义实际上应该被认为是共享变量的声明，不同的location模块都能够成功识别$foo这个变量。但是，这个变量的值，抑或说这个变量的内存在不同的location下是相互独立的。
看到上面这个例子，可能会觉得，变量的在内存中的值是和location绑定的，不同的变量在不同的location中会有一份独立的副本存放它的值。但是事实上，一个变量在内存中的副本是和请求绑定的。如从/test运行echo_exec跳转到/，此时在/这个location块下输出foo的值，仍然是hahah。也就是说，在内部跳转的过程当中，它对一个变量使用了同一份内存副本。这就证明了，某个location中使用的变量在内存中的副本并不是和location本身强绑定的，而是和请求绑定的。
内建变量 nginx中的变量基本可以分为两类，一类是被索引的，另外一类是未被索引的。其实说白了，就是一类是可寻址的，另外一类是不可寻址的。
可寻址的变量在内存中有一个确定的位置来保存这个变量的值，我们所定义的变量都是这样的。但是一些内建变量，也就是nginx预定义的变量是没有和特定的内存位置来绑定的，对未被索引的内建变量进行读写的时候，实际上都是nginx的存取处理程序在起作用。
nginx中的存取处理程序类似于面向对象当中的get和set方法。这两个方法将读写操作都封装了起来。你不知道它在内部做了什么操作，只知道他会返回给你想要的数据。一些未被索引的内建变量在读写的时候就依赖的这种机制。如$arg_xxx这个内建变量，它就是未被索引。每次当我们想要读取$arg_变量内保存的参数值的时候，实际上都是由它的存处理程序完成的，它会动态的去解析url的参数串在我们需要的时候，而不是在一开始就把url参数串的参数都解析好了放在那里等我们取用。
Nginx中的请求 nginx中的请求分为两种：
 主请求：此类请求是由http客户端从nginx外部发起的，通过echo_exec和rewrite发起的请求也属于主请求。 子请求：此类请求和http没有任何关系，属于在Nginx内部将一个请求划分为多个子请求进行处理，是一种抽象的概念。这些子请求既可以串行的进行，也可以并行的进行。  前面说过，nginx中的变量的内存分配情况是和请求相关的。对于同名的变量，每一个请求都对这个变量有一份唯一的内存拷贝，保证了多个请求之间对同名变量的操作是独立的。这一特性不仅仅体现在主请求中，在父子请求的逻辑关系中对待同名变量也是一样的。但是这种行为是有特殊情况的，比如在ngx_auth_request模块中的auth_request指令，此时它所执行的子请求就和父请求是共享同名变量的内存的。
上面所说的受主，子请求影响的变量是用户自定义的。那么nginx内建的变量会受什么影响呢？ 如果是通过nginx的存取处理程序来处理的内建变量，在主，子请求上，大部分的内建变量都是基于当前请求的，主请求和子请求在访问这种内建变量的时候都会重新去计算内建变量的数据。但是也有少数内建变量只作用于主请求，如$request_method内建变量。
变量类型 nginx中正常的变量只有一种类型：字符串。但是同时也有两种比较特殊的类型：invalid和notfound。但定义了一个变量但是没有给他赋值的时候，那么这个变量的值就是不合法的。当一个变量不存在的时候，你直接去引用它，这个变量就是不存在的。
在使用set指令创建一个变量的时候，在某个请求中，如果此变量还没有被初始化，按理来说，此时使用这个变量的时候它保存值的类型是invalid。那么为什么在使用的时候没有报错呢？是因为set指令在创建一个变量的时候自动为这个变量创建了一个取处理程序，如果在使用这个具有明确内存空间的变量但是里面没有被初始化值的时候，nginx就会调用这个取处理程序，生成一个空值，并且缓存在这个变量对应的内存空间中。一些支持缓存机制的内建变量也是用的这样的一套机制，只不过这种内建变量，在生存周期内只会运行一次取处理程序。
nginx指令执行顺序 经过这几天对nginx的了解，我觉得nginx和我们本身在写的服务端是没有什么区别的，只不过不同服务的职责是不同的。nginx这种服务主要是处理请求的负载均衡，在请求还没有正式进入我们后面的负责业务逻辑的服务中，先做一层代理，以便我们可以从容的控制用户的请求，防止被被攻击，为后端核心服务提供稳定运行的保证。通过前面的学习，我们知道，在nginx的配置文件中，可以通过制定Location来自定义服务器的路由。那么在一个Location块中，处理这个请求的众多nginx指令是按照什么顺序执行的？一定是按照书写的顺序执行的么？
nginx在处理一个用户请求的时候，只按照不同的阶段依次处理的。nginx中请求的处理阶段一共有11个，常见的有三个。
 rewrite access content  这三个阶段的处理的先后顺序从上到下。其中我们之前看到的set指令就是在rewrite阶段运行的，而echo指令是在content阶段运行。对于多条set指令而言，他们的执行顺序是按照ngx_rewrite模块来保证和书写顺序一致的,同理多条echo指令也是一样的。
rewrite ngx_rewrite模块中所包含的指令基本都运行在请求处理的rewrite阶段。如果该模块中的指令是在server配置模块中，那么这一系列的指令将会运行在一个更早的阶段，叫做server_rewrite。一些第三方的模块如果也是在rewrite阶段执行的， 并且其指令已经嵌入到了ngx_rewrite指令序列中，那么第三方模块的指令和ngx_rewrite会按照我们书写的顺序进行执行。否则，他们之间的执行顺序是不能保证的。在nginx当中，可能会有多个模块的只能共同运行在同一阶段中，但是运行在同一阶段中的指令执行顺序并没有一个明确的规定，所以应该避免写这种依赖执行顺序的代码。
rewrite是对请求较早的处理阶段，一般在这个阶段里会对请求做一些定制化的操作，如改写头部等
access 在此阶段执行的指令，大多数都是对请求的访问性进行一些控制，如黑白名单，访问权限等等。ngx_access模块内的指令如果有多条出现在access阶段中，那么他们会顺序执行。与access相关的指令也都在ngx_access这个模块内。无论是在rewrite还是access阶段，想要插入我们自定义的lua脚本代码，都需要用到ngx_x模块中的xxx_by_lua指令。这条指令一般在rewrite和access阶段的末尾执行，也就是说ngx_x模块的指令都执行完了，最后在执行xxx_by_lua。
content content阶段的指令主要负责生成Http中的response并返回给外部请求的客户端。在rewrite阶段和access阶段中，只要是该阶段的指令，那么多个模块的指令是可以配合起来用的。比如在access阶段中的ngx_access和ngx_lua两个模块中的指令可以共存于access阶段，并且按照一定的顺序运行。但是对于content来说，这是不可行的。
在一个location块中，只能有一个所谓的&amp;rdquo;内容处理程序&amp;rdquo;。如果我们写入了多个内容处理程序，那么究竟谁会执行是无法确定的。如：
worker_processes 1; events { worker_connections 1024; } error_log logs/error.</description>
    </item>
    
    <item>
      <title>golang-net-http-package源码分析-4</title>
      <link>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/</link>
      <pubDate>Mon, 24 Apr 2017 10:34:39 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/</guid>
      <description>Transport Field  Proxy func(*Request) (*url.URL, error)  Proxy接受一个request，返回一个url.URL的对象。Proxy和ProxyEnvironment的函数签名是一样的，他们有着同样的类型。
DialContext func(ctx context.Context, network, addr string) (net.Conn, error) Dial func(network, addr string) (net.Conn, error)  上面两个Field都是函数类型的对象，调用之后可以生成一个未加密的tcp链接。但是文档中明确提示我们，应该使用DialContext来代替Dail，因为前者可以在我们不需要这个链接的时候主动将它结束。如果一个Transport类型对象中这两个属性都赋值了，那么DialContext的优先级是要比Dial高的。
DialTLS func(network, addr string) (net.Conn, error)  DialTLS的调用将会建立一个TLS的链接。如果DialTLS成员被初始化，那么对于https的request就会被DialTLS成员给Hook住，否则就由Dial和TLSClientConfig两个成员配合起来做这件事。DialTLS调用之后返回的链接默认是Tls握手已经完成的状态。
TLSClientConfig *tls.Config TLSHandshakeTimeout time.Duration  客户端上https的相关配置以及https握手的超时时间。但是在DialTLS成员被赋值了之后，这两个成员的值就将会被忽略了。
DisableKeepAlives bool  如果这个属性被置为true,那么在两次不同的请求之间就不会对同一条tcp链接进行重用了，即关闭了Keep-Alive功能。
MaxIdleConns int MaxIdleConnsPerHost int IdleConnTimeout time.Duration  MaxIdleConns属性为一个Transport对所有的host开启的最大空闲连接数，是一个总量，MaxIdleConnsPerHost为对每一个host所能开启的最大空闲连接数。IdleConnTimeout为空闲链接保持多久将会关闭它。
ResponseHeaderTimeout time.Duration  ResponseHeaderTimeout指定了一个从发送了所有的request到得到服务端发送的response的Header的超时时间。
MaxResponseHeaderBytes int64  MaxResponseHeaderBytes指定了客户端所能接受服务端发送response的最大字节数。
Method func (t *Transport) CancelRequest(req *Request) { t.cancelRequest(req, errRequestCanceled) }  cancelRequest函数内部读取了一个map[*Request]func(error)类型的map，key为request，value是request对应的cancel方法。先把request对应的cancel method拷贝出来，然后从map中删除这一条记录，最后调用这个cancel方法，关闭连接。</description>
    </item>
    
    <item>
      <title>golang-net-http-package源码分析(3)</title>
      <link>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/</link>
      <pubDate>Thu, 06 Apr 2017 23:36:39 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/</guid>
      <description>MaxBytesReader // MaxBytesReader is similar to io.LimitReader but is intended for // limiting the size of incoming request bodies. In contrast to // io.LimitReader, MaxBytesReader&#39;s result is a ReadCloser, returns a // non-EOF error for a Read beyond the limit, and closes the // underlying reader when its Close method is called. // // MaxBytesReader prevents clients from accidentally or maliciously // sending a large request and wasting server resources. func MaxBytesReader(w ResponseWriter, r io.</description>
    </item>
    
    <item>
      <title>golang-net/http-package源码分析(2)</title>
      <link>http://littledriver.net/posts/golang-net-http%E5%8C%85%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/</link>
      <pubDate>Wed, 05 Apr 2017 19:58:31 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/golang-net-http%E5%8C%85%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/</guid>
      <description>Table of Contents generated with DocToc
 DetectContentType Error Handle ServeMux ServeMux.Handle Handler ServeMux.ServeHTTP HandleFunc ListenAndServe  DetectContentType 	func DetectContentType(data []byte) string  DetectContentType函数的功能是根据data字节数组的内容来判定它的Content-Type值。该函数最多取data数据的前512个字节来进行判断，函数内部会先过滤掉一些空白字符，然后就根据一定的匹配算法来进行匹配，如果没有匹配到任何已知的类型，就会返回一个 &amp;ldquo;application/octet-stream&amp;rdquo;。
Error 	func Error(w ResponseWriter, error string, code int)  Error方法将会把用户指定的error和状态码写入ResponseWriter。但是调用这个函数的人需要确保没有进一步的信息写入到ResponseWriter。 实现代码是非常简单的，就是向response的header中写入数据：
	func Error(w ResponseWriter, error string, code int) { w.Header().Set(&amp;quot;Content-Type&amp;quot;, &amp;quot;text/plain; charset=utf-8&amp;quot;) w.Header().Set(&amp;quot;X-Content-Type-Options&amp;quot;, &amp;quot;nosniff&amp;quot;) w.WriteHeader(code) fmt.Fprintln(w, error) }  Handle 	func Handle(pattern string, handler Handler)  Handle这个方法以及HandleFunc，ServeHttp，DefaultServeMux等都是golang实现的http框架中非常重要的元素，这篇文章接下来的篇幅我将和大家请一起学习它们，以了解一个http请求是如何通过golang实现的http框架来到服务后端处理并最终返回给客户端的。
简单来说，handle这个函数就是把请求的url和处理这个请求的方法绑定到一起。它的实现如下：
// Handle registers the handler for the given pattern // in the DefaultServeMux.</description>
    </item>
    
    <item>
      <title>Learning for APUE(3)--从操作系统的角度来看文件共享</title>
      <link>http://littledriver.net/posts/learning-for-apue-3/</link>
      <pubDate>Tue, 04 Apr 2017 22:01:51 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-for-apue-3/</guid>
      <description>先谈文件I/O效率 在unix系统中读写文件会涉及到i/o操作，大家也都清楚i/o操作是非常消耗系统资源的。对于简单的读写文件来说，其i/o效率的变化是有一定规律可循的。apue第三章中提到了这样一个例子，从终端标准输入中读取数据，每次读取buffsize个字节，然后将读取到的数据写入到文件中，直到处理完标准输入中全部的数据。作者怀疑，我们选取的buffsize值可能会影响读写文件的i/o效率，所以控制buffsize大小这个变量，每次实验增加一倍的大小，直至处理完所有的数据。但是在做这个实验之前需要注意的是，进程在处理数据的时候会把它们加载到主存里，如果我们每一次实验读取的数据都是同样的，那么操作系统的缓存机制是不会将常用的数据置换出去的，它们会一直保存在内存中。这样一来，我们的测试结果就是不准确的。为了保证实验结果的准确性，每一次实验在改变buffsize大小的同时也要更换实验demo读写的文件内容。
实现demo代码如下：
#include &amp;quot;apue.h&amp;quot; #include &amp;quot;apueerror.h&amp;quot; #define BUFFSIZE 4096 int main() { int n; char buff [BUFFSIZE]; while((n = read(STDIN_FILENO, buff, BUFFSIZE))&amp;gt; 0) { if (write(STDOUT_FILENO, buff, n) != n) { err_sys(&amp;quot;write file fail&amp;quot;); } } if (n &amp;lt; 0){ err_sys(&amp;quot;read file fail&amp;quot;); } return 0; }  实验结果如下:
这张表是非常有意思的，仔细看看就能发现一些特别的规律。首先看第一列，随着buffsize的增加，用户cpu的时间逐渐减小，猜测是因为buffsize一开始选取的太小，导致需要多次的进行i/o操作。但是到了buffsize为4096个字节之后，用户cpu的时间再没有明显的变化了。由于该表是在linux ext4文件系统上测试的结果，我们自然就可以得知，ext4文件系统一个磁盘块的长度是4kb，也就是4096个字节，由于从4096个字节之后，buffsize的大小是翻倍的，也就是我们每次都读写整个一个磁盘块长度的数据，这样的消耗自然要比之前的小且稳定。
再看buffsize为32字节的时候对应的clocktime为8.82秒，但是随着buffsize的增加，时钟时间也不在有明显的变化。这是因为大多数文件系统都采用了一种叫做就近预读的技术。当系统检测到某个进程正在以一定的顺序进行读取的时候，系统将会试图读入比进程要求更多字节的数据进内存，以备使用。操作系统之所以会有这样的行为，是因为它假定了该进程会很快使用它预读到内存的数据。
文件共享 不同进程间的文件共享也是进程间通讯的一种体现。在之前的开发生涯中，我对共享，和进程间通讯的了解仅仅止步于锁的使用以及如何避免死锁等问题的层面上。apue的3.10一节，让我从操作系统+数据结构的角度窥视了系统实现文件共享的原理，着实让我看的大呼过瘾。
操作系统内核为一个打开的文件维护了三种数据结构：
 进程表项： 在操作系统中，维护着一张进程表。每一个进程在这个表里都有一个进程表项，里面保存了和该进程相关所有的信息。和我们本次讲述的主题有关的信息是文件描述符表，这张表里记录了进程操作的文件描述符标志以及文件指针等信息。
 文件表项：进程的文件描述符表中的每一项都有一个文件指针，这个指针指向了一个名为文件表项的数据结构。文件表项存在于内核维护的一张文件表中。它和进程表对系统的含义是一样的。每一个文件表项中都包括文件的状态标识（读， 写，追加等），文件当前的偏移量以及一个v节点的指针。这里需要提到的是，既然文件表和进程表都是属于内核维护的唯一的一份关于进程和文件的信息，那么也就说明，同一个进程或同一个文件在进程表和文件表中都只有唯一的一项，不会重复。
 v-node节点：在使用linux系统的时候，我了解过i-node这个节点的概念，它是系统中一个文件的索引节点，里面存在文件的所有者等相关信息，其中最重要的当属一个指向文件实际数据块存放磁盘位置的指针。文件的实际数据在磁盘中是存储于每个磁盘块中的，多个磁盘块根据链表的形式链接起来。想要找到文件存在磁盘上的实际数据，就得首先找到其i-node节点。这里的v-node节点让我感觉到了像是数据结构当中的一个链表头一样，里面存了一些与文件相关的信息，更重要的是一个指向该文件i-node节点的指针。
  通过上面的描述，我们可以很自然的画出这三种数据结构的相互关系：
当两个不同的进程打开同一个文件的时候，进程表项，文件表项，v-node节点的关系如上图所示。在这张图中，我们更加印证了：每一个进程有一个唯一的进程表项，进程打开一个文件有一个与其关联的文件表项，但是如果两个进程最终打开的文件都是一个，他们将都指向一个vnode节点。仔细想想为什么不是一个进程一个vnode节点呢？为什么不同进程打开同一个文件需要不同的文件表项呢？
第一个问题，按照之前的理解vnode节点只是去取文件落在磁盘上真实数据的一个引路人，其内部数据结构保存的信息基本是不会变的，所以并不需要每一个进程都要有一份，
第二个问题，多个进程打开同一个文件，就单以读这个操作来讲，两个进程不可能以同样的速度读取。因为在读取的过程中会改变当前文件的偏移量，而这个偏移量又仅对当前这个进程有效，所以要每个进程维护一份文件表项，其内部数据会跟随进程的操作而变化。这也就解释了，我们之前说的多进程同时操作一个很大的文件的时候，互相之间可以从文件中不同的位置开始处理但是却不会受到它人的影响的现象。</description>
    </item>
    
    <item>
      <title>Learning Process In Modern Operating System (1)</title>
      <link>http://littledriver.net/posts/learning-process-in-modern-operating-system/</link>
      <pubDate>Wed, 29 Mar 2017 22:59:50 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-process-in-modern-operating-system/</guid>
      <description>为什么会有进程这个概念 进程的模型 创建进程 终止进程 进程的层次结构 进程的三种状态  为什么三种状态之间只有四种转换    我学习操作系统，向来不喜欢死记硬背，无论是当初上大学的时候，还是现在为了夯实基础重新开始学习。我一直觉得操作系统是计算机中最有魅力的一个方向，并且坚信，现代操作系统的原型有如今这样一番样貌都是通过一点一点的改进而形成的。所以，在学习操作系统的时候，我更喜欢经常反问自己，比如为什么需要进程这个概念？为什么需要lru算法，其他的算法有什么优劣。我相信，只有你真正把一个现象想通了，并且知道了他的来龙去脉，才能最终在你的脑海里帮你形成一张操作系统的网络。
 为什么会有进程这个概念 当一台服务器同时接到很多网络请求的时候，如果现在让你设计这台服务器的操作系统处理请求的逻辑，你想怎么办呢？是想让这些请求都排着队一个接一个的处理，还是说以阳光普照的形式，给每一个请求都一个执行的的机会，即使某个请求在某次机会上并没有被完全处理。稍加思考就可以知道，以请求的发起者来考虑，肯定不想自己的请求排在后面，并且一旦位置靠前请求包含了些昂贵的操作，如i/o等，那么排在队尾耗时较少的请求很可能就会等待非常多的时间。这显然是不合理的。
为了照顾每一个到来的请求，我们必须要想出一个办法来管理这些请求的执行，切换，以及关闭。进程的概念由此提出。 在操作系统中，进程定义是一个正在运行的程序。如果接着上面的例子来讲的话，一个请求可能就会启动一个进程来进行处理，那么多个进程执行的时候就需要对进程进行切换，挂起，保存上下文等操作。就单核的计算机来讲，某一瞬间只有一个进程在执行，但是一秒之内却有很多进程在执行，这就给使用者造成了一种错觉，计算机在并行的执行我们的请求。其实上面说的通过进程间切换的方式达到的仅仅是一种伪并行的效果，严格来讲应该叫做并发。只有在硬件层面多出几个cpu的时候，才是真正意义上的并行。
进程的概念以及它的数据结构，都为操作系统对进行的操作和管理提供了极大的便利。
进程的模型 一个进程通常包括以下几部分：数据，程序，进程相关的信息。在一个多道程序的操作系统中，会在不同的进程间进行切换，每一个进程所获得执行时间可能是不同的，并且在某一个时间点也并不能确定是哪个进程在运行。进程和程序将比较起来，按我的理解，两者主要的区别就是：进程是动态的但是程序是静态的，进程不单单需要程序，还需要处理一定的数据甚至有的进程还会最终输出一些数据。cpu通过一定的调度算法去管理系统中众多进程的运行和挂起。如果同一份程序运行了两个，那么内存中将会有两个进程的信息，但是他们共享的程序代码在内存中仅有一份。
创建进程 其实在我对操作系统的了解当中，无论是用户自己主动创建的，还是操作系统创建的，亦或是操作系统本身，我觉得他们都是属于进程的。为什么会这么想呢？很简单，因为进程是正在运行的一段程序，有了这个概念你就知道，无论是什么软件还是操作系统本身都是一段在内存当中运行的程序。要创建一个进程的方式有很多种：
 用户自己主动创建，如用户点击一个软件的图标 系统初始化，操作系统在初始化的时候为了能让我们正常的使用它，肯定会启动一系列的进程 正在执行的进程创建它们的子进程 批处理作业的初始化  系统在初始化的时候会启动很多进程，这些进程有运行在“前台”的，也有运行在“后台”的。在前台的进程负责和用户进行交互，后台的进程和用户无关，但是却在后台提供着相应的服务。这类没有父进程的进程被称作是守护进程，他们一直驻留在后台，等待提供服务。
通过上面所讲的集中进程的创建方式来看，一个独立的进程通常都是被一个正在运行当中的进程通过调用一个系统调用所创建而来的（至于我刚才说的守护进程是没有父进程的，这是比较特殊的一种情况，它和unix操作系统当中的进程层级有关，涉及到一种特殊的行为叫“脱壳”），unix系统中与创建进程唯一相关的一个系统调用就是fork。
在一个进程调用了fork创建一个新的进程的时候，操作系统会创建一个有着与其父进程同样副本的新进程。在刚刚创建完成的时候，子进程和其父进程的内存空间里的内容是一样的，是直接copy了父进程内存空间中所有的数据。但是父进程和子进程的地址空间是不一样的，也就是说，两个进程在内存当中有着不同的位置，占用了两份空间。如果你确实有认真思考fork函数的行为就知道，到此为止fork只是创建了一个子进程而已，但是子进程的内存空间中存的还是父进程相关的数据以及代码，我们只有紧接着通过指定一些参数来调用execve函数，将一个新的程序load到子进程的内存空间中运行，此时才算做是真正的启动了一个新的进程。如shell一般就是先创建一个子进程，然后把我们指定的程序load到刚刚创建的进程的内存空间中进行执行。unix之所以将创建一个新进程的操作设计成两段，就是因为在执行完fork函数之后，子进程有时间调整自己对一些文件描述符的控制以及对标准输出，输入，错误等重定向的问题。这和window平台略有不同，window中都是通过调用一个CreateProcess函数一部到位完成所有的操作，因为该函数所接受的参数也是较多的。
终止进程 进程被终止的几种场景其实也很容易想到。
 程序正常执行完毕 遇到错误了，但是命中了程序正常的错误处理逻辑 严重的错误，无法执行下去 被其他进程干掉  其中前两种场景没什么好说的，都是我们预料之中的。第三种是因为一些比较严重的错误，如除数为0等，无法继续执行下去。此时正常情况来讲是应该马上终止这个进程的执行，但是在一些操作系统中允许进程自己捕获一些因某些错误系统发送的中断信号，从而按照自己的意愿来处理这个错误。
进程的层次结构 操作系统在启动的时候，必定是有一个进程来负责一些初始化工作的，然后以此创建更多的与系统相关的进程，直到系统启动完成。这个负责初始化的进程叫做init。也就是说，一个unix系统当中所有的进程都是以Init这个初始化的进程为起点创建出来的。系统中所有的进程结构类似于unix目录，是一个树形结构，根节点就是init进程。一棵树有层级之分，那么unix系统中的进程也是有层级之分的，进程之间的层级主要体现在父进程和子进程上面，一个进程只有一个父进程，但是可以有很多子进程。能操作这个进程本身的只有他的父进程。window则不同，虽然它也有父子进程的概念，但是因为父进程掌握了子进程句柄，并且可以把它传递给其他进程来操作其创建的子进程，这样一来就没有严格的进程层级关系了。
这里还要提到的，就是我们之前说过的守护进程。守护进程是一系列没有正在运行父进程的进程的集合，它们的进程层级仅位于init进程之下。之所以说守护进程没有正在运行的父进程，是因为守护进程在被其父进程创建并且开始运行之后终止了其父进程，这种行为就叫做“脱壳”。但是这种kill掉自己父进程但是自己还完好无损的行为在一些系统上是不可行的，因为有的系统一旦父进程被干掉，随之他所有的子线程也都会被干掉。
进程的三种状态 本科上操作系统课程的时候，学过进程的三态图，它标识了进程在操作系统中的三种状态：
 阻塞态 运行态 就绪态  其中就绪态和运行态之间的转换是由操作系统的进程调度程序来完成的。系统为了对所有的进程一视同仁，都给了他们一定的运行时间。所以，某一个进程不能一直占用着cpu时间，进程调度程序可以决定下一时刻哪个进程可以运行，运行多长时间，哪个进程这一轮的cpu时间已经消耗完毕要转入就绪态。相反，就绪态中的进程也会按照顺序一个一个的去享受属于他们的cpu时间执行相应的操作。
一个进程如果在运行的过程当中需要依赖一些其他程序的运行结果或者说一些外部事件发生才能够继续向下执行的时候，进程就进入了阻塞态。如果其等待条件一直没有满足，那么是有可能饥饿或者饿死的。一旦条件满足，进程就马上会进入就绪态，等待系统给他分配cpu运行时间执行相应的操作。
从进程模型的角度来考虑，在某一时刻内，系统中肯定有一个正在运行的进程，以及一些和系统相关的进程。如磁盘进程，终端进程等。和系统相关的进程此时应该都处于阻塞状态，当用户或者正在运行当中的进程出发了一些条件，就可能会唤醒阻塞的进程。操作系统中对于进程的运行，终止以及中断等操作的逻辑都在进程调度的程序当中，可以理解为进程调度这个程序是在底层的，其上层就是因为各种原因创建的普通进程，进程调度根据系统的信号管理着上面的普通进程。
为什么三种状态之间只有四种转换 按理来说，三个状态之间应该有6种转换才对。实际上，阻塞态和运行态是可以相互转化的，在cpu空闲的时候，如果一个刚刚被阻塞的进程所需的运行条件很快被满足，那么该进程是可以从阻塞态直接进入运行态的。但是就绪态是不可能像阻塞态进行转化的。因为处于就绪态的进程，首先是没有可用的cpu时间，其次阻塞态明显是一个运行的进程因为某些条件不满足而进入的下一个状态，处于就绪态的进程不可能做出任何的事情的，只有运行的进程才能被阻塞。</description>
    </item>
    
    <item>
      <title>Learning for APUE(2)--文件 I/O</title>
      <link>http://littledriver.net/posts/learning-for-apue-2/</link>
      <pubDate>Sun, 19 Mar 2017 17:01:23 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-for-apue-2/</guid>
      <description>文件描述符 对于内核而言，任何对文件的操作都需要文件描述符，因为这个文件描述符唯一标识了这个文件。文件描述符的有效性是针对某一个进程的，内核对不同的进程维护着不同的文件描述符。
open函数 	int open(const char *path, int oflag, ...); int openat(int fd, const char *path, int oflag, ...);  open函数的作用是按照用户指定的模式去打开一个文件。在模式的选择上，unix系统采用了一系列具有特定意义的常量参数进行或运算的结果来表示。还有一个和open函数相似的函数我们也应该知道，它就是openat。openat和open函数唯一的区别，就是在path参数的指定上。如果传给两个函数的path参数是绝对路径的话，openat和open两个函数的行为是一样的。但是如果path参数表示的文件名是一个相对路径名的话，openat函数与open函数相比较多出来的一个参数fd可以指出我们传递的相对路径名在文件系统中的起始地址，fd一般来说可以根据打开相对路径名所在的目录获取。openat函数中的fd参数有一个特殊值AT_FDCWD，它表示path存储的相对路径名的起始地址是当前目录，当指定了这个参数的时候，其行为和open函数是一致的。
create函数 	int creat(const char *path, mode_t mode);  create函数用来创建一个新文件。在一些老版本的系统中，由于当时open函数还没有打开一个不存在的文件则创建的功能，想要使用一个新的文件，得先调用create，然后close，最后在通过open打开这个文件获得其文件描述符进行操作。现在新的版本中，只要在open函数的oflag字段指定一个O_CREAT常量即可。
close函数 	int close(int fildes);  close函数接收一个文件描述符的参数，然后将这个文件关闭。需要注意的是，当一个进程结束的时候，内核会自动关闭这个进程打开的所有文件。
lseek函数 每一个打开的文件都有一个叫做当前文件偏移量的东西，它类似指针一样，标识了文件当中的某个位置。文件偏移量以从文件开始处到其所在位置的字节数来表明它所指向的文件的位置。对文件的读，写等操作都是从该文件的文件偏移量指向的位置开始的。如果我们不是以追加的模式打开某一个文件，一般来说，文件偏移量在文件打开的时候都会被赋值为0。
	off_t lseek(int fildes, off_t offset, int whence);  函数接收三个参数，第一个参数是将要操作的文件描述符，第二个参数是将要设置的文件偏移量的值，第三个参数最为重要，表明了我们应该怎样解释offset的值。
 SEEK_SET: 此时offset被解释为设置文件的偏移量为从文件的起始处加上offset个字节的位置 SEEK_CUR: 设置该文件的偏移量为其当前值加上offset个字节 SEEK_END: 设置该文件的偏移量为文件的长度加上offset个字节  函数的第一个参数代表了一个文件描述符，但是并不是所有的文件描述符指向的对象都可以通过lseek设置文件偏移量，如管道和网络套接字在调用lseek的时候就会报错。使用lseek函数的时候要注意，其返回的值代表了设置后的文件偏移量，但是文件偏移量有可能是负值，所以，在检测lseek是否调用成功的时候，只需要校测其返回值是否为-1即可。
SEEK_END 上面提到的whence参数的三个预设值，SEEK_END值得仔细研究一下，为什么一个文件的偏移量可以设置为比其文件长度还大的值呢。其实按我自己的理解，文件偏移量就是一个用字节数来表示其当前位置的一个指针而已，它决定了下一次对文件的读或者写的操作从何处开始。那么这个偏移量完全就可以指向任何位置，一旦使用SEEK_END,那么就代表，lseek返回的文件偏移量是从文件当前的长度开始，向后移动offset个字节的位置。
很容易想象出来，这样的一次lseek函数的调用，在当前文件偏移量的位置和原文件长度的位置之间留出了一块空地，而至于这块空地占不占用实际的磁盘存储区，要看文件系统的具体实现。
char buf1[] = &amp;quot;1234567890&amp;quot;; char buf2[] = &amp;quot;0987654321&amp;quot;; int main(void) { int f = 0; if ((f = open(&amp;quot;lseek.</description>
    </item>
    
    <item>
      <title>Learning for APUE(1)</title>
      <link>http://littledriver.net/posts/learning-for-apue-1/</link>
      <pubDate>Sun, 19 Mar 2017 12:11:38 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/learning-for-apue-1/</guid>
      <description>说来惭愧，像apue这种书籍被我买了一直放在书架里，仔细想想大学四年做的唯一一件错事就是没用大量的空闲时间去多读书。从集训队出来之后，一直被找工作困扰，失去了自己学习的节奏，现在看来是非常得不偿失的。俗话说，出来混总是要还的。在你想做一些技术含量更好的工作的时候，而不是仅仅的谢谢if-else，像apue书中所讲的一些基础知识是怎么也绕不开的。
 文件和目录 unix文件系统中，目录和文件的结构是树形的，起点是根目录 “/”。unix中目录其实也是一个文件，这个文件记录了此目录下每一个目录项的详细信息，如文件大小，文件权限，文件的最后修改时间等等。系统在创建一个目录的时候，会自动的创建两个目录项，“.”和“..”，前者表示当前目录，后者表示其父目录。在根目录中，这两者的意义自然是相同的。
在系统中，我们通过文件路径来定位一个文件。文件路径分为相对路径和绝对路径。绝对路径以根目录为起点，但是相对路径是以当前目录为起点，所以，相对路径是相对当前目录的文件路径。
输入和输出 在unix系统中如果我们对一个文件进行操作的话，自然需要知道如何才能找到这个文件。当一个进程在运行的时候，系统实际上会为他们打开三个文件，分别是标准输入，标准输入和标准错误。被进程访问的文件在进程内部通常会得到这个文件的文件描述符，文件描述符是一个非负整数，是内核用来标识一个被进程访问的文件。进程中在对该文件进行操作的时候，都可以使用这个文件描述符。
默认情况下，上面三种默认打开的文件描述符都指向终端，但是我们可以通过重定向的命令，将这几个文件描述符重定向到某个文件，这样一来，一些流入标准输出，标准错误的信息，以及要从标准输入读入的信息，他们的来源均从终端变成了某个特定的文件。
程序和进程 每一个进程在执行的时候，内核都会分配给它一个唯一的id来标识它。在unix系统中他被存在一个pid_t类型的变量里。跟进程密切相关的函数有三个：
 fork waitpid exec  fork是一个大家耳熟能详的函数，它可以在一个进程中创建一个该进程的子进程。调用一次fork函数通常会返回两次，它对父进程返回子进程的pid，对子进程返回0。fork的时候，子进程将会复制父进程的内存状态，这样一来子进程的运行起点就从调用fork函数的位置开始，而不是从整个程序的起始部分开始。
exec函数将根据接受的参数，执行一个新的程序文件用于替换旧的。
waitpid函数将会根据接受的参数，等待某个特定进程执行结束。
一个进程通常只是提供了其内部运行的一些资源，真正执行计算的是进程内的线程。线程和进程的内存模型相似，一个进程内的所有线程都共享一块进程的内存，线程也有id，但是这个id只在其对应的进程中有用。
出错处理 unix系统函数通常在调用出错的时候，都会返回一个负值。如open函数，在成功的时候会返回一个文件描述符，但是在失败的时候就会返回一个-1。由于错误的种类不同，所以在调用函数失败的时候，返回的errno值也会不一样。这个头文件中包含了函数返回errno值的预定义值。另外还有一些函数可以将这个errno值映射为文字的错误信息，如strerror。
信号 信号通常用于通知一个进程。比如一个进程如果执行了除数为0的除法的时候，系统就会发送SIGFPE信号给进程，如果该进程没有对此信号进行特别的处理，那么就会采用默认的方式，即终止此线程的执行。当然，我们也可以提供一些函数，在程序中专门用来捕捉和处理某一种特定的信号。
时间 unix系统中一般有两种时间，第一种是utc时间，它表示了从1970年1月1日到现在为止所经历的描述，用于描述文件最近的一次修改时间。另外一种是进程时间，用来衡量一个进程使用的cpu资源。unix系统一般来说会提供三个进程运行时间：
 时钟时间： 即该进程一共运行的多久，这个时间可能包括进程因cpu调度而等待的时间 内核时间： 该进程执行系统调用的时间 用户时间： 该进程执行的非系统调用的时间  系统调用和库函数的区别 我们常说的操作系统，一般包括两部分，内核和一些其他必须的软件。内核负责控制计算机软硬件资源，提供程序的运行环境。在编程中使用的系统调用，一般指的是内核的接口，它向我们开辟了一条路径，由于请求内核的相关服务。unix系统在标准的C库中为我们提供了一些和系统调用同名的函数。在使用c编程的时候，如果我们调用了一个和系统调用重名的库函数，那么这个库函数在内部将负责调用对应的系统调用函数。
虽然在使用的角度上，我们可以将系统调用看做为c的库函数。但是在实现的角度上，他们是有所不同的。比如malloc库函数，它用于分配内存，但是实际上真正提供分配内存功能的是系统调用sbrk函数。这个时候，如果我们对库函数不满意，完全可以用sbrk来实现一个自己的malloc函数。总的来说也就是，库函数可以替代，但是系统调用不可以。
系统调用函数提供给我们的往往是原始的功能，如sbrk就是分配一块内存空间，那么具体按什么方式分配甚至是如何管理这段内存空间，都是在库函数中处理的。类似一些返回系统时间的库函数也是一样，系统调用可能只返回一个从1970年1月1日到现在的秒数，至于如何解析这些秒数是库函数来决定的。所以从这两个例子可以看出，系统调用一般提供一个核心的功能，吐给库函数基本的数据，至于这些数据以何种方式使用则由库函数决定。</description>
    </item>
    
    <item>
      <title>Reflection in golang (4)</title>
      <link>http://littledriver.net/posts/reflection-in-golang-4/</link>
      <pubDate>Sun, 12 Mar 2017 13:00:47 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/reflection-in-golang-4/</guid>
      <description>Reflection in Struct tag golang中的struct类型，在使用的时候为了编码方便我们经常会给其内部的field指定一些特定的tag。这些tag不仅仅可以用来改变编码后相应字段名称，还有一些特别的作用。比如在使用Json这种数据格式的时候，我们可以为struct的field指定一个自定义的json tag，在按照json的格式编码转换的时候，该field的名字就会使用我们自定义的，而不是采取默认的将field的名字的小写形式。如果此时再加上一些特殊的tag，会改变编码时候的行为。如omiempty，在编码一个struct的时候，只要有field值为0值的时候，都不会把他们encoding到json当中。
在reflect中，我们同样可以通过strct的tag以及fieldName等属性对一个未知的struct进行操作。
Extract params to appropriate Type 在使用golang实现一些网络接口的时候，普遍需要的一项工作就是解析参数。将http请求带过来的参数解析到之前约定好的数据类型的变量内。这些变量一般都是struct类型，毕竟golang中是通过struct来实现类型的组合，以便构造自定义类型的。那么实现一个这样的结构需要哪几步呢？
 获取http传递进来的所有参数，无论是get还是post 定义好包含所有要处理参数的struct类型 用解析到http请求参数的字段名通过反射获取到对应struct类型中与其tag相等的field 将之前从请求中解析出的参数值赋值给已经找到的field  第一步完成比较容易，使用http包中的ParseForm即可将请求中传递的参数都放在req.Form中。第二步也同样简单，我们只需要将要接收的参数以及其对应的类型定义到一个struct内就行了。但是要注意的是，每一个字段tag名称都必须要和请求参数中对应的字段名相等才可以，不然我们就没法通过请求参数的字段名和将要填充的Struct类型的变量链接在一起了。整个解析参数的功能最重要的就是第三步。
我们可以在外循环遍历请求参数中的字段名，内循环遍历定义好的struct类型变量的field。但是这样效率是非常低的。稍微考虑一下就可以将定义好的struct类型的变量保存在一个map[string]reflect.Value内，map的key为struct内对应的tag，value则为对应的字段值。这样我们就可以在拿到请求参数字段名的时候，以O(1)的复杂度在这个map内找到其对应的field值，然后调用相应的Set方法即可将请求参数的值解析到具体类型的变量中。之所以能够通过构造出来的map改动定义好的struct类型变量内的field值，是因为map本身是一个引用类型，其底层使用的内存空间还是属于struct。
实现代码如下
import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/qiniu/errors&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;reflect&amp;quot; &amp;quot;strconv&amp;quot; ) func ExtractHttpReq(req *http.Request, ptr interface{}) error { m := make(map[string]reflect.Value) value := reflect.ValueOf(ptr).Elem() if value.Kind() != reflect.Struct || !value.IsValid() { return errors.New(&amp;quot;input data is invalid&amp;quot;) } for i := 0; i &amp;lt; value.NumField(); i++ { tag := value.Type().Field(i).Tag.Get(&amp;quot;http&amp;quot;) if tag == &amp;quot;&amp;quot; { tag = value.</description>
    </item>
    
    <item>
      <title>Reflection in golang (3)</title>
      <link>http://littledriver.net/posts/reflection-in-golang-3/</link>
      <pubDate>Sat, 11 Mar 2017 15:53:31 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/reflection-in-golang-3/</guid>
      <description>前景提要 上一篇文章当中，主要以实现了一个display函数为主要内容，阐述了如何通过golang的reflect包将一个自定义类型的数据格式化输出，它类似于一个简洁版的fmt.Printf函数。但是这个函数只是简单的实现了从具体类型的数据到字符串的输出功能。在日常的工作中，我们时常要通过http/https请求调用一些第三方服务的接口，传输的数据不但需要保持一定的编码格式，还需要按照被调用服务接口接收数据的规范来构造我们的请求数据。
在使用golang的过程当中，一定少不了对encoding/json包当中的json.Marshal方法的使用，他接收一个interface值, 返回一个以ASN.1格式编码byte数组。 ASN.1是一套正式且公用的编码标准，使用json.Marshal对我们的数据进行编码之后，得到的结果就可以在进程间以及服务间进行传输。编码这件事，说白了，就是把我们准备好的数据通过一定的规则转换成另外一种表现形式，比如把十进制数字转换成二进制数据，我觉得这就是一种编码。再不理解的话，也可以想想莫斯密电码，应该很容易就明白了。
今天这篇文章，将会对上次我们实现的display函数进行再一次的扩展，使其能够具备数据编码的功能.
Encoding golang的标准库内为我们提供了几种常见并且通用的编码格式，如，xml, json, asn.1等。熟悉lisp或者scheme的人肯定知道s-expression这种格式。仿照《The Go programing language》一书中的例子，我自己也实现了一个将普通数据编码成s-expression形式的函数。（由于注重的是反射的使用，所以在代码风格上面没有做过多的注意，各位看官勿喷）
在我实现的Encoding函数中，我主要处理了String, int, array, slice, struct, map, ptr等类型的数据编码，其余的如interface等类型，为了简单起见，我并没有对其做处理。编码的规范如下：
 数字的类型如int还是直接编码成对应的字符串表示。字符串类型也是一样 struct类型编码成（(fieldName fieldValue) (fieldName fieldValue) &amp;hellip;）的形式 map类型编码成 ((keyName value), (keyName value) &amp;hellip;)的形式 指针类型不做特殊处理，而是获取到其指向的变量之后，对这个变量编码。   package main import ( &amp;quot;bytes&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;reflect&amp;quot; ) func Encode(value reflect.Value, buff *bytes.Buffer) (err error) { switch value.Kind() { case reflect.Invalid: _, err = buff.Write([]byte(&amp;quot;nil&amp;quot;)) case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: fmt.Fprintf(buff, &amp;quot;%d&amp;quot;, value.Int()) case reflect.</description>
    </item>
    
    <item>
      <title>Reflection in golang (2)</title>
      <link>http://littledriver.net/posts/reflection-in-golang-2/</link>
      <pubDate>Thu, 09 Mar 2017 19:41:34 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/reflection-in-golang-2/</guid>
      <description>前景提要 在上一篇Reflection in golang的文章中，主要介绍了reflect.Type, reflect.Value, reflect.ValueOf, reflect.Type等数据结构以及接口。文章的开头我们想通过switch/case的方式来将任意类型的数据进行格式化操作。因为遇到数据类型太多，难以维护的问题，我们选择了反射当中的kind类型来解决。文章的最后给出了一个简单的demo来表明，通过reflect.Kind的帮助，我们可以将无限的数据类型归结到几个有限的基本数据类型中，使得我们的格式化操作变得比之前要简单得多。但是同样有一个问题就是，对于slice，array，map等这种复合型的数据类型的时候，我们没有办法递归的进行操作。只能简单的输出他们的类型或者是地址。这篇文章将会以此问题为起点，继续介绍reflect包中的种种特性。
Recursive display 先来看看我实现的递归输出任意类型内容的demo
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;reflect&amp;quot; &amp;quot;strconv&amp;quot; ) func display(path string, v reflect.Value) { switch v.Kind() { case reflect.Invalid: fmt.Printf(&amp;quot;%s = invalid\n&amp;quot;, path) case reflect.Slice, reflect.Array: for i := 0; i &amp;lt; v.Len(); i++ { display(fmt.Sprintf(&amp;quot;%s[%d]&amp;quot;, path, i), v.Index(i)) } case reflect.Struct: for i := 0; i &amp;lt; v.NumField(); i++ { display(fmt.Sprintf(&amp;quot;%s.%s&amp;quot;, path, v.Type().Field(i).Name), v.Field(i)) } case reflect.Map: for _, key := range v.</description>
    </item>
    
    <item>
      <title>Reflection in golang (1)</title>
      <link>http://littledriver.net/posts/reflection-in-golang/</link>
      <pubDate>Sun, 05 Mar 2017 19:40:29 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/reflection-in-golang/</guid>
      <description>工作当中需要用到go的反射相关功能，实话说，这还是第一次接触我所学语言当中的反射机制。我觉得这对我来说是一个好的现象，一个是证明了自己之前做的东西技术含量是比较低的，给自己一个警醒，另外一个是证明我现在做的东西和之前做的东西已经有很大的不同了。本次工作当中对反射的应用，我会另写一篇博文，并且将部分源码up到github上面。这篇文章就先讲一讲我学习的go反射相关的知识。
 0x001 go语言在reflect包中为我们提供了反射的功能，那么具体什么是反射功能呢？反射功能其实就是在程序运行时可以对一个变量进行一些操作，如查询，更新，但是实际上在程序的编译时期我们是并不知道这个变量的类型是什么的。这就和我们之前写golang程序的思路不太一样。golang本身是一个强类型的语言，它和js和py不同，golang所产出的程序需要编译，并且在编译的时候就会做一些静态类型的检查工作。之前写代码的时候，对于变量类型的要求也是很高的，比如类型断言，类型转换的时候。不同类型的变量有不同的方法，可以进行不同的操作，但是变量所做的操作都是在已知类型的基础上完成的。golang提供的reflect包却可以在不知道变量具体类型的前提下正确的进行它能够执行的操作。
0x002 一个能体现go反射存在意义的最好的例子就是fmt.Fprintf函数，他接收任意类型的值，包括用户自定义的。然后根据传递进来的值的类型进行格式化，最终返回一个字符串。我们先可以按照它的例子也实现一个简单的Sprint格式化函数。
	type Stringer struct { String() string } func Sprint(x interface{}) string { switch x := x.(type){ case Stringer: return x.String() case bool: if x { return &amp;quot;true&amp;quot; }else { return &amp;quot;false&amp;quot; } case int: return strconv.Itoa(x) default: return &amp;quot;???&amp;quot; } }  我们实现的函数接收一个interface的值，进入函数的逻辑之后，首先x.(type)得到x变量所包含值的具体的动态类型是什么。如果是Stringer类型，那么说明这个类型实现了String方法，可以直接调用。但是对于其他的类型，我们就需要自己来做转换。但是上面函数一个很大的缺陷，就是我们需要去维护那个switch语句，因为只要我们需要去处理不同类型的数据，我们就要在这个switch里面加入相应的case。并且，如果需要处理的类型是我们自定义，亦或是其余第三方包定义的类型，虽然他们的底层类型可能都是一些基础的数据类型，但是case里面的基础类型是不会匹配到底层类型的，而且，如果匹配的是自定义的类型，那么势必这个函数会对很多包产生不必要的依赖。
在使用反射之前，我们貌似没有任何办法去查看未知类型变量的表示。
0x003 reflect.Type go语言的reflect当中定义了两种非常重要的类型，reflect.Value, reflect.Type。其中reflect.Type可以表示一个具体的go的类型，它实现了很多的方法，可以区分reflect.Type具体包含的是哪种类型的变量，并且还可以检查对应类型一些更详细的信息。reflect.Type的实现细节和Interface类型实现相同，其内部包含了所存储数据具体的动态类型。
reflect包还提供了一个TypeOf方法，这个方法可以接受一个interface的参数，并且以reflect.Type的形式返回这个interface值内所保存数据的动态类型到底是什么。
 t := reflect.TypeOf(3) fmt.Println(t.String()) “int” fmt.Println(t) &amp;quot;int&amp;quot;  在上面代码中，我们将3以隐式转换的形式赋值给TypeOf的interface类型的参数，通过调用TypeOf函数，我们可以得出所传参数具体的golang类型是什么。当我们将3转换成一个interface值的时候，这个interface类型的参数实际上包含了两个重要的信息：
 传递给他数据的具体数据类型&amp;mdash;&amp;ndash;&amp;gt; int (动态类型) 传递给他数据的值是&amp;mdash;-&amp;gt; 3 (动态值)  这也就提醒了我们，当我们把一个具体类型的数据赋值给一个Interface类型的变量的时候，它内部实际会生成这两种信息。</description>
    </item>
    
    <item>
      <title>工作中踩过的的坑之golang的临时变量</title>
      <link>http://littledriver.net/posts/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E8%B8%A9%E8%BF%87%E7%9A%84%E7%9A%84%E5%9D%91%E4%B9%8Bgolang%E7%9A%84%E4%B8%B4%E6%97%B6%E5%8F%98%E9%87%8F/</link>
      <pubDate>Sun, 05 Mar 2017 11:26:36 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E8%B8%A9%E8%BF%87%E7%9A%84%E7%9A%84%E5%9D%91%E4%B9%8Bgolang%E7%9A%84%E4%B8%B4%E6%97%B6%E5%8F%98%E9%87%8F/</guid>
      <description>在使用golang的时候，经常在一些比较小的地方被绊倒，这些“坑”并不是什么难以理解和运用的技术，而是一些语言的细节我们没有了解清楚。这类的问题我将他统一称为golang中的“坑”。这些细小的问题容易遗漏，特此记录，以便随时翻看。
 golang中的临时变量 在使用golang的时候，我们都遇到过遍历一个集合的情况，如遍历Slice，Map等。golang遍历Slice格式如下：
	for _, item := range s { fmt.printfln(item) }  在循环的过程当中，并不是每一次循环都申请一个不同的临时变量item，而且整次循环只声明一个临时变量，在循环结束后这个变量会被gc回收。每次循环都会把Slice中的一个值赋值给item，然后输出出来。如上面代码实例中使用是没有问题的，下面来看看两种异常情况。
多个goroutine并发得到了同一个值 	import &amp;quot;fmt&amp;quot; func main() { doneChan := make(chan int) for i := 0; i &amp;lt; 10; i++ { go func() { fmt.Printf(&amp;quot;Address: %#v Value: %d\n&amp;quot;, &amp;amp;i, i) }() } &amp;lt;-doneChan }  上述代码的本意是每一个goroutine读取一个i值，随着goroutine的增加，读取到的i的地址以及i的值也会随之变化。我们期待的结果是这样的
	Address: (*int)0x11111 Value: 0 Address: (*int)0x22222 Value: 1 Address: (*int)0x33333 Value: 2 Address: (*int)0x33333 Value: 3 ...  但是实际上，得到的结果却是这样的:</description>
    </item>
    
    <item>
      <title>golang net/http package源码分析(1)</title>
      <link>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Fri, 17 Feb 2017 15:11:16 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/golang-net-http-package%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>平时在工作的过程当中对net包里面的各种库有非常多的使用，本文将先对golang标准库中net/http包进行剖析，文末会向大家展示使用golang编写的简单网络服务的运行机制。基于golang1.8版本。
 	func CanonicalMIMEHeaderKey(s string) string  函数功能描述 该函数将会按照mime的规范来格式化你传递进来的http header字符串。如你传入keep-alive，该函数就会返回一个Keep-Alive。具体的规范简单来说就是把开头的第一个字母以及如果连字符后面的第一个字母变成大写。
函数的内部会逐个字节的去检测你传递进来需要转换的http header字符串里面的字符是否合法。具体的检测规则如下：
 检查某个字符的ascii码值是否处于ascii码表中可显示字符能表示的范围内 在net/textproto包下会有一个预定义的数组，它以ascii码值为索引，true为对应的元素值。在上一条检测成功的前提下，还会利用检查的字符作为数组索引去预定义的数组中取值的方式来再次校验这个字符是否真的是合法的ascii字符。  检测函数内容如下：
// validHeaderFieldByte reports whether b is a valid byte in a header // field name. RFC 7230 says: // header-field = field-name &amp;quot;:&amp;quot; OWS field-value OWS // field-name = token // tchar = &amp;quot;!&amp;quot; / &amp;quot;#&amp;quot; / &amp;quot;$&amp;quot; / &amp;quot;%&amp;quot; / &amp;quot;&amp;amp;&amp;quot; / &amp;quot;&#39;&amp;quot; / &amp;quot;*&amp;quot; / &amp;quot;+&amp;quot; / &amp;quot;-&amp;quot; / &amp;quot;.</description>
    </item>
    
    <item>
      <title>Https通信机制</title>
      <link>http://littledriver.net/posts/https%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Thu, 16 Feb 2017 19:45:51 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/https%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/</guid>
      <description>为何要使用加密通信 这个标题看起来似乎是一句废话，谁不想自己将要在互联网传递的信息是加密的呢？但是事实并不是这样。互联网中传递的信息类型有很多种，我们谈到的信息指的是隐私信息，如用户名和密码或者是聊天记录等。 如今的web服务在应用层上大多采取http协议来进行通信，http协议是没有加密机制的，所有需要传递的信息都是明文传输。为了信息的完整性，保密性，正确性，我们需要以更安全的协议来保证通信的安全。
使用密钥加密通信的内容 我们很容易想到使用对称加密的方式，客户端和服务端共享同一个密钥，客户端和服务端之间通信的时候使用此密钥来加密通信的内容，当加密的信息到达目的地的时候，需要使用相同的密钥解密才能读取出信息。 但是处于互联网中的通信双方彼此之间的距离是很远的，很难做到在线下交换需要共享的密钥。线上传输的方式也并不安全，所以人们对加密方式进行改进，使用公开密钥的方式进行加密。
公开密钥加密属于非对称的加密方式。服务端发布一个公开密钥，这个密钥是所有的人都可以拿到的。客户端在想和服务端通信之前，用服务端发布的公开密钥加密要传输的数据。服务端保存了一个私有密钥，这个密钥只在服务端存在。当服务端接受到客户端发来加密的数据的时候，使用私钥进行解密以得到客户端想要传输的数据。这种非对称加密的方式改进了共享密钥中无法安全传递加密密钥的问题。但是，公开密钥也有一个缺点：客户端无法验证它得到的密钥就是其想通信的那个服务端发布的公开密钥。也就是说，公开密钥加密的通信方式存在着遭受中间人攻击的风险。中间可以伪装成合法的服务端与客户端进行通信。
用证书来证明你的身份 既然公开密钥加密的方式的弊端是无法验证公开密钥的发布者是否合法。那么就需要借助第三方证书机构颁发的证书来证明其身份。第三方的证书机构很多，如果你遵循一定的协议，自己也可以成为一个证书的颁发机构。但是你建立的证书颁发机构是不受信任的。
服务端会将自己想发布的公开密钥发送给证书机构，证书机构将此公开密钥集成到将要颁发的证书内。但这时大家可能会有疑问，我们如何得知客户端得到的这个服务端的证书是合法的呢？当然也是通过一定的验证机制。但是因为证书验证是否合法的过程也需要密钥的传输，问题好像就进入了无限的递归之中。所以，一般的浏览器在发布版本的时候，都会把受信任的证书机构的密钥集成在浏览器中。当客户端接受到了服务端的证书的时候，会使用该密钥来验证证书的完整性和合法性。如果验证通过，那么客户端将会得到两个信息：
 该证书是由合法的证书机构颁发的，收信任的 随证书附带的服务端公开密钥也是合法的  HTTPS的通信过程 Https的通信分为两部分，第一部分是建立https连接，第二部分是在建立的安全连接上进行通信。https通信之所以安全是因为它使用密钥对通信内容进行了加密，对通信双方的身份进行确认，一旦通信的内容有被篡改会立刻知道。
HTTPS链接的建立  客户端首先发送一个client hello的报文，主动与服务端建立https连接。报文内包含客户端支持的ssl协议的版本以及一些加密组件。最重要的是，报文内会包含一个随机数，用于之后的通信加密。
 服务端接收到客户端发来的client hello的报文之后，会返回一个server hello的报文，里面包括了服务端支持的ssl协议的版本以及通过客户端加密组件筛选出来的共同支持的部分组件，同样，报文内会包含一个随机数。之后，服务端会把带有公开密钥的证书发送给客户端。最后，发送一个server hello done报文表示第一次握手成功。
 客户端接受到服务端发来的证书证书之后会对证书进行验证。验证成功之后会取出随证书携带的服务端公开密钥。此时客户端会再向服务端发送一个随机数，这个随机数已经是链接建立过程当中发送的第三个随机数，又称为pre-master-key。这个报文发送给服务端之后，通信的双方都各自拥有了三个随机数，他们会根据这三个随机数生成一个用于之后通信的对称密钥。然后，客户端还会给服务端发送一个ChangeCipherSpec报文来通知服务端接下来的通信加密方式和使用的密钥会改变。最后，客户端发送给服务端一个Finished报文，此报文包含了从建立连接开始一直到本次之前客户端发送报文的整体校验值。服务端会拿到这个报文的内容进行校验。
 服务端接收到客户端发来的报文之后，向客户端发送ChangeCipherSpec报文，表明之后服务端向客户端的通信当中通信加密的方式和使用的密钥也会发生改变。最后再发送一个Finished报文，代表https链接正式建立成功。
  HTTPS通信 基于上面https链接的成功建立，接下来的通信过程中使用共享密钥的方式进行加密。整个通信过程都会受到ssl的保护。
HTTPS混合了两种加密方式 https在整个通信的过程当中使用了两种加密方式。利用公开密钥加密传输以后通信过程当中使用的共享密钥。因为公开密钥加密方式属于非对称加密，加密和解密次数多了自然会影响性能。我们可以用公开密钥的加密方式来传输一个共享密钥，保证了共享密钥传输的安全性之后就可以使用共享密钥加密来传输数据，从而提高了性能。</description>
    </item>
    
    <item>
      <title>RabbitMq的数据持久化</title>
      <link>http://littledriver.net/posts/rabbitmq%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/</link>
      <pubDate>Tue, 03 Jan 2017 10:10:25 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/rabbitmq%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/</guid>
      <description>我们一般在异步系统的实现中，选择使用消息队列.其中一个比较重要的原因就是担心数据储存在内存中会因为物理上的影响而丢失。消息队列中都有使消息持久化的机制。本次我在项目中应用rabbitmq的时候，也有消息持久化的需求，结合实际经验，介绍一下rabbimq中消息持久化的原理。
为什么需要持久化 一般来说，异步的系统在处理某一个任务的时候，都不会一次性将任务当中所有的逻辑处理完。很多时候会因为服务重启，优先级高低，甚至是硬盘损坏等原因而影响了任务的正常运行。既然有中断，就证明处理中的任务会丢失。如果不对消息持久化的话，服务重启之后可能就忘记了之前处理过但是没处理完的任务，导致了系统中有很多的垃圾任务存在。一个基本的异步系统是需要在服务遇到问题重启之后，将之前未完成的任务重新加载继续执行的。
诚然，这种功能可以通过数据库来实现。但是，一方面利用数据库重新实现一个消息队列的时间较长，功能不完善，还有一个就是，异步系统中任务数量一旦增多，优先级和并发度也是需要我们考虑的。综合来讲，使用rabbitmq这种消息队列是非常适合的。
rabbitmq消息持久化三要素 持久化的队列和交换器 rabbitmq所在的服务器可能会因为各种情况而重启，如果不做设置的话，重启前建立的交换器和队列在重启后都会消失，不会重新建立。在创建队列和交换器的时候，可以设置durable的属性为true，即可实现队列和交换器的持久化。每次rabbitmq因故重启的时候，都会重新创建这些队列和交换器，而不需要我们再主动创建。
持久化的消息投递模式 现在我们有了持久化的队列和交换器，如果想实现消息持久化的话，需要将我们投递的消息也改成一种“持久化的模式”。在send消息的时候，将消息的投递模式设为常数2，此时该消息被标记为持久化的消息。
投递消息到持久化的队列和交换器中 有了持久化的消息，队列，交换器，现在要做的就是将持久化的消息投递到持久化的交换器，最终达到持久化的队列。如果持久化的消息投递到了非持久化的队列或者交换器，很有可能因为rabbitmq重启而导致丢失。
消息持久化的实现 持久化消息在被发送到持久化的交换器之后，消息的相关信息会被写入一个持久化的日志文件里，这个日志文件是存储在硬盘的。只有这个消息被写入了这个日志之后，你的发送消息的操作才会得到响应。消息到达队列之后，如果在未消费之前，rabbitmq重启，此时队列和交换器都会重新自动建立，然后读取持久化日志中的消息，重新插入到相应的队列或交换器中。正常被消费掉的消息在持久化日志里面的记录会被标记为待收集处理的状态，这里的正常消费是只消息被拿走处理并且被ACK掉的。
最后的手段 如果你觉得有了上面的手段就可以保证消息肯定不会丢失，那确实是为时过早了。根据上面提供的方法，你是否注意到，在消息写入到持久化日志之前如果出现异常，是不是也会出现消息丢失的情况呢。所以说，最靠谱的保持消息不丢失的办法就是，在消息没有被正常保存到持久化日志之前通知消息的生产者，让它重发一遍不就行了么？直到消息真正的被持久化起来。
rabbitmq在信道这一层级为我们提供了一个发送确认的模式，当我们使用的信道开启了这种模式的时候，消息在到达了持久化队列并且成功写入了持久化日志之后会给消息的生产者发送一个“确认接受”的消息，生产者接受这个消息之后可以根据自定义的回调函数才执行一些逻辑。同样，如果消息没有被正确的投递或者持久化，rabbitmq也会发送一个nack的消息给生产者，代表这个消息已经丢失了，可以在相应的回调函数中来决定是否要重发这条消息。</description>
    </item>
    
    <item>
      <title>Review 2016</title>
      <link>http://littledriver.net/posts/%E6%80%BB%E7%BB%93-%E6%9C%9F%E6%9C%9B-2016/</link>
      <pubDate>Thu, 29 Dec 2016 20:58:59 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/%E6%80%BB%E7%BB%93-%E6%9C%9F%E6%9C%9B-2016/</guid>
      <description>我的一生，是抗争的一生。
 Review Plan 整个第四季度都在忙，今天终于有空能够好好总结一下这即将过去的一年。仔细想想自己还有几天就要跨入24岁的高龄，不由得虎躯一震，感叹时光飞快。话不多说，来看看我今年年中的时候给自己制定的这一年的规划吧。
 坚持记账，规范用钱习惯。这一个目标从毕业到目前为止贯彻的还是比较不错的。无意外情况，每个月都能够存下4000块，小金库目前余额12k。之所以制定这条目标的原因，并不是要去尽力的省钱，而是如目标中所写，要规范自己的用钱习惯，看看自己一个月来的工资都花到了什么地方。半年来下，几个花销比较大的项目：吃，书，电子产品。我自己一向对吃的东西要求较高，每个星期酸奶+牛奶要喝很多，水果和零食也比较多，算是恩格尔系数比较高的人群。书这半年买了很多，但是仍然是买书如山倒，读书如抽丝。电子产品，这半年新添了一个BOSE25的耳机，一台PS4，然后又买了几个PS4上面的游戏。总体来说，没有不理智的消费，这条目标完成度我给自己打95分。 坚持健身，减重130。定这个目标的原因是因为过年的时候检查身体，发现自己有轻微的高血脂，体重也达到了150斤。觉得身体健康不能忽视，第三季度的时候，健身比较多，饮食上也控制的不错，体重降到了140斤。但是第四季度，疯狂加班三个月，健身几乎没有了。下一年第一季度要继续努力。无论是工作还是梦想，首先都要有一个好的身体，身体没了什么都没了。这条目标完成度我给自己打70分。 学会游泳。这个目标制定的初衷和上一条一样，为了保持身体健康，第三季度游泳较多。成功学会了蛙泳。第四季度很少去游泳，给自己打85分吧。 学习数据库的操作和实现，mongo，redis。很遗憾，这条目标几乎是没有完成。我反思了一下，一个是工作太忙，导致我几乎没有自己的时间去学习。另外一个原因可能是我还是有一些时间没有利用起来。这条目标的完成度我给自己打0分。 对golang有更深入的了解。这半年来，我在编写golang代码的时候，是非常注重代码风格的。至于其他方面，确实没有了解深入，甚至一个标准库的源码都没有看过。一些高级的用法也还没有学会。这条目标的完成度我给自己打50分。 读完书单上的书。ok，既然说到读书，就来看看我给自己拉的书单吧。cdn技术详解只看了2章，软技能-代码之外的生存指南是黄总所赠，只看了40%，go语言圣经看了15%，go源码分析0%， 图解http协议100%， go web编程%0， 算法第四版只看了20%，csapp和apue 0%。总体来说，完成度非常低，客观原因确实没有太多的时间，主观上给自己定的目标过高了。在计划17年的读书计划时要量力而行才是。这条目标的完成度给自己打40分。 睡觉前不看手机和电脑。这一条是因为有一段时间，觉得自己睡了很久但是非常疲劳，所以决定改善一下睡眠习惯。大概坚持了一个月吧，就放弃了。完成度给自己打30分。 学习python。完成度是0%。恩，一个是中途觉得自己定这个目标是太贪心了，还有一个就是觉得应该先把golang玩熟。平时也没有找到合适的project来练习，所以完成度不好。 复习数据结构知识。这一条是我感到最遗憾的，这半年我是跟了coursera上面的普林斯顿大学的算法第四版的公开课，目前还是只完成第第二周的大作业和课程。感受较深的还是，学习算法需要一定的时间来保证，断断续续其实很难有所收获。这一条目标会移动到下半年继续完成。目前的完成度，20%吧。 在linux下熟练工作。恩，怎么说呢，这个目标当初定的也比较飘。没有把目标实体化到可以做的地步。本来想学习一下vim和shell脚本的使用，但是这两样都没有完成。这个目标仍然是需要继续努力的，但是在下半年的计划中，要指定的更加细化一点才对。完成度给自己打30分。 学习韩语，流利交流。好吧，我承认这个目标是为了女朋友定的。女朋友家里和她自己都是习惯用韩语交流的，恩，既然要成为一家人，还是要在这上面多做努力才是。期间和女朋友学过几次，但是都因为不太认真，中途放弃了。完成度为0分。  恩，实话说，之前半年的计划完成的并不是很好。尤其是技术上和读书上的计划，被工作占据的时间太多，一定程度上影响了计划的完成。不过也有自己的原因，确实没有平衡好学习和工作。计划的制定上还是犯了很多错误，比如没有考虑到自己的实际情况，还有就是某些计划太飘了，这样到年终也不好去衡量到底完成了没有。指定明年计划的时候要注意一下。
Review Work 这一年的工作基本都是在七牛云。2月末从猎豹移动离职，正式从PC端转向服务端。3月和4月，一个半月的时间主要是在适应团队和工作环境，负责了cdnspider的开发和维护，但是完成的并不是很好。
6月份毕业之后，离开学校，正式踏入社会，成为了一名码农。所在sophon小组负责七牛云cdn项目管理系统的开发和维护。贴一部分工作记录，review一下。 整体看来，这半年的工作还是非常，非常，非常饱和的。尤其是第四季度，9 11 6了2个月，身体和精神上都收到比较大的考验。不过，劳累的同时也确实学到了很多。总结一下，这半年来主要完成了以下几个事情。
 接入CDN提供商，开始了解并熟悉cdn相关业务，对golang语言的使用起到了一定的锻炼作用。期间因为做事方法不对，走了很多弯路，这个项目完成之后，给自己也定下了一个死规矩：万事先想好再写代码，一个项目的完成，99%的时间都是在思考，想清楚之后，%1的时间即可将自己的想法实现出来。 为cdn管理系统接入新版缓存规则。主要任务是推动融合的9家厂商适配新版的缓存规则。自己在这个项目中不但是其中三家厂商的负责人，需要在代码上亲自完成适配过程。同时还需要和其他几位同事合作，推动他们加快其负责厂商的新缓存适配。做这个项目的过程中，我慢慢明白了，程序员的工作内容不只有代码，还需要良好的沟通技能，以及强大的责任心，甚至是高效的做事方法。每一个项目都有自己最终的一个目标，作为这个项目的负责人，我有责任和义务去推动它，直至完成。 域名配置的标准化操作。主要任务是负责校准管理系统本地的域名配置和厂商的域名配置不匹配的地方。如本地的域名防盗链配置和cdn厂商的不一样，此时就应该以厂商为标准，校准本地数据库中的配置。在这个项目中，第一次接触了mongodb，非关系型的数据库。由于大学和第一份实习都是WIN平台下的，数据库只接触过sql server。而且操作的都是线上的数据，整个过程还是比较紧张的，怕误操作导致一些线上的事故。最终还是有惊无险的完成了这个项目，也体验了一把”洗库“的”快感“。从连mongo的查询语句都不会写，到后来也能够熟练操作，是一个从0到1的过程。但遗憾的是，最终和其他同事配合的不太好，清理工作的实际效果并没有那么大。 中间源镜像存储。主要的任务是接入中间源的镜像存储以及源站的独立。这是我第一次接手的一个非常紧急的需求，时间紧，东西虽然做出来了，但是离上线的程度还有一定的差距，最终导致了项目延期，花费了好多时间来补之前因为图快而埋下的坑。反思之后，还是觉得自己的工作方法有问题，把问题都归结到时间少上面是不对的。工作中很多时候都需要在有资源限制的条件下去完成一件事，对手中资源的利用以及目标的拆解和明确还做的非常不到位。 cdn管理系统重构。系统重构应该是今年我做的最大的一个项目，整个第四季度都是在围着他转，疯狂的加班了2个月。在这个项目中，我也负责了相对来说比较重要的几部分：整体框架的设计和实现，删除接口的实现，四家cdn厂商服务的设计与实现。不得不承认，虽然这个过程是很痛苦的，但是确实得到了不小的锻炼。尤其是在框架的设计和实现上，第一次用到了第三方的消息队列，第一次去试着设计一个小型的分布式系统，第一次非常正式的去整理项目的技术文档。在这个项目中有很多个第一次，让我得到了非常大的成长，无论是技术上的，还是工作技能上的。最大的体会有以下几点：a. 文档很重要，思考很重要，任何事情都需要想清楚在做。而文档就是为了在实施的过程当中指引着我们，不要偏离原来的航道。并且，要写出一个优秀的技术文档是非常非常难的。框架的设计和文档的编写我几乎花了一个多星期的时间，但是现在看来，这一切都是值得的。编程，有的时候想法比代码更重要。b.项目周期长，难度大，我仍然不能很好的控制住自己的情绪，会出现烦躁不安，易怒的情况。这也是一直让我头痛的地方，这半年努力的在改正，但是仍然没有完全控制住。c. 一个人的强大是没用的，现实就是需要我们去照顾一些能力较弱的同伴，毕竟大家在一条船上，我划的再快也是没用的。  总的来说，这一年工作上面收获还是蛮多的，唯一让我遗憾的就是，工作仍然在吞噬着我90%以上的时间。我没有太多的时间去总结和提升自己，成长并不一定就是无休止的工作，对于新人来讲，总结和反思更加重要。长时间如此，只能沦为工作机器，下一年在平衡生活和工作上面还要多做功能才是。
Review Me 自己这半年思想上波动比较大，年底也是做了一个比较大胆的决定，对生活和工作都有一些新的理解。感受最深的几点如下:
 绝不以自己的标准去要求别人，可以给出建议，但是不会强迫对方改正。但是我能改变的是我自己，严于律己，保持高度的自律是不可动摇的。 要努力控制住自己的情绪，不然其实还是会失去很多机会的。 做任何事情之前都要多想，想好这件事情应该怎么做，到底要做成什么样。工作和追求技术是两回事，追求技术可以细思慢想，但是工作确实需要在有限的时间和有限的条件下完成目标。以最直接最简洁的方式完成目标，自己的工作效率才会有一个大的提升。 对技术应该继续深入，迄今为止，我的计划仍然是两年之内不会考虑任何技术之外的事情，我需要在工作的前几年来沉淀自己的技术。  恩，对于自己来说，暂时想到的就是这样。叹一口气，感觉又是一年过去了。实话实说，毕业之后的日子过的一直很累，也诚惶诚恐。生怕自己一停下来，就变成了”api程序员“，工作几年之后却没有相应年限的能力。身边一些鲜活的例子也让给我敲响了警钟，一句话，在技术的道路上落后就要挨打。
未来的一年里，除了在工作上要更上一层路，软实力上也要注意锻炼了。年龄越大，工作的时间越久，可能以后需要的相关能力也就越多，早做准备，才能以防万一。趁着元旦假期也应该好好想想，自己下一个半年的计划了。</description>
    </item>
    
    <item>
      <title>初识RabbitMq</title>
      <link>http://littledriver.net/posts/rabbitmq-learning-nodes-1/</link>
      <pubDate>Sat, 17 Dec 2016 15:11:59 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/rabbitmq-learning-nodes-1/</guid>
      <description>RabbitMq &amp;amp; AMQP rabbitmq是一个实现了AMQP(高级消息队列协议)的消息队列系统。消息队列使用消息将通信双方链接起来，使得消息通过像rabbitmq这种消息代理服务器在不同的程序之间进行路由。这就像是在通信双方之间放置了一座邮局一样。rabbitmq不是对AMQP协议的唯一实现，与其说我们在学习rabbitmq，倒不如说我们在学习AMQP，一种标准的消息通信协议。
RabbitMq中的路由模型 rabbitmq作为一个邮局的角色，将通信的双方链接了起来。通信双方生产者和消费者的身份可以随意的变换，对于生产者和消费者来说，他们都不知道彼此的存在。 生产者生产消息发送给rabbitmq的服务器，消费者接受rabbitmq服务器发送的消息进行消费。通过rabbitmq服务器路由的消息通常分为两部分：
 消息主体内容(以二进制形式存储) 消息标签  主体内容不用解释，消息标签也很容易理解，它相当于有这条消息的一个描述，会决定这个消息最终的去向。
信道 想要发送和接受消息，通信双方首先都需要与rabbitmq服务器建立一条TCP连接。一旦TCP链接打开，我们就可以在其上建立一条AMQP信道。信道是建立在真实的TCP链接之上的，以后众多的AMQP命令都是通过信道来完成的。之所以不直接用TCP链接，是因为多线程频繁发送AMQP命令的时候 ，如接受消息，发送消息等。都会在短时间建立大量的TCP链接，首先这对系统的资源是一个极大的浪费，其次，线程的调度会使得TCP链接频繁的建立和关闭，这势必会影响到消息通信的性能。 我们的目标是在不影响性能和使用较少资源的前提下，满足多线程工作对于链接的需求。线程启动之后，在TCP连接上建立属于自己的AMQP信道，在不影响性能的前提下也保证的了通信的私密性。并且 ，在一条TCP链接上，一秒可以建立成千上万个AMQP信道，对于高并发系统的多线程调度是完全能够满足的。
AMQP的协议模型 AMQP协议栈中，有三个不可或缺的部分，以自上而下的顺序来展示，分别是：
 交换器 绑定 队列  队列 队列是最接近消费者的部分，也是rabbitmq中消息传递的终点。队列核心的作用有两个:
 存储未派发的消息 派发消息给相应的消费者  一条消息从生产者发出，首先要发送到交换器，交换器根据特定的规则将消息发送给相应的队列。此时消息存储在队列中。 如果当前有消费者订阅了该队列，那么就将消息派发给消费者，否则继续保存此消息。消费者订阅队列的方式也有以下两种：
 单次订阅： 通过AMQP提供的basic.get方法，每当想要处理消息的时候，都订阅一个队列，接收到消息之后取消订阅。等到再次需要消息的时候，与队列重新建立订阅关系，获取消息。 持续订阅： 通过AMQP提供的basic.consume方法，订阅一个队列，自动不断的接收队列派发的消息。直到主动取消与队列的订阅关系为止。  通过上面的描述，很容易看出，单次订阅是有需要的时候再去消息，消费者是主动的。而持续订阅消费者是被动的，要一直等待接收队列中的消息。 但是切记不要把单次订阅放在死循环中来模拟持续订阅的效果，这将会对rabbitmq的性能造成极大的影响。
队列中的消息派发给消费者之后，并不会立刻将这个消息从队列中删除，而是会等消费者回复一个确认的消息来确定此消息已经被成功接收了。这个时候，rabbitmq的队列才会放心的将此消息移除。rabbitmq在等待某个消息的确认信息之前，会一直在队列保存着这个消息。这样一来即使消费者因进程崩溃等原因断开了与mq的链接， rabbitmq会认为该消息没有被成功派发，进而会派发给其他订阅这个队列的消费者。假设没有这种机制，队列将消息派发出去之后并不知道该消息是否派发成功就删除它，那么， 如果此时消息派发失败，这个消息就被彻底丢掉了。这种成功接收消息的确认机制能够保证，队列中任意一个消息都会被消费者成功处理。
上面说的情况，是建立在队列派发的消息正是消费者需要处理的消息的前提下。假设，消费者并不想处理队列派发过来的消息， 通常有两种方式可以选择。
 在向rabbitmq发送确认信息前，主动断开链接。这个时候，rabbitmq会认为此消息没有被成功接受，会派发给其他订阅的消费者 调用AMQP提供的Reject命令，来拒绝接受此消息。  需要说明的是，在调用reject命令的时候，有一个叫做requeue的布尔型参数需要设置。若该值为true，那么被拒绝的消息会重新回到队列中等待被派发。 如果为false，那么该消息将会被发送至一个“死信”队列中。该队列中的消息都是一些被拒绝且不需要继续处理的。
要按照规则使用队列之前，首先需要创建队列。在AMQP中，消费者和生产者都可以创建队列，但是同一个队列只能被成功创建一次，另外一次的创建就是无效的，但是不会报错。 对消费者来说，如果已经在某个信道上订阅了某个队列，那么在对这个队列取消订阅之前，不得再创建新的队列。创建队列的时候会需要提供一个队列的名称，如不提供则rabbitmq会为你指定一个默认的名称。至于到底是由生产者来创建队列还是消费者来创建队列，最好的办法是消费者和生产者都创建。首先，创建两次并不会造成什么不利的影响，因此猜测创建队列的操作应该是幂等的。 其次，无论是让谁单独来创建，都有可能在消息从交换器发送过来的时候，队列还没有创建成功。如果出现这种情况，那么消息就会被丢弃。综合上面两个因素，生产者和消费者都做创建队列的工作比较稳妥。
交换器和绑定 有了交换器和绑定，他们和队列在实现了rabbitmq中的消息路由框架。一个完整的消息传递过程如下：生产者生产消息，消息由主体内容和标签构成。生产完毕后，将消息发送给交换器。交换器接收到消息后， 会根据消息的标签来决定将其投放给哪一个队列。这个标签也被称为路由键。队列需要携带特定的路由键绑定到交换器上。如果消息携带的路由键和队列绑定在交换器上的路由键相匹配，那么此时该消息就会被发送给这个队列。 否则，消息中的路由键没有和任何一个队列的路由键匹配，该消息将会被丢弃。投放到队列中的消息会被订阅该队列的消费者取走处理。
之所以使用这么复杂的方式，将生产者的消息送到消费者的身边。除了能够通过交换器和路由键实现多样化的通信方式之外，还有的一个好处就是，通信的双方都不需要在意对方的任何状态。他们交互的对象都是rabbitmq，这样一来 ，即使开发过程中出现消费者和生产者的更换，或者使用不同的语言来实现消费者和生产者，切换起来都是无障碍的，非常方便。在代码层面上，生产者和消费者也就彻底的解耦了。
交换器 交换器的类型决定了消息通信的使用场景，大致分为以下几类：
 direct fanout topic  direct direct交换器的作用非常简单：只要消息的路由键和队列的路由键匹配，那么就将消息投放到相应的队列中。rabbitmq服务器会自动实现一个direct类型的交换器。
fanout 见名知意，fanout类型的交换器即为扇形交换器。他会将收到的消息投放给所有绑定到该交换器上面的队列。这种特性能够帮助我们对同一个消息采取多种的处理方式。</description>
    </item>
    
    <item>
      <title>乐观锁和悲观锁</title>
      <link>http://littledriver.net/posts/pessimistic-and-optimistic-locker/</link>
      <pubDate>Sun, 27 Nov 2016 20:59:27 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/pessimistic-and-optimistic-locker/</guid>
      <description>本科的时候学习过数据库，但是只限于书面上的东西，并没有过多的实践。而且我依稀记得，当时教学的重点是在关系代数和数据库三大范式这两方面。 学完之后，晕晕乎乎，最后只记得自己学会了写sql语句。最近在工作中遇到了处理数据库并发性访问的难题，组里的前辈教会我用golang实现了悲观锁和乐观锁。虽然结合现实工作中的场景能够了解为何需要这两个东西。 但自己本身对乐观锁和悲观锁还是第一次听说(MD,好菜~~(&amp;gt;_&amp;lt;)~~)，觉得学习新东西不应该只停留在会用的层面上，要尝试理解一下背后的原理。So，写此博文，做一记录，同时也提醒自己，该补补数据库系统的知识了。
 数据库的并发控制 简单来说，数据库中并发控制的任务就是确保在多个事务同时存取同一份数据的时候，能够不破坏事务的统一性，隔离性以及数据的一致性。 乐观并发控制和悲观并发控制是实现并发控制的两种主要技术手段。将这两种思想进行延伸，就得出了我上面所说的，乐观锁和悲观锁。 乐观锁和悲观锁其实并不局限于关系型数据库中，非关系型数据库同样可以实现这两种锁。本质上来讲，乐观锁和悲观锁都是处理并发控制问题的一种思想。
悲观锁的定义(以关系型数据库为例) 在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务操作的数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。 悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。
场景1 众所周知，数据库的增删改查操作是原子操作。何为原子操作？说白了就是你在对一份数据做增删改查操作的时候，同一时间肯定只会有一个操作作用于这条数据上。不会出现丢失更新的情况。 实际应用的过程当中，我遇到过下面的情形：
 我需要先读数据库中id为1的一条数据，在代码中对取出的这份数据有所更改。最后把我修改后的这份数据写回数据库，以达到对id为1这条数据更新的目的。
 根据上面的描述我们可以知道，读，改，更新，这三个操作是捆绑在一起的，只有这三个操作都完成之后，才能够说明他们对id为1的这条数据处理完成了。所以，这三个操作在一起应该是一个原子性的操作。换句话说，在对id为1的这条数据执行那三个操作的时候，中途不得有人再去碰那条数据，无论是读写。 读的话可能会造成脏读的现象，写的话可能会造成丢失更新的情况。无论是同一个进程中的多线程，还是同一个服务部署在多台服务器上的多进程场景，都会出现上面所说的那种情况。 为了处理这样的问题，我们很容易想到的就是，为数据库中的每一条数据都加上锁。任何进程或者线程想要操作某条数据的时候，都必须通过加锁的方式来得到这条数据的操作权限。当对这条数据的操作完成之后，要通过解锁的方式释放掉这条数据操作权限，以便其他进程和线程来使用。
悲观锁思想的特别之处在于它对数据的修改是持悲观态度的，他假定脏读和更新覆盖这两个问题是有很大概率会发生的，所以对每条数据都加了锁。并且在任意一条数据处理的过程中，这条数据都是处于被锁定的状态。
悲观锁的实现 悲观锁的实现，既可以依靠数据库的排它锁机制，也可以自己实现一个“排它锁”来保证数据访问的排他性，这里只介绍实现悲观锁的主要工作流程，至于用什么方式来实现，各位读者可以根据自己的需求来决定。 （本人工作当中用的数据库是mongodb，是通过建立一个数据锁的collection来实现的。）
 对任意一条数据操作前，都加上排他锁 如果对每条数据加锁失败，说明获取此条数据的操作权限失败。此时，你可以选择设定超时时间进行等待，亦或是直接跑出异常 加锁成功，则获取到对应数据的操作权限。在操作完成之后，需要解锁 在某条数据正在处理期间，若有其他事务想要操作这条数据，都会等待解锁或者直接抛出异常  悲观锁的优缺点 悲观锁用比较极端的方式来保证了数据的一致性。但是服务运行的过程中，频繁对锁的操作会产生很多额外的开销。
乐观锁的定义(以关系型数据库为例) 在关系数据库管理系统里，乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。他假设多用户并发的事务并不会互相影响，各自的事务能够在不产生锁的情况下处理只影响自己的那份数据。在更新数据的时候，都会 去检查该事务读取数据之后有没有其他的事务对数据作了修改。如果比较结果和之前所读取到的数据没变化，那么则可以正常更新数据。如果比较结果有其他事务修改了数据，则正在提交的事务会回滚。
乐观锁的思想在于，他假设数据一般情况下不会出现冲突。所以也就不会对数据加锁。只有最后在提交对数据的更新的时候，才会去检测到底是否出现冲突。
场景2 同一条数据在并发环境中可能会被多个进程或者线程同时处理。假设目前有两个进程AB在同时处理一条数据data1，AB可能同时也可能先后拿到了data1，此时两个进程开始处理属于他们独立的两份数据，在处理期间互不影响。 当两个进程要处理完毕之后，需要更新data1。此时我们要求只有一个进程可以更新成功，如果不做限制，那么就又吃了并发的亏，造成了更新丢失的现象。
上述场景是非常符合乐观锁的思想的。乐观锁只在最后提交数据更新的时候去检测data1是否与之前读取到的版本是一致的，如果发现有修改，就证明在此进程处理的过程中已经有其他的更新操作成功了，此时就不能够再去覆盖已经成功的修改了。
乐观锁的实现 一般实现乐观锁的方式，是在每条数据内加上版本号。在读取数据的时候将版本号一起读出，在更新的时候，会先对比该数据的版本号看数据是否被修改过。如果没有，则将版本号+1，更新数据。否则放弃此次更新操作。需要注意的是，对比数据的版本号和更新版本号两个操作在一起必须是一个原子操作，mongo的update操作内部是保证了这一点，至于其他的数据库要想办法来实现这两个操作的原子性才可以。
乐观锁的优缺点 乐观锁的锁操作较悲观锁少了很多，减轻了很多系统的开销。但无法避免脏读现象。
到底该用哪个 如果你的系统对性能要求较高，并且允许同一份数据的不同形态在多进程或者多线程的环境下运行，直到最后更新数据的时候再处理冲突的话。那么可以使用乐观锁。 如果你的系统对数据的一致性要求较高，只允许一份数据以一种形态运行在多个进程或者线程中运行。不但写的时候要避免冲突，读的时候也要防止脏读。那么可以使用悲观锁。</description>
    </item>
    
    <item>
      <title>三种基本排序算法</title>
      <link>http://littledriver.net/posts/three-basic-sort-algorithms/</link>
      <pubDate>Wed, 09 Nov 2016 19:36:42 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/three-basic-sort-algorithms/</guid>
      <description>学习三种最基本的排序算法思想，文中代码已经经过大量测试。如发现有误，欢迎指正。
 选择排序算法 核心思想 如果以一个不含有重复元素并且元素个数为N的数组为排序对象的话，选择排序将从索引为0的数组元素开始，进行N趟排序，每趟排序都将确定一个元素是有序的。在进行第i趟排序的时候(i&amp;gt;=0)，待排序的元素为array[i],选择排序将会遍历a[i+1&amp;hellip;..N-1]的所有元素，找到i~N-1区间内的最小元素，然后和array[i]交换位置。每一趟排序，在寻找最小元素的时候，只是记录下当前最小元素的索引，直到本趟排序遍历结束才交换array[i]和找到的最小元素。当然，第i趟待排序的array[i]很可能就是当前最小的元素。
实现 	void sort(int[] a) { for(int i = 0; i &amp;lt; len(a); i++){ int min = i; for(int j = i+1; j &amp;lt; len(a); j++) { if (a[min] &amp;gt; a[j]){ min = j; } } int temp = a[i]; a[i] = a[min]; a[min] = temp; } }  性能分析 由上面的实现代码可以看书，选择排序最昂贵的操作是元素之间的比较。从索引为i的元素开始(i &amp;gt;= 0)，a[i]这个元素需要N-i-1次比较，依次类推，直到数组当中的最后一个需要排序的元素a[N-2],需要1次比较。由等差数列求合公式可知（1 + 2 + 3 + &amp;hellip; + N-1）= (N^2 - N)/2，所以选择排序的比较次数约为N^2/2。</description>
    </item>
    
    <item>
      <title>程序员，你的工作不只有代码</title>
      <link>http://littledriver.net/posts/programer-work/</link>
      <pubDate>Fri, 04 Nov 2016 07:48:24 +0000</pubDate>
      
      <guid>http://littledriver.net/posts/programer-work/</guid>
      <description>代码不是工作的全部，工作也不是生活的全部
 当我还在大学的时候，我对以后的工作的认知可以用一句话来概括：技术好的吃肉，技术不好的吃屎。计算机虽然是一个专业性和技术性较强的行业， 但是彼时的我太过迷信于技术，思想偏激，导致在第一份工作当中走了很多的极端。我想，很多计算机专业的学生可能在学校的时候，甚至是刚刚实习的时候都和我有一样的想法：工作就是写代码，代码写的牛逼了，自然什么好事就都找上门来了。然而，如今工作近一年，我对工作和技术以及生活，都有了与之前截然不同的认识。
工作的核心是解决问题 仔细想想，我们工作的内容既不是无休止的加班写代码，也不是每天疲惫的奔波于大小会议之间，而是解决我们自身遇到的，公司遇到的，用户遇到的各种问题。如果要给一个码农的工作能力打分的话，要关注的绝对不是他代码写的多牛逼，技术多好，解决问题的能力才是决定他工作能力高低的最重要的因素。漂亮的代码和高效的方案，其实都是我们解决问题的一种手段或是一项技能。可能有的技术能力非常突出，但是并不代表他能很好的完成工作。这也就解释了，为什么我们身边有那么多“技术大牛”，但是最后能够功成名就的没几个，甚至是“怀才不遇”。
单就工作来说，任何一个公司都不需要整个团队里面全都是技术牛人，但是却急需能够高效快速的解决问题并把事情做好的人。后者其实就是我们口中常说的，靠谱的人。当靠谱的人从接受一件事情到最后完美解决这件事情，一些良好的做事习惯和处理问题的细节将会贯穿整个过程。比如拿到问题之后先拆解问题，分而治之，如果是一个小团队的leader，他可能会考虑底下人的性格和做事风格，把对的事交给合适的人去做。总之，靠谱的人会积极整合和利用手中的资源，花费最少的时间把这件事做成。而技术，仅仅是这其中的一环。技术高超那当然是非常不错的，但是往往做一件事，解决我一个问题的时候，拿出70%的技术实力就可以了，剩下的30%可能跟你的情绪，做事风格有关。不然为什么有的时候技术上很简单的一个东西，但是最后把这件事做成却花费了相当多的时间呢？所以说，工作的核心是解决问题把交代给你的事情顺利的做好，技术的好坏只能影响你是否能够把这件事即完成但是又做的很漂亮，属于锦上添花。不要总是闷头搞技术，其他能力也一样重要。
做事不要走极端 很多码农兄弟可能看了上面一段话在心里会想:这个货一看就不是一个纯粹的技术人员，每天想这么多有啥用的，不抓紧提升自己的技术实力，毕竟技术强才是硬道理啊。对此，我不想多说什么，毕竟几个月前的我也是这种想法。
我觉得工作能力和技术能力是两种不同的力量，就像我们在大学里最后毕业有去考研搞科研的同学，也有投身于互联网企业想要锻炼自己工程能力的人一样。两种能力虽然会互相影响，但是本质上来说他们还是独立的。如果是一个工作能力很强，把每一件交代给他的事情都能完成很好的人，他的技术能力可能并不需要很牛，原因我上面也提到了。但是，作为一个技术人员，提升自己的技术实力是义不容辞的，甚至说是伴随整个职业生涯的。作为一个有追求并热爱自己工作的码农，肯定会想尽办法来提升自己，但这并不代表对其他的事情你可以什么都不想。我个人在这种走极端处理事情的方式上已经吃过很多亏，俗话说吃一堑，长一智。我觉得肯定是有办法来平衡这两者的。
周围有一个同事A，基础很好，名校毕业，让我一时间崇拜不已，心想，人家咋就那么NB呢，我为啥这么渣呢？！冷静下来仔细思考，一个人之所以能够达到他现在的状态，是受很多因素影响的，家庭环境，教育条件，努力程度，甚至是天赋。总是有那么几个人能够360°完爆你。我不想给自己灌鸡汤，也不想把这篇文章变做一碗鸡汤。事实就是很多人是比你强的，但是一时半会又追不上。怎么办？“日拱一卒”即可。
发现周围有厉害的人进而发现自己的不足这是正确的，但是妄自微薄却是错误的。总不能说他比我厉害我就不活了吧。生活还是要继续往下走，你现在意识到不如人家，哪里不如人家就可以一点一点的努力补上来，就算你到最后也追不上他，但是如果能一直保持你们之间的那个距离，是不是也算是一种进步呢？所以，为了弥补自己在技术上面的不足，我宁愿每天晚上拿出2个小时来看书，算法不好我补算法，Linux系统我不懂就刷apue。天长日久，付出一定会得到回报。
不要抱怨没成长 刚毕业的码农们都希望自己在技术上有着突飞猛进的成长，当你刚刚进入一家公司的时候这种感觉是最强烈的。但是天长日久，你很快就会发现你的工作内容已经对你的技术实力的成长没有多大帮助了，这个时候人就会感觉到慌，害怕，觉得自己如逆水行舟不进则退。这是一种非常普遍的现象，当然，我说的是那些对自己的前途有着理想的人，参加工作之后混吃等死的并不在我说的这一类人中。这个时候怎么办？靠天靠地不如靠自己，不要想着让别人给你创造一个让你快速成长的环境，抓紧时间提升自己。在技术方面制定一个计划，多看书，多实践，在提升自己实力的同时也在等待着机会。毕竟机会都是留给有准备的人。指不定哪一天就有一个从0到1的项目叫你去做，那个时候你就会庆幸自己在成长饱和期内没有浪费时间。总的来说，能不能成长靠你自己，总抱怨是没用的，与其怪罪他人没提供给你一个可以锻炼自己的事情和环境，不如马上行动起来，自己动手，丰衣足食。
学会承担 任何一个人的工作不是独立的，而是融于某一个团队之中。预估给每个人分配的任务都是一种假想：假定你在规定的时间内能够完成预估的工作。规定时间内完成不了的现象不说，当你在完成被分配的工作之后，并且还有一些冗余时间你会怎么做？去看一些自己想看的技术文章，还是去问问其他人有没有其他的工作可以帮着分担一下呢？
其实这两种想法都没有错，你去看技术文章可能是出于对自己负责，想提升自己的实力更好的工作，这无可厚非。但是如果你能够主动的去帮团队内其他成员分担一些繁重的工作，说明你更加成熟，考虑问题更加全面，想用自己的力量让整个团队的效率更高。
之前的我可能是比较自私的，我是属于那种工作完成就想去看技术书籍的人。但是最近的一些事让我感受到，我不单单需要技术上的成长，同时也需要工作能力的提升。我想观察下自己，是否有能力能够帮助整个团队做好一件事情并且一直向好的方向发展，这也是我在预判自己以后的职业走向的一种方式。团队和个体都是互惠互利的，当你想向团队索取的时候，先想想自己为团队贡献过什么吧。
return 0 经历多了，才知道一个优秀码农并不仅仅是技术上的优秀，还有很多因素制约着一个人的成长。上面说的几点，我现在仍然还没有做好，写这篇文章也并不想站在道德的制高点去贬低谁，只是觉得，既然发现了问题，就要思考问题，然后解决问题。
下一次，将会给大家带来另外一个话题：程序员，你的生活不仅仅只有工作。请大家敬请期待吧。</description>
    </item>
    
  </channel>
</rss>