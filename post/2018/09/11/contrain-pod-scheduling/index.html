<!DOCTYPE html>
<html lang="en-us" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Contrain Pod Scheduling - LittleDriver</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xuran" />
  <meta name="description" content="Overview 当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可" />







<meta name="generator" content="Hugo 0.51" />


<link rel="canonical" href="http://littledriver.net/post/2018/09/11/contrain-pod-scheduling/" />



<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.f384a7fa4673433bc34e238c5074bb1303d90e5d5a29a2ffbaf61226ef213c29.css" integrity="sha256-84Sn&#43;kZzQzvDTiOMUHS7EwPZDl1aKaL/uvYSJu8hPCk=" media="screen">





<meta property="og:title" content="Contrain Pod Scheduling" />
<meta property="og:description" content="Overview 当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://littledriver.net/post/2018/09/11/contrain-pod-scheduling/" /><meta property="article:published_time" content="2018-09-11T16:52:58&#43;08:00"/>
<meta property="article:modified_time" content="2018-09-11T16:52:58&#43;08:00"/>

<meta itemprop="name" content="Contrain Pod Scheduling">
<meta itemprop="description" content="Overview 当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可">


<meta itemprop="datePublished" content="2018-09-11T16:52:58&#43;08:00" />
<meta itemprop="dateModified" content="2018-09-11T16:52:58&#43;08:00" />
<meta itemprop="wordCount" content="3822">



<meta itemprop="keywords" content="Kubernetes," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Contrain Pod Scheduling"/>
<meta name="twitter:description" content="Overview 当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">LittleDriver</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/">Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/tags/">Tags</a>
          
        
      </li>
    
  </ul>
</nav>


  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      LittleDriver
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/">Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://littledriver.net/tags/">Tags</a>
          

        

      </li>
    
    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">Contrain Pod Scheduling</h1>
      
      <div class="post-meta">
        <time datetime="2018-09-11" class="post-time">
          2018-09-11
        </time>
        <div class="post-category">
            <a href="http://littledriver.net/categories/%E5%B7%A5%E4%BD%9C%E4%BA%86%E4%B9%9F%E4%B8%8D%E8%83%BD%E6%94%BE%E6%9D%BE%E7%B3%BB%E5%88%97/"> 工作了也不能放松系列 </a>
            
          </div>
        

        
        

        
        
      </div>
    </header>

    
    

    
    <div class="post-content">
      

<h2 id="overview">Overview</h2>

<p>当我们创建一个 Pod 之后，Kubernetes 自身的一些调度规则将会根据集群节点的各项指标，为这个 Pod 选出一个合适的 Node 且将其调度上去。我们姑且可以认为这个调度的过程对我们来说是「随机」的。因为在没有了解清楚 Kubernetes 的调度规则之前，我们也不知道创建的一个 Pod 将会被调度到哪台节点上。
但是在日常开发的过程中，尤其是基于 k8s 开发一些数据库相关应用的时候，我们通常对 Pod 被调度的节点是有要求的，比如：我们需要将主节点强制调度到某台机器上，或者我们需要主从节点所在的 Pod 不能调度到同一个 Node 上。</p>

<p>这些要求在 kubernetes 上是可以实现的，它提供以下几种方式来方便使用者干预 Pod 的调度策略：</p>

<ol>
<li>NodeSelector</li>
<li>Taints and Tolerations</li>
<li>Anti-Affinity/Affinity</li>
</ol>

<h2 id="nodeselector">NodeSelector</h2>

<p>NodeSelector 是依靠 LabelSelector 实现的一种调度策略。若想使用 NodeSelector， 需要分为两部分来考虑</p>

<h3 id="node">Node</h3>

<p>我们需要在集群中某一个我们想要调度到的 Node 上打上一个label，比如，A Node上的硬盘是 SSD，那我们就可以使用 kubectl 命令将 A Node 打上一个 disktype=ssd 的 Label。 <code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;</code></p>

<h3 id="pod">Pod</h3>

<p>对于 Pod 来说，我们需要在它的 spec 段内，加入 nodeSelector 段。并且在 nodeSelector 段中填入「目的 Node」 所携带的 Label。比如我们想将一个 Pod 强制调度到 Node A 上，那么在构造 Pod 的基本信息的时候，就要相应的做如下修改（以 yaml 文件为例）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></pre></td>
<td class="lntd">
<pre class="chroma">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd</pre></td></tr></table>
</div>
</div>
<p>(该 Pod 将会被强制调度到 Node Label 中有 disktype=ssd 的 Node 上)
一个可被调度的 Node 中的 labels 必须包含 Pod 的 nodeSelector 中指定的 labels。</p>

<h2 id="anti-affinity-affinity">Anti-Affinity/Affinity</h2>

<p>Kubernetes 提供了一种基于「亲和性」和「反亲和性」的调度策略。它实现了 NodeSelector（NodeSelector 基本相当于 NodeAffinity）能够提供的所有功能，并且额外做了如下改进：</p>

<ol>
<li>不但支持 Node 与 Pod 之间的「关系计算」（NodeSelector），还支持待调度 Pod 和已经运行在某个 Node 上面的 Pod 之间的「关系计算」</li>
<li>「关系计算」支持逻辑运算（AND， OR…）</li>
<li>对一个调度策略所需要满足的条件支持两种模式：「硬性」/「软性」。「硬性」意味着如果找不到符合指定条件的 Node，那么 Pod 将会调度失败。「软性」意味着，如果有符合调度条件的 Node 那么将优先采用。否则，将会根据默认的策略进行调度，从而最大限度的保证 Pod 被成功调度运行</li>
</ol>

<h3 id="nodeaffinity">NodeAffinity</h3>

<p>NodeAffinity 有两种类型:</p>

<ol>
<li>requiredDuringSchedulingIgnoredDuringExecution</li>
<li>preferredDuringSchedulingIgnoredDuringExecution</li>
</ol>

<p>两种类型的区别在于前半部分：requiredDuringScheduling or preferredDuringScheduling。 简单来说，requiredDuringScheduling属于「硬性」要求，即如果它指定的条件没有被满足的话，那么相应的 Pod 可能会调度失败。而 preferredDuringScheduling 属于「软性」要求，即如果它指定的条件没有被满足的话，也会按照默认的策略继续调度 Pod。 NodeAffinity 提供的调度策略的功能基本上和 NodeSelector 是一样的，都是通过 Node 和 Pod 之间的「关系计算」指定一个调度策略。</p>

<p>两种类型的相同点在于后半部分： IgnoredDuringExecution。也就是说，如果某一个 Pod 所在的 Node 的 label 发生了变化，导致一些已经运行在该 Node 上的 Pod 不符合运行条件，那么这些 Pod 也不会受到影响。总结下来就是，目前的「调度策略」都是在调度的过程中生效，至于已经调度完成的 Pod 不会受到影响。</p>

<p>为了更加直观的了解 NodeAffinity 的功能，我们通过一个例子来了解：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></pre></td>
<td class="lntd">
<pre class="chroma">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0</pre></td></tr></table>
</div>
</div>
<p>首先，如果在一个 Pod.Spec 段内同时指定了两种 NodeAffinity 的话，那么最好的情况下是调度到两个调度策略都满足的 Node 上。否则，优先满足 required 的。</p>

<p>其次，对于 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 类型来说：</p>

<ol>
<li>如果一个 nodeSelectorTerms 中包含多个 item，那么必须要所有的 item 都满足，该条件才会被认为已满足</li>
<li>如果是有多个 nodeSelectorTerms 的话，那么只要有一个 nodeSelectorTerms 满足，该条件就会被认为已满足</li>
</ol>

<p>两者之间的差别在于 weight 字段。因为 preferred是一个「软性」的要求，所在在调度 Pod 的时候，如果同时有多个候选 Node 的话，k8s 会对这些 Node 进行打分，从而选出一个分数最高的 Node 进行调度。打分的过程中，如果preferredDuringSchedulingIgnoredDuringExecution 中指定的条件被满足的话，Weight 就会被计入相应 Node 所获得的总分内。</p>

<blockquote>
<p>如果 NodeAffinity 和 NodeSelector 同时被指定的时候，必须两者都满足，该 Pod 才可被成功调度。</p>
</blockquote>

<h3 id="inter-pod-affinity-and-anti-affinity">Inter-pod affinity and anti-affinity</h3>

<p>pod affinity/anti-affinity 是基于 Pod 的 label 制定调度策略，而不是 Node 的 label。同时，除了 affinity 特性之外，Pod 还增加了 anti-affinity。两者的区别可以做如下理解：</p>

<blockquote>
<ul>
<li>affinity: 如果 affinity 的条件满足，那么 Pod 可以被调度到对应的 Node 上</li>
<li>anti-affinity：如果 anti-affinity 的条件满足，那么 Pod 不可以被调度到对应的 Node 上</li>
</ul>
</blockquote>

<p>Pod affinity/anti-affinity 实现的调度策略可以用一句话来概括</p>

<blockquote>
<p>The rules are of the form “this pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more pods that meet rule Y”</p>
</blockquote>

<p>其中 Y 可以被认为是 PodSelector， 即去筛选特定的一类 Pod。X 在这里也不仅仅代表某一个 Node，而是代表了一组 Node。Nodes 以一个特殊的字段作为标准进行分组：<code>topologyKey</code>（其实 topologyKey 就是 NodeLabel 中的一个 Key， 可以认为我们把带有 Label 为 topologyKey 且值相同的 Node 划分成了一组）。所以，对于 pod 的 affinity/anti-affinity 来说，我们首先要找到一组候选的 Node，然后再根据这些 Node 上面运行的 Pod 的 Label 来决定该 Pod 是否应该调度（or 不应该调度）到相应的节点上。</p>

<p>为了更加直观的了解，我们可以看一个具体的例子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></pre></td>
<td class="lntd">
<pre class="chroma">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: k8s.gcr.io/pause:2.0</pre></td></tr></table>
</div>
</div>
<p>该 Pod 设置了 pod affinity/anti-affinity 两个调度条件。
对于 podAffinity 来说，我们需要先找到一组带有 label 为 failure-domain.beta.kubernetes.io/zone 的 Node。如果这些 Node 上有运行 label key 为 security， value 为 S1 的 Pod，那么应该将 Pod 调度到这些 Node 上。该条件为「硬性」限制。也就是说，如果找不到符合上面要求的 Node，这个 Pod 将会面临着调度失败的情况。</p>

<p>对于 podAntiAffinity 来说， 我们首先要找到一组带有 label 为 kubernetes.io/hostname 的 Node（这个 Label 是每个 Node 默认具有的，不需要我们制定，属于 built-in 类型）。如果这些 Node 上有运行 label key 为 security ， value 为 S2 的 Pod，那么不应该将 Pod 调度到这些 Node 上。但是这个条件为「软性」限制，满不满足都不会影响 Pod 的调度。</p>

<p>综合起来，对于实例中的这个 Pod 来说，它的调度策略应为如下内容：</p>

<blockquote>
<p>首先要找到一组 Node 带有 label 为 failure-domain.beta.kubernetes.io/zone 且 Node 上需要运行 label key 为 security， value 为 S1 的 Pod。否则，调度失败。</p>

<p>若 podAffinity 条件成立，则在已经筛选出来的 Node 中继续寻找带有 label 为 kubernetes.io/hostname 的 Node 且 Node 上有运行 label key 为 security ， value 为 S2 的 Pod。若找到符合 pod anti-Affinity 条件的 Node 则不会将 Pod 调度到这些机器上。不过 pod anti-Affinity 指定的是一个「软性」条件，如果此时两个筛选条件找出的 Node 是重合的，Scheduler 将不会管 pod anti-Affinity 的策略。</p>
</blockquote>

<p>除了上面我们谈到的几点之外，pod affinity/anti-affinity 还有一个比较特殊的地方，就是 <code>namespace</code>。因为 Pod 是处于某个 namespace 下面的。所以在通过 pod 的 labelSelector 筛选符合条件的 Node 的时候，必须要带上 namespace。默认情况下，如果不指定 namespace，可以认为它就是 定义了 pod affinity/anti-affinity 的 pod 所在的 namespace（被调度的 Pod 所在的 namespace）。</p>

<h2 id="taints-and-tolerations">Taints and Tolerations</h2>

<p>taints 和 toleraitons 其实是一对的，需要配合在一起使用。taints 作用于 Node 侧，tolerations 作用于 Pod 侧。taints 和 tolerations 本质上来说和 label 的作用差不多，是一种起到标记作用的文本。</p>

<p>当一个 Node 打上了一个或者多个 taints，如果一个 Pod 没有指定 tolerations 或者指定的 tolerations 和 这个 Node 所指定的 taints “不匹配”。这个 Pod 就不能够被调度到这个 Node 上。如果“匹配”，那么这个 Pod 就可以被调度到这个 Node 上。</p>

<p>我们可以通过 kubectl 命令去指定 Node 的 taints <code>kubectl taint nodes node1 key=value: NoSchedule</code>。指定之后，通过 kubectl 观察 Node 的基本信息，可以看到如下内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></pre></td>
<td class="lntd">
<pre class="chroma">  taints:
  - effect: NoSchedule
    key: key
    value: &#34;value&#34;</pre></td></tr></table>
</div>
</div>
<p>这个 taints 的作用就是：不允许任何 Pod 调度到该 Node 上，除非这个 Pod 指定了 tolerations 且和该 taints“匹配”。</p>

<p>taints 的作用通过 effect 字段进行标识。除了 NoSchedule 之外，还有一个 PreferNoSchedule，与 NoSchedule 相比，它是一个「软性」的作用。也就是说，k8s 尽量不把 Pod 调度到 tolerations 和 taints 不匹配的 Node上，但是不排除「特殊情况」。taints 还有另外一个作用就是 NoExecute。当一个 Node 指定了 effect 为 NoExecute 的 taints，如果该 Node 上面的 Pod 的 tolerations 没有匹配它的 taints，这些 Pod 会被 kubelet 给 evicted 掉。(小朋友不要轻易尝试这个操作 o(<em>￣︶￣</em>)o)。不过，对于 tolerations 和 taints 匹配的 Pod，在 effect 为 NoExecute 的时候也不一定是「安全」的。原因在于，我们可以在 pod.Spec 中配置一个 tolerationSeconds 的字段，它的作用是：即使是 pod 的 tolerations 和 Node 的 taints 匹配，该 Pod 也只会存活 tolerationSeconds 秒的时间（从匹配成功开始算起），最终仍然会被 evicted 掉。</p>

<p>tolerations 需要在 pod.Spec 中进行指定：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></pre></td>
<td class="lntd">
<pre class="chroma">tolerations:
- key: &#34;key&#34;
  operator: &#34;Equal&#34;
  value: &#34;value&#34;
  effect: &#34;NoSchedule&#34; // &#34;PreferNoSchedule&#34;

tolerations:
- key: &#34;key&#34;
  operator: &#34;Exists&#34;
  effect: &#34;NoSchedule&#34;</pre></td></tr></table>
</div>
</div>
<p>tolerations 和 taints 的匹配过程如下：</p>

<ol>
<li>tolerations.key, tolerations.effect 必须要与 taints 的同名字段的值严格匹配</li>
<li>tolerations.value 在与 taints.value 匹配的过程中，需要遵循 operator 的规则</li>
</ol>

<p>operator 的规则有 Exists 和 Equal 之分：</p>

<ol>
<li>Exists：只要是 key 匹配就可以了，value 是什么并不重要。且在指定 tolerations 的时候，如果 operator 为 Exists，不应该指定 value</li>
<li>Equal：tolerations.value 在与 taints.value 必须要严格匹配</li>
</ol>

<h2 id="add-taints-automatically">Add taints automatically</h2>

<p>在 k8s 1.6 的版本之后，对于 pod 新增了一些基于 taints 的 evict 策略。在 1.8 版本之后更是增加了「根据 node condition 自动添加 taints」 的特性。综合起来可以认为：nodeController（or kubelet） 会根据 node 当前的健康状态为其添加一些 effect 为 NoExecute 的taints。从而实现「根据 Node 状态 evict Pod」的功能。</p>

<p>例如，当某个 Node 节点的网络情况发生异常的时候，nodeController 就会给该 Node 打上一个如下的 taints：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></pre></td>
<td class="lntd">
<pre class="chroma">  taints:
  - effect: NoExecute
    key: node.kubernetes.io/network-unavailable
    value: &#34;value&#34;</pre></td></tr></table>
</div>
</div>
    </div>

    
    

    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="http://littledriver.net/tags/kubernetes/">Kubernetes</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/2018/09/26/the-brief-of-pv-and-pvc/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">The brief of PV and PVC</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/2018/08/27/heade-first-redis-sentinel/">
            <span class="next-text nav-default">Heade First Redis Sentinel</span>
            <span class="prev-text nav-mobile">Next</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    Show Disqus Comments
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "http://littledriver.net/post/2018/09/11/contrain-pod-scheduling/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'fengzixu';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>
  
    



        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
  
    <a href="mailto:hnustphoenix@email.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://twitter.com/Haier0715" rel="me noopener" class="iconfont"
      title="twitter"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1264 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M1229.8616 18.043658c0 0-117.852626 63.135335-164.151872 67.344358-105.225559-164.151872-505.082682-92.598492-437.738325 223.078185C278.622548 312.675223 89.216542 47.506814 89.216542 47.506814s-117.852626 189.406006 75.762402 345.139833C127.097743 396.85567 55.544363 371.601535 55.544363 371.601535S26.081207 535.753407 253.368414 615.724832c-21.045112 29.463156-113.643603 8.418045-113.643603 8.418045s25.254134 143.10676 231.496229 180.987961c-143.10676 130.479693-387.230056 92.598492-370.393967 105.225559 206.242095 189.406006 1119.599946 231.496229 1128.01799-643.98042C1179.353331 249.539887 1263.533778 123.269217 1263.533778 123.269217s-130.479693 37.881201-138.897738 33.672179C1225.652577 98.015083 1229.8616 18.043658 1229.8616 18.043658"></path>
</svg>

    </a>
  
    <a href="https://github.com/fengzixu" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>


<a href="http://littledriver.net/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
       -
    2019
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xuran
        
      </span></span>

  
  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ="></script>














  <script id="dsq-count-scr" src="//fengzixu.disqus.com/count.js" async></script>





</body>
</html>
